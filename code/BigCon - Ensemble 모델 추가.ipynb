{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import joblib \n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# 회귀분석\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from math import sqrt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:/Users/KimMinyoung/Desktop/깃헙관련/빅콘git/data/total/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "['2020빅콘테스트_스포츠투아이_제공데이터_개인타자_2016.csv', '2020빅콘테스트_스포츠투아이_제공데이터_개인타자_2017.csv', '2020빅콘테스트_스포츠투아이_제공데이터_개인타자_2018.csv', '2020빅콘테스트_스포츠투아이_제공데이터_개인타자_2019.csv', '2020빅콘테스트_스포츠투아이_제공데이터_개인타자_2020.csv', '2020빅콘테스트_스포츠투아이_제공데이터_개인투수_2016.csv', '2020빅콘테스트_스포츠투아이_제공데이터_개인투수_2017.csv', '2020빅콘테스트_스포츠투아이_제공데이터_개인투수_2018.csv', '2020빅콘테스트_스포츠투아이_제공데이터_개인투수_2019.csv', '2020빅콘테스트_스포츠투아이_제공데이터_개인투수_2020.csv', '2020빅콘테스트_스포츠투아이_제공데이터_경기_2016.csv', '2020빅콘테스트_스포츠투아이_제공데이터_경기_2017.csv', '2020빅콘테스트_스포츠투아이_제공데이터_경기_2018.csv', '2020빅콘테스트_스포츠투아이_제공데이터_경기_2019.csv', '2020빅콘테스트_스포츠투아이_제공데이터_경기_2020.csv', '2020빅콘테스트_스포츠투아이_제공데이터_등록선수_2016.csv', '2020빅콘테스트_스포츠투아이_제공데이터_등록선수_2017.csv', '2020빅콘테스트_스포츠투아이_제공데이터_등록선수_2018.csv', '2020빅콘테스트_스포츠투아이_제공데이터_등록선수_2019.csv', '2020빅콘테스트_스포츠투아이_제공데이터_등록선수_2020.csv', '2020빅콘테스트_스포츠투아이_제공데이터_선수_2016.csv', '2020빅콘테스트_스포츠투아이_제공데이터_선수_2017.csv', '2020빅콘테스트_스포츠투아이_제공데이터_선수_2018.csv', '2020빅콘테스트_스포츠투아이_제공데이터_선수_2019.csv', '2020빅콘테스트_스포츠투아이_제공데이터_선수_2020.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀_2016.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀_2017.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀_2018.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀_2019.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀_2020.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀타자_2016.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀타자_2017.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀타자_2018.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀타자_2019.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀타자_2020.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀투수_2016.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀투수_2017.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀투수_2018.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀투수_2019.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀투수_2020.csv']\n"
     ]
    }
   ],
   "source": [
    "data_list = os.listdir(data_dir)\n",
    "print(len(data_list))\n",
    "print(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020년 데이터: \n",
      " [               G_ID   GDAY_DS T_ID VS_T_ID  HEADER_NO TB_SC   P_ID  START_CK  \\\n",
      "0     20200505HHSK0  20200505   HH      SK          0     T  61208         1   \n",
      "1     20200505HHSK0  20200505   SK      HH          0     B  61353         1   \n",
      "2     20200505HHSK0  20200505   HH      SK          0     T  62700         1   \n",
      "3     20200505HHSK0  20200505   SK      HH          0     B  62895         1   \n",
      "4     20200505HHSK0  20200505   SK      HH          0     B  63450         1   \n",
      "...             ...       ...  ...     ...        ...   ...    ...       ...   \n",
      "8055  20200719WOSK0  20200719   SK      WO          0     B  75847         1   \n",
      "8056  20200719WOSK0  20200719   SK      WO          0     B  76802         1   \n",
      "8057  20200719WOSK0  20200719   SK      WO          0     B  77463         0   \n",
      "8058  20200719WOSK0  20200719   WO      SK          0     T  78168         1   \n",
      "8059  20200719WOSK0  20200719   WO      SK          0     T  79456         1   \n",
      "\n",
      "      BAT_ORDER_NO  PA  ...  BB  IB  HP  KK  GD  ERR  LOB  P_HRA_RT  P_AB_CN  \\\n",
      "0                7   4  ...   0   0   0   0   0    0    0       0.0        1   \n",
      "1                2   4  ...   0   0   0   0   0    0    1       0.0        0   \n",
      "2                9   3  ...   0   0   0   1   1    0    1       0.5        2   \n",
      "3                5   3  ...   0   0   0   1   0    0    0       0.0        1   \n",
      "4                9   2  ...   0   0   0   1   0    0    0       0.0        0   \n",
      "...            ...  ..  ...  ..  ..  ..  ..  ..  ...  ...       ...      ...   \n",
      "8055             3   4  ...   0   0   0   1   0    0    1       0.0        1   \n",
      "8056             9   4  ...   1   0   0   0   0    0    4       1.0        1   \n",
      "8057             6   1  ...   1   0   0   0   0    0    0       0.0        0   \n",
      "8058             1   5  ...   0   0   0   0   1    0    0       0.0        1   \n",
      "8059             6   4  ...   0   0   0   0   0    0    0       0.0        0   \n",
      "\n",
      "      P_HIT_CN  \n",
      "0            0  \n",
      "1            0  \n",
      "2            1  \n",
      "3            0  \n",
      "4            0  \n",
      "...        ...  \n",
      "8055         0  \n",
      "8056         1  \n",
      "8057         0  \n",
      "8058         0  \n",
      "8059         0  \n",
      "\n",
      "[8060 rows x 31 columns],                G_ID   GDAY_DS T_ID VS_T_ID  HEADER_NO TB_SC   P_ID  START_CK  \\\n",
      "0     20200505HHSK0  20200505   SK      HH          0     B  50815         1   \n",
      "1     20200505HHSK0  20200505   SK      HH          0     B  63894         0   \n",
      "2     20200505HHSK0  20200505   HH      SK          0     T  69744         1   \n",
      "3     20200505HHSK0  20200505   SK      HH          0     B  76350         0   \n",
      "4     20200505LTKT0  20200505   KT      LT          0     B  50040         1   \n",
      "...             ...       ...  ...     ...        ...   ...    ...       ...   \n",
      "2882  20200719WOSK0  20200719   WO      SK          0     T  67313         1   \n",
      "2883  20200719WOSK0  20200719   WO      SK          0     T  67391         0   \n",
      "2884  20200719WOSK0  20200719   WO      SK          0     T  68341         0   \n",
      "2885  20200719WOSK0  20200719   WO      SK          0     T  69399         0   \n",
      "2886  20200719WOSK0  20200719   SK      WO          0     B  75138         0   \n",
      "\n",
      "      RELIEF_CK  CG_CK  ...  KK GD  WP  BK  ERR  R  ER  P_WHIP_RT  P2_WHIP_RT  \\\n",
      "0             0      0  ...   4  1   0   0    0  3   3        2.0    1.200000   \n",
      "1             1      0  ...   1  0   0   0    0  0   0        0.0    0.000000   \n",
      "2             0      1  ...   2  0   0   0    0  0   0        0.0    0.000000   \n",
      "3             1      0  ...   0  0   0   0    0  0   0        0.0    0.000000   \n",
      "4             0      0  ...   8  1   0   0    0  1   1        0.0    0.666667   \n",
      "...         ...    ...  ...  .. ..  ..  ..  ... ..  ..        ...         ...   \n",
      "2882          0      0  ...   7  0   1   0    0  0   0        0.0    1.000000   \n",
      "2883          1      0  ...   1  0   0   0    0  0   0        0.0    0.000000   \n",
      "2884          1      0  ...   2  0   1   0    0  4   4        0.0    0.000000   \n",
      "2885          1      0  ...   0  0   0   0    0  0   0        3.0    3.000000   \n",
      "2886          1      0  ...   0  0   0   0    0  0   0        0.0    0.000000   \n",
      "\n",
      "      CB_WHIP_RT  \n",
      "0       0.857143  \n",
      "1       1.500000  \n",
      "2       0.750000  \n",
      "3       0.000000  \n",
      "4       1.000000  \n",
      "...          ...  \n",
      "2882    2.250000  \n",
      "2883    0.000000  \n",
      "2884    1.500000  \n",
      "2885    0.000000  \n",
      "2886    0.000000  \n",
      "\n",
      "[2887 rows x 38 columns],               G_ID   GDAY_DS VISIT_KEY HOME_KEY  HEADER_NO GWEEK STADIUM\n",
      "0    20200505HHSK0  20200505        HH       SK          0     화      문학\n",
      "1    20200505LTKT0  20200505        LT       KT          0     화      수원\n",
      "2    20200505NCSS0  20200505        NC       SS          0     화      대구\n",
      "3    20200505OBLG0  20200505        OB       LG          0     화      잠실\n",
      "4    20200505WOHT0  20200505        WO       HT          0     화      광주\n",
      "..             ...       ...       ...      ...        ...   ...     ...\n",
      "315  20200719HHLG0  20200719        HH       LG          0     일      잠실\n",
      "316  20200719KTNC0  20200719        KT       NC          0     일      창원\n",
      "317  20200719LTSS0  20200719        LT       SS          0     일      대구\n",
      "318  20200719OBHT0  20200719        OB       HT          0     일      광주\n",
      "319  20200719WOSK0  20200719        WO       SK          0     일      문학\n",
      "\n",
      "[320 rows x 7 columns],         GDAY_DS T_ID   P_ID ENTRY_YN\n",
      "0      20200505   HH  62797        N\n",
      "1      20200505   HH  63464        N\n",
      "2      20200505   HH  63700        Y\n",
      "3      20200505   HH  63703        N\n",
      "4      20200505   HH  63765        N\n",
      "...         ...  ...    ...      ...\n",
      "45555  20200719   LG  79140        Y\n",
      "45556  20200719   LG  79150        Y\n",
      "45557  20200719   LG  79192        N\n",
      "45558  20200719   LG  79617        N\n",
      "45559  20200719   LG  79825        Y\n",
      "\n",
      "[45560 rows x 4 columns],      GYEAR  PCODE   NAME T_ID POSITION  AGE_VA     MONEY\n",
      "0     2020  50030    소형준   KT        투      19    2700만원\n",
      "1     2020  50036    이강준   KT        투      19    2700만원\n",
      "2     2020  50040  데스파이네   KT        투      33  450000달러\n",
      "3     2020  50054    천성호   KT        내      23    2700만원\n",
      "4     2020  50066    강현우   KT        포      19    2700만원\n",
      "..     ...    ...    ...  ...      ...     ...       ...\n",
      "622   2020  79705    김회성   HH        내      35    6500만원\n",
      "623   2020  79764    장민재   HH        투      30   11000만원\n",
      "624   2020  79825    여건욱   LG        투      34    7000만원\n",
      "625   2020  79847    김태훈   SK        투      30   24000만원\n",
      "626   2020  99445    권오준   SS        투      40    8000만원\n",
      "\n",
      "[627 rows x 7 columns],   T_ID T_NM\n",
      "0   HH   한화\n",
      "1   HT  KIA\n",
      "2   KT   KT\n",
      "3   LG   LG\n",
      "4   LT   롯데\n",
      "5   NC   NC\n",
      "6   OB   두산\n",
      "7   SK   SK\n",
      "8   SS   삼성\n",
      "9   WO   키움,               G_ID   GDAY_DS T_ID VS_T_ID  HEADER_NO TB_SC  PA  AB  RBI  RUN  \\\n",
      "0    20200505HHSK0  20200505   SK      HH          0     B  30  29    0    0   \n",
      "1    20200505HHSK0  20200505   HH      SK          0     T  35  31    3    3   \n",
      "2    20200505LTKT0  20200505   KT      LT          0     B  35  30    2    2   \n",
      "3    20200505LTKT0  20200505   LT      KT          0     T  37  32    7    7   \n",
      "4    20200505NCSS0  20200505   SS      NC          0     B  36  30    0    0   \n",
      "..             ...       ...  ...     ...        ...   ...  ..  ..  ...  ...   \n",
      "635  20200719LTSS0  20200719   LT      SS          0     T  31  29    2    2   \n",
      "636  20200719OBHT0  20200719   HT      OB          0     B  38  32    4    4   \n",
      "637  20200719OBHT0  20200719   OB      HT          0     T  41  37    8    8   \n",
      "638  20200719WOSK0  20200719   SK      WO          0     B  37  30    3    4   \n",
      "639  20200719WOSK0  20200719   WO      SK          0     T  37  33    3    3   \n",
      "\n",
      "     ...  BB  IB  HP  KK  GD  ERR  LOB  P_HRA_RT  P_AB_CN  P_HIT_CN  \n",
      "0    ...   1   0   0   2   0    0    3  0.000000        1         0  \n",
      "1    ...   2   0   0   5   1    0    5  0.250000        8         2  \n",
      "2    ...   5   0   0   8   1    0    6  0.000000        4         0  \n",
      "3    ...   4   0   0   9   1    1    3  0.666667        3         2  \n",
      "4    ...   6   0   0   8   0    0    9  0.000000        5         0  \n",
      "..   ...  ..  ..  ..  ..  ..  ...  ...       ...      ...       ...  \n",
      "635  ...   1   0   0   7   3    0    2  0.000000        2         0  \n",
      "636  ...   6   0   0   5   1    0    7  0.571429        7         4  \n",
      "637  ...   2   1   2   4   0    0    6  0.363636       11         4  \n",
      "638  ...   7   0   0  10   0    0    9  0.250000        8         2  \n",
      "639  ...   3   0   1   4   1    0    7  0.333333        3         1  \n",
      "\n",
      "[640 rows x 28 columns],               G_ID   GDAY_DS T_ID VS_T_ID  HEADER_NO TB_SC  CG_CK WLS  HOLD  \\\n",
      "0    20200505HHSK0  20200505   SK      HH          0     B      0   L     0   \n",
      "1    20200505HHSK0  20200505   HH      SK          0     T      1   W     0   \n",
      "2    20200505LTKT0  20200505   KT      LT          0     B      0   L     0   \n",
      "3    20200505LTKT0  20200505   LT      KT          0     T      0   W     0   \n",
      "4    20200505NCSS0  20200505   SS      NC          0     B      0   L     0   \n",
      "..             ...       ...  ...     ...        ...   ...    ...  ..   ...   \n",
      "635  20200719LTSS0  20200719   LT      SS          0     T      0   W     2   \n",
      "636  20200719OBHT0  20200719   HT      OB          0     B      0   L     0   \n",
      "637  20200719OBHT0  20200719   OB      HT          0     T      0   W     1   \n",
      "638  20200719WOSK0  20200719   SK      WO          0     B      0   W     0   \n",
      "639  20200719WOSK0  20200719   WO      SK          0     T      0   L     2   \n",
      "\n",
      "     INN2  ...  KK  GD  WP  BK  ERR  R  ER  P_WHIP_RT  P2_WHIP_RT  CB_WHIP_RT  \n",
      "0      27  ...   5   1   0   0    0  3   3   1.500000    1.200000    1.333333  \n",
      "1      27  ...   2   0   0   0    0  0   0   0.000000    0.000000    0.750000  \n",
      "2      27  ...   9   1   0   0    0  7   7   3.000000    1.285714    1.875000  \n",
      "3      27  ...   8   1   0   0    0  2   2   0.000000    1.000000    2.571429  \n",
      "4      27  ...   9   0   0   0    0  4   4   1.500000    1.058824    2.142857  \n",
      "..    ...  ...  ..  ..  ..  ..  ... ..  ..        ...         ...         ...  \n",
      "635    27  ...   9   1   0   0    0  1   1   0.600000    1.111111    1.500000  \n",
      "636    27  ...   4   0   0   0    0  8   8   2.142857    1.800000    0.750000  \n",
      "637    27  ...   5   1   0   0    0  4   4   3.000000    1.000000    1.875000  \n",
      "638    27  ...   4   1   0   0    0  3   3   1.500000    1.105263    2.142857  \n",
      "639    24  ...  10   0   2   0    0  4   4   1.800000    1.312500    1.333333  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[640 rows x 34 columns]]\n",
      "팀투수 데이터:\n",
      " [               G_ID   GDAY_DS T_ID VS_T_ID  HEADER_NO TB_SC  CG_CK WLS  HOLD  \\\n",
      "0     20160401HHLG0  20160401   LG      HH          0     B      0   W     0   \n",
      "1     20160401HHLG0  20160401   HH      LG          0     T      0   L     0   \n",
      "2     20160401HTNC0  20160401   NC      HT          0     B      0   W     0   \n",
      "3     20160401HTNC0  20160401   HT      NC          0     T      0   L     0   \n",
      "4     20160401KTSK0  20160401   SK      KT          0     B      0   L     0   \n",
      "...             ...       ...  ...     ...        ...   ...    ...  ..   ...   \n",
      "1435  20161008SSSK0  20161008   SS      SK          0     T      0   L     0   \n",
      "1436  20161009KTNC0  20161009   NC      KT          0     B      0   L     0   \n",
      "1437  20161009KTNC0  20161009   KT      NC          0     T      0   W     1   \n",
      "1438  20161009WOLT0  20161009   LT      WO          0     B      0   W     1   \n",
      "1439  20161009WOLT0  20161009   WO      LT          0     T      0   L     0   \n",
      "\n",
      "      INN2  ...  KK  GD  WP  BK  ERR  R  ER  P_WHIP_RT  P2_WHIP_RT  CB_WHIP_RT  \n",
      "0       36  ...  10   1   0   0    0  4   4   0.642857    1.285714    2.400000  \n",
      "1       34  ...  11   0   1   0    0  5   4   1.500000    1.000000    0.750000  \n",
      "2       27  ...  10   1   2   0    0  4   4   1.333333    1.038462    2.142857  \n",
      "3       24  ...   9   1   0   0    0  5   5   0.500000    1.695652    1.875000  \n",
      "4       27  ...   7   0   0   0    0  8   8   1.000000    2.357143    2.250000  \n",
      "...    ...  ...  ..  ..  ..  ..  ... ..  ..        ...         ...         ...  \n",
      "1435    24  ...   9   1   0   0    0  7   6   1.500000    2.538462    1.666667  \n",
      "1436    27  ...   7   0   2   1    0  7   7   0.900000    2.217391    1.500000  \n",
      "1437    27  ...  13   0   0   0    1  4   3   0.562500    1.428571    1.090909  \n",
      "1438    27  ...   6   1   1   0    0  5   5   1.500000    1.500000    1.000000  \n",
      "1439    24  ...   4   2   1   0    0  8   4   2.250000    1.666667    3.000000  \n",
      "\n",
      "[1440 rows x 34 columns],                G_ID   GDAY_DS T_ID VS_T_ID  HEADER_NO TB_SC  CG_CK WLS  HOLD  \\\n",
      "0     20170331HHOB0  20170331   OB      HH          0     B      0   W     0   \n",
      "1     20170331HHOB0  20170331   HH      OB          0     T      0   L     0   \n",
      "2     20170331HTSS0  20170331   SS      HT          0     B      0   L     0   \n",
      "3     20170331HTSS0  20170331   HT      SS          0     T      0   W     0   \n",
      "4     20170331KTSK0  20170331   SK      KT          0     B      0   L     0   \n",
      "...             ...       ...  ...     ...        ...   ...    ...  ..   ...   \n",
      "1435  20171003NCHH0  20171003   NC      HH          0     T      0   D     0   \n",
      "1436  20171003SKOB0  20171003   OB      SK          0     B      0   L     1   \n",
      "1437  20171003SKOB0  20171003   SK      OB          0     T      0   W     2   \n",
      "1438  20171003WOSS0  20171003   SS      WO          0     B      0   W     0   \n",
      "1439  20171003WOSS0  20171003   WO      SS          0     T      0   L     0   \n",
      "\n",
      "      INN2  ...  KK  GD  WP  BK  ERR   R  ER  P_WHIP_RT  P2_WHIP_RT  \\\n",
      "0       27  ...   8   0   1   0    0   0   0   0.600000    0.857143   \n",
      "1       24  ...   8   0   0   0    0   3   1   0.600000    0.450000   \n",
      "2       27  ...   5   0   1   0    0   7   6   2.000000    1.363636   \n",
      "3       27  ...   7   1   1   0    0   2   2   0.000000    0.857143   \n",
      "4       27  ...   8   1   0   0    1   3   2   1.200000    1.222222   \n",
      "...    ...  ...  ..  ..  ..  ..  ...  ..  ..        ...         ...   \n",
      "1435    36  ...   6   0   1   0    0   8   7   2.000000    1.031250   \n",
      "1436    27  ...   8   0   0   0    0   3   2   1.500000    1.222222   \n",
      "1437    27  ...   5   1   0   0    0   2   2   1.200000    1.000000   \n",
      "1438    27  ...  12   1   1   0    0   9   8   1.666667    2.000000   \n",
      "1439    24  ...   5   0   0   0    0  10   6   2.571429    2.538462   \n",
      "\n",
      "      CB_WHIP_RT  \n",
      "0       1.200000  \n",
      "1       0.272727  \n",
      "2       3.428571  \n",
      "3       1.333333  \n",
      "4       0.818182  \n",
      "...          ...  \n",
      "1435    3.000000  \n",
      "1436    1.500000  \n",
      "1437    0.600000  \n",
      "1438    4.500000  \n",
      "1439    2.000000  \n",
      "\n",
      "[1440 rows x 34 columns],                G_ID   GDAY_DS T_ID VS_T_ID  HEADER_NO TB_SC  CG_CK WLS  HOLD  \\\n",
      "0     20180324HHWO0  20180324   WO      HH          0     B      0   W     2   \n",
      "1     20180324HHWO0  20180324   HH      WO          0     T      0   L     0   \n",
      "2     20180324KTHT0  20180324   HT      KT          0     B      0   L     0   \n",
      "3     20180324KTHT0  20180324   KT      HT          0     T      0   W     2   \n",
      "4     20180324LGNC0  20180324   NC      LG          0     B      0   W     2   \n",
      "...             ...       ...  ...     ...        ...   ...    ...  ..   ...   \n",
      "1435  20181013NCHH0  20181013   NC      HH          0     T      0   L     0   \n",
      "1436  20181013WOSS0  20181013   SS      WO          0     B      0   W     0   \n",
      "1437  20181013WOSS0  20181013   WO      SS          0     T      0   L     0   \n",
      "1438  20181014OBLT0  20181014   LT      OB          0     B      0   L     1   \n",
      "1439  20181014OBLT0  20181014   OB      LT          0     T      0   W     1   \n",
      "\n",
      "      INN2  ...  KK  GD  WP  BK  ERR   R  ER  P_WHIP_RT  P2_WHIP_RT  \\\n",
      "0       27  ...   9   0   0   0    1   3   2   1.090909    1.600000   \n",
      "1       24  ...  10   0   2   0    0   6   5   1.750000    3.000000   \n",
      "2       27  ...   9   0   0   0    0   5   5   2.000000    1.333333   \n",
      "3       27  ...   8   0   0   0    0   4   4   1.200000    1.555556   \n",
      "4       27  ...   9   1   0   0    0   2   2   0.000000    0.777778   \n",
      "...    ...  ...  ..  ..  ..  ..  ...  ..  ..        ...         ...   \n",
      "1435    24  ...   4   1   1   0    0  10  10   6.000000    2.250000   \n",
      "1436    27  ...   6   1   0   0    0   5   5   3.000000    0.500000   \n",
      "1437    24  ...   8   1   0   0    1  12  10   2.000000   15.000000   \n",
      "1438    27  ...   2   1   1   0    0   5   5   2.250000    0.954545   \n",
      "1439    27  ...   4   3   0   0    0   1   1   1.000000    1.285714   \n",
      "\n",
      "      CB_WHIP_RT  \n",
      "0       2.250000  \n",
      "1       2.250000  \n",
      "2       1.500000  \n",
      "3       2.625000  \n",
      "4       1.333333  \n",
      "...          ...  \n",
      "1435    1.200000  \n",
      "1436    3.000000  \n",
      "1437    2.625000  \n",
      "1438    1.000000  \n",
      "1439    0.900000  \n",
      "\n",
      "[1440 rows x 34 columns],                G_ID   GDAY_DS T_ID VS_T_ID  HEADER_NO TB_SC  CG_CK WLS  HOLD  \\\n",
      "0     20190323HHOB0  20190323   OB      HH          0     B      0   W     1   \n",
      "1     20190323HHOB0  20190323   HH      OB          0     T      0   L     0   \n",
      "2     20190323KTSK0  20190323   SK      KT          0     B      0   W     1   \n",
      "3     20190323KTSK0  20190323   KT      SK          0     T      0   L     0   \n",
      "4     20190323LGHT0  20190323   HT      LG          0     B      0   L     0   \n",
      "...             ...       ...  ...     ...        ...   ...    ...  ..   ...   \n",
      "1435  20190930SKHH0  20190930   SK      HH          0     T      0   W     1   \n",
      "1436  20191001NCOB0  20191001   OB      NC          0     B      0   W     0   \n",
      "1437  20191001NCOB0  20191001   NC      OB          0     T      0   L     4   \n",
      "1438  20191001WOLT0  20191001   LT      WO          0     B      0   L     0   \n",
      "1439  20191001WOLT0  20191001   WO      LT          0     T      0   W     5   \n",
      "\n",
      "      INN2  ...  KK  GD  WP  BK  ERR  R  ER  P_WHIP_RT  P2_WHIP_RT  CB_WHIP_RT  \n",
      "0       27  ...   7   2   1   0    0  4   4   1.200000    1.888889    3.857143  \n",
      "1       24  ...   3   1   1   0    0  5   5   3.000000    1.375000    3.000000  \n",
      "2       27  ...  10   1   0   0    0  4   4   0.666667    1.375000    0.818182  \n",
      "3       24  ...   5   1   0   0    0  7   6   2.000000    1.826087    1.500000  \n",
      "4       27  ...  12   3   0   0    0  2   2   0.500000    1.222222    0.600000  \n",
      "...    ...  ...  ..  ..  ..  ..  ... ..  ..        ...         ...         ...  \n",
      "1435    27  ...   5   2   1   0    0  2   2   0.000000    1.000000    0.600000  \n",
      "1436    27  ...   7   1   2   0    0  5   5   1.250000    2.192308    2.000000  \n",
      "1437    25  ...   6   0   0   0    2  6   5   1.363636    1.695652    1.500000  \n",
      "1438    27  ...   9   1   2   0    0  3   1   1.333333    1.222222    0.545455  \n",
      "1439    27  ...   8   0   0   0    0  1   1   0.600000    1.111111    1.500000  \n",
      "\n",
      "[1440 rows x 34 columns],               G_ID   GDAY_DS T_ID VS_T_ID  HEADER_NO TB_SC  CG_CK WLS  HOLD  \\\n",
      "0    20200505HHSK0  20200505   SK      HH          0     B      0   L     0   \n",
      "1    20200505HHSK0  20200505   HH      SK          0     T      1   W     0   \n",
      "2    20200505LTKT0  20200505   KT      LT          0     B      0   L     0   \n",
      "3    20200505LTKT0  20200505   LT      KT          0     T      0   W     0   \n",
      "4    20200505NCSS0  20200505   SS      NC          0     B      0   L     0   \n",
      "..             ...       ...  ...     ...        ...   ...    ...  ..   ...   \n",
      "635  20200719LTSS0  20200719   LT      SS          0     T      0   W     2   \n",
      "636  20200719OBHT0  20200719   HT      OB          0     B      0   L     0   \n",
      "637  20200719OBHT0  20200719   OB      HT          0     T      0   W     1   \n",
      "638  20200719WOSK0  20200719   SK      WO          0     B      0   W     0   \n",
      "639  20200719WOSK0  20200719   WO      SK          0     T      0   L     2   \n",
      "\n",
      "     INN2  ...  KK  GD  WP  BK  ERR  R  ER  P_WHIP_RT  P2_WHIP_RT  CB_WHIP_RT  \n",
      "0      27  ...   5   1   0   0    0  3   3   1.500000    1.200000    1.333333  \n",
      "1      27  ...   2   0   0   0    0  0   0   0.000000    0.000000    0.750000  \n",
      "2      27  ...   9   1   0   0    0  7   7   3.000000    1.285714    1.875000  \n",
      "3      27  ...   8   1   0   0    0  2   2   0.000000    1.000000    2.571429  \n",
      "4      27  ...   9   0   0   0    0  4   4   1.500000    1.058824    2.142857  \n",
      "..    ...  ...  ..  ..  ..  ..  ... ..  ..        ...         ...         ...  \n",
      "635    27  ...   9   1   0   0    0  1   1   0.600000    1.111111    1.500000  \n",
      "636    27  ...   4   0   0   0    0  8   8   2.142857    1.800000    0.750000  \n",
      "637    27  ...   5   1   0   0    0  4   4   3.000000    1.000000    1.875000  \n",
      "638    27  ...   4   1   0   0    0  3   3   1.500000    1.105263    2.142857  \n",
      "639    24  ...  10   0   2   0    0  4   4   1.800000    1.312500    1.333333  \n",
      "\n",
      "[640 rows x 34 columns]]\n"
     ]
    }
   ],
   "source": [
    "#년도별 저장\n",
    "data_list_2016 = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-1].startswith('2016') ]\n",
    "data_2016 = [pd.read_csv(os.path.join(data_dir,data_list_2016[x]),encoding='cp949') for x in range(len(data_list_2016))]\n",
    "data_list_2017 = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-1].startswith('2017') ]\n",
    "data_2017 = [pd.read_csv(os.path.join(data_dir,data_list_2017[x]),encoding='cp949') for x in range(len(data_list_2017))]\n",
    "data_list_2018 = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-1].startswith('2018') ]\n",
    "data_2018 = [pd.read_csv(os.path.join(data_dir,data_list_2018[x]),encoding='cp949') for x in range(len(data_list_2018))]\n",
    "data_list_2019 = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-1].startswith('2019') ]\n",
    "data_2019 = [pd.read_csv(os.path.join(data_dir,data_list_2019[x]),encoding='cp949') for x in range(len(data_list_2019))]\n",
    "data_list_2020 = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-1].startswith('2020') ]\n",
    "data_2020 = [pd.read_csv(os.path.join(data_dir,data_list_2020[x]),encoding='cp949') for x in range(len(data_list_2020))]\n",
    "print('2020년 데이터: \\n', data_2020)\n",
    "\n",
    "#항목별 저장\n",
    "data_list_single_hitter = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-2].startswith('개인타자')]\n",
    "data_single_hitter = [pd.read_csv(os.path.join(data_dir, data_list_single_hitter[x]), encoding='cp949') for x in range(len(data_list_single_hitter))]\n",
    "data_list_single_pitcher = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-2].startswith('개인투수')]\n",
    "data_single_pitcher = [pd.read_csv(os.path.join(data_dir, data_list_single_pitcher[x]), encoding='cp949') for x in range(len(data_list_single_pitcher))]\n",
    "data_list_games = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-2].startswith('경기')]\n",
    "data_games = [pd.read_csv(os.path.join(data_dir, data_list_games[x]), encoding='cp949') for x in range(len(data_list_games))]\n",
    "data_list_player_enroll = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-2].startswith('등록선수')]\n",
    "data_player_enroll = [pd.read_csv(os.path.join(data_dir, data_list_player_enroll[x]), encoding='cp949') for x in range(len(data_list_player_enroll))]\n",
    "data_list_players = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-2].startswith('선수')]\n",
    "data_players = [pd.read_csv(os.path.join(data_dir, data_list_players[x]), encoding='cp949') for x in range(len(data_list_players))]\n",
    "data_list_teams = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-2].startswith('팀')]\n",
    "data_teams = [pd.read_csv(os.path.join(data_dir, data_list_teams[x]), encoding='cp949') for x in range(len(data_list_teams))]\n",
    "data_list_team_hitter = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-2].startswith('팀타자')]\n",
    "data_team_hitter = [pd.read_csv(os.path.join(data_dir, data_list_team_hitter[x]), encoding='cp949') for x in range(len(data_list_team_hitter))]\n",
    "data_list_team_pitcher = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-2].startswith('팀투수')]\n",
    "data_team_pitcher = [pd.read_csv(os.path.join(data_dir, data_list_team_pitcher[x]), encoding='cp949') for x in range(len(data_list_team_pitcher))]\n",
    "print('팀투수 데이터:\\n', data_team_pitcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_y_next(df, rm_range, target):\n",
    "    df['y_next'] = 0\n",
    "    \n",
    "    t_dict = {}\n",
    "    t_list = []\n",
    "    count = 0\n",
    "    f_count = 0\n",
    "    for i, v in enumerate(df[target]):\n",
    "\n",
    "        if df[target].index[i] == df[target].index[-1]:\n",
    "            t_dict[str(f_count)] = round(np.mean(np.array(t_list)),5)\n",
    "\n",
    "        t_list.append(v)\n",
    "        count += 1\n",
    "\n",
    "        if count == rm_range:\n",
    "            t_dict[str(f_count)] = round(np.mean(np.array(t_list)),5)\n",
    "            count = 0\n",
    "            t_list = []\n",
    "            f_count += 1\n",
    "\n",
    "    rm_dict = {}\n",
    "    rm_list = []\n",
    "    rm_count = 0\n",
    "    rm_fcount = 0\n",
    "\n",
    "    for i, v in enumerate(df['y_next']):\n",
    "        if df['y_next'].index[i] == df['y_next'].index[-1]:\n",
    "            rm_dict[str(rm_fcount)] = rm_list\n",
    "\n",
    "        rm_list.append(i)\n",
    "        rm_count += 1\n",
    "\n",
    "        if rm_count == rm_range:\n",
    "            rm_dict[str(rm_fcount)] = rm_list\n",
    "            rm_count = 0\n",
    "            rm_list = []\n",
    "            rm_fcount +=1\n",
    "\n",
    "    for k, v in rm_dict.items():\n",
    "        try:\n",
    "            df['y_next'].iloc[v] = t_dict[str(int(k)+1)]\n",
    "        except: \n",
    "            df['y_next'].iloc[v] = np.nan\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM + RF + SVR 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def models_pitcher_eda(year, team_name):\n",
    "    year_index = year - 2016 \n",
    "    \n",
    "    # team_pitcher data에 해당 경기의 해당 팀의 홈 어웨이 여부 column 추가\n",
    "    data_games_year = data_games[year_index]\n",
    "    data_games_year = data_games_year[['G_ID', 'VISIT_KEY', 'HOME_KEY']]\n",
    "    data_games_year = data_games_year.set_index('G_ID')\n",
    "    \n",
    "    data_team_pitcher_year = data_team_pitcher[year_index]\n",
    "    data_team_pitcher_year = pd.merge(data_team_pitcher_year, data_games_year, how='left', on=['G_ID'])\n",
    "\n",
    "    data_team_pitcher_year_team = data_team_pitcher_year[data_team_pitcher_year.T_ID == team_name]\n",
    "    df = data_team_pitcher_year_team\n",
    "    df['HOME_KEY'] = df['HOME_KEY'].map(lambda x: 1 if x == team_name else 0 )\n",
    "    df = df.drop(columns = ['VISIT_KEY', 'GDAY_DS'])\n",
    "    \n",
    "    # LabelEncoding\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    df['VS_T_ID'] = encoder.fit_transform(df['VS_T_ID']) #상대편\n",
    "    df['TB_SC'] = encoder.fit_transform(df['TB_SC']) #이닝 초/말\n",
    "    df['WLS'] = encoder.fit_transform(df['WLS']) #승패무 여부\n",
    "    \n",
    "    \n",
    "    ### 이부분은 모든 EDA에서 동일하게 작성\n",
    "    \n",
    "    df['H1']=df['HIT']-df['H2']-df['H3']-df['HR']\n",
    "\n",
    "    #df['G_ID'] = pd.Series(dt)\n",
    "    df['year']=df['G_ID'].str.slice(0,4).astype(int)\n",
    "    df['month']=df['G_ID'].str.slice(4,6).astype(int)\n",
    "    \n",
    "    df[\"FIP\"] = ((13*df.HR+3*(df.BB+df.HP-df.IB)-2*df.KK)/(df.INN2/3)) +3.2\n",
    "    df[\"BABIP\"] = (df.HIT-df.HR)/(df.AB-df.KK-df.HR+df.SF)\n",
    "    df['SLG']=(df['HIT']+df['HP']+df['BB'])/(df['AB']+df['BB']+df['HP']+df['SF'])\n",
    "    df['OBA']=(df['H1']+2*df['H2']+3*df['H3']+4*df['HR'])/df['AB']\n",
    "    df['W_OPS']=0.57* df['SLG']+0.43*df['OBA']\n",
    "\n",
    "    # 시계열 데이터(경기당 방어율 계산)이기 때문에 경기 코드를 index로\n",
    "    df = df.set_index('G_ID')\n",
    "\n",
    "    # 경기당 방어율 column 생성\n",
    "    df['ERA'] = df['ER'] * 27 / df['INN2']\n",
    "\n",
    "    df = df.drop(columns=['T_ID'])\n",
    "    df_year_team_name = df\n",
    "    \n",
    "    return df_year_team_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VS_T_ID</th>\n",
       "      <th>HEADER_NO</th>\n",
       "      <th>TB_SC</th>\n",
       "      <th>CG_CK</th>\n",
       "      <th>WLS</th>\n",
       "      <th>HOLD</th>\n",
       "      <th>INN2</th>\n",
       "      <th>BF</th>\n",
       "      <th>PA</th>\n",
       "      <th>AB</th>\n",
       "      <th>...</th>\n",
       "      <th>HOME_KEY</th>\n",
       "      <th>H1</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>FIP</th>\n",
       "      <th>BABIP</th>\n",
       "      <th>SLG</th>\n",
       "      <th>OBA</th>\n",
       "      <th>W_OPS</th>\n",
       "      <th>ERA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20160401OBSS0</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>147</td>\n",
       "      <td>37</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>6.755556</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.384194</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20160402OBSS0</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>160</td>\n",
       "      <td>42</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>4.644444</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.454774</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20160405SSKT0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>159</td>\n",
       "      <td>42</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>6.325000</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.499732</td>\n",
       "      <td>9.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20160406SSKT0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>138</td>\n",
       "      <td>38</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>6.866667</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20160407SSKT0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>139</td>\n",
       "      <td>34</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>2.533333</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.205784</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161003LGSS0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>188</td>\n",
       "      <td>46</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>6.088889</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.509417</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161004LGSS0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>148</td>\n",
       "      <td>35</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>4.977778</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.385974</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161005HTSS0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>140</td>\n",
       "      <td>38</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>2.866667</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.416125</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161006SSHT0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>153</td>\n",
       "      <td>35</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>4.088889</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.294384</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161008SSSK0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>140</td>\n",
       "      <td>40</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.481047</td>\n",
       "      <td>6.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               VS_T_ID  HEADER_NO  TB_SC  CG_CK  WLS  HOLD  INN2   BF  PA  AB  \\\n",
       "G_ID                                                                            \n",
       "20160401OBSS0        6          0      0      0    1     0    27  147  37  31   \n",
       "20160402OBSS0        6          0      0      0    2     0    27  160  42  38   \n",
       "20160405SSKT0        2          0      1      0    1     0    24  159  42  32   \n",
       "20160406SSKT0        2          0      1      0    2     0    27  138  38  36   \n",
       "20160407SSKT0        2          0      1      0    2     1    27  139  34  30   \n",
       "...                ...        ...    ...    ...  ...   ...   ...  ...  ..  ..   \n",
       "20161003LGSS0        3          0      0      0    1     0    27  188  46  40   \n",
       "20161004LGSS0        3          0      0      0    2     1    27  148  35  32   \n",
       "20161005HTSS0        1          0      0      0    1     0    27  140  38  32   \n",
       "20161006SSHT0        1          0      1      0    2     2    27  153  35  32   \n",
       "20161008SSSK0        7          0      1      0    1     0    24  140  40  37   \n",
       "\n",
       "               ...  HOME_KEY  H1  year  month       FIP     BABIP       SLG  \\\n",
       "G_ID           ...                                                            \n",
       "20160401OBSS0  ...         1   6  2016      4  6.755556  0.222222  0.333333   \n",
       "20160402OBSS0  ...         1   5  2016      4  4.644444  0.354839  0.380952   \n",
       "20160405SSKT0  ...         0   6  2016      4  6.325000  0.370370  0.452381   \n",
       "20160406SSKT0  ...         0   2  2016      4  6.866667  0.148148  0.236842   \n",
       "20160407SSKT0  ...         0   3  2016      4  2.533333  0.190476  0.235294   \n",
       "...            ...       ...  ..   ...    ...       ...       ...       ...   \n",
       "20161003LGSS0  ...         1   9  2016     10  6.088889  0.393939  0.422222   \n",
       "20161004LGSS0  ...         1   7  2016     10  4.977778  0.318182  0.323529   \n",
       "20161005HTSS0  ...         1   8  2016     10  2.866667  0.423077  0.400000   \n",
       "20161006SSHT0  ...         0   3  2016     10  4.088889  0.208333  0.257143   \n",
       "20161008SSSK0  ...         0   8  2016     10  6.950000  0.360000  0.375000   \n",
       "\n",
       "                    OBA     W_OPS   ERA  \n",
       "G_ID                                     \n",
       "20160401OBSS0  0.451613  0.384194  4.00  \n",
       "20160402OBSS0  0.552632  0.454774  6.00  \n",
       "20160405SSKT0  0.562500  0.499732  9.00  \n",
       "20160406SSKT0  0.500000  0.350000  6.00  \n",
       "20160407SSKT0  0.166667  0.205784  1.00  \n",
       "...                 ...       ...   ...  \n",
       "20161003LGSS0  0.625000  0.509417  8.00  \n",
       "20161004LGSS0  0.468750  0.385974  4.00  \n",
       "20161005HTSS0  0.437500  0.416125  3.00  \n",
       "20161006SSHT0  0.343750  0.294384  1.00  \n",
       "20161008SSSK0  0.621622  0.481047  6.75  \n",
       "\n",
       "[144 rows x 41 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_SS = models_pitcher_eda(2016, 'SS')\n",
    "df_SS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "\n",
    "def models_hitter_eda(year, team_name):\n",
    "    year_index = year - 2016 \n",
    "    \n",
    "    # team_hitter data에 해당 경기의 해당 팀의 홈 어웨이 여부 column 추가\n",
    "    data_games_year = data_games[year_index]\n",
    "    data_games_year = data_games_year[['G_ID', 'VISIT_KEY', 'HOME_KEY']]\n",
    "    data_games_year = data_games_year.set_index('G_ID')\n",
    "    \n",
    "    data_team_hitter_year = data_team_hitter[year_index]\n",
    "    data_team_hitter_year = pd.merge(data_team_hitter_year, data_games_year, how='left', on=['G_ID'])\n",
    "\n",
    "    data_team_hitter_year_team = data_team_hitter_year[data_team_hitter_year.T_ID == team_name]\n",
    "    df = data_team_hitter_year_team\n",
    "    df['HOME_KEY'] = df['HOME_KEY'].map(lambda x: 1 if x == team_name else 0 )\n",
    "    df = df.drop(columns = ['VISIT_KEY', 'G_ID'])\n",
    "    \n",
    "    df.reset_index(inplace=True, drop=True) #index 재정렬\n",
    "    \n",
    "    # LabelEncoding\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    df['VS_T_ID'] = encoder.fit_transform(df['VS_T_ID']) #상대편\n",
    "    df['TB_SC'] = encoder.fit_transform(df['TB_SC']) #이닝 초/말\n",
    "    \n",
    "    \n",
    "    ### 이부분은 모든 EDA에서 동일하게 작성\n",
    "    \n",
    "    # GDAY_DS => Datetime type\n",
    "    df['GDAY_DS'] = df['GDAY_DS'].astype(str) + (df['HEADER_NO']+1).astype(str)\n",
    "    df['year']=df['GDAY_DS'].str.slice(0,4).astype(int)\n",
    "    df['month']=df['GDAY_DS'].str.slice(4,6).astype(int)\n",
    "    df['H1']=df['HIT']-df['H2']-df['H3']-df['HR']\n",
    "    dt = []\n",
    "    for i in df['GDAY_DS']:\n",
    "        dt_ = datetime.strptime(i, '%Y%m%d%H')\n",
    "        dt.append(dt_)\n",
    "    df['GDAY_DS'] = pd.Series(dt)\n",
    "    df['month']=df['GDAY_DS'].apply(lambda x: x.month)\n",
    "    \n",
    "    #장타율\n",
    "    df['SLG']=(df['HIT']+df['HP']+df['BB'])/(df['AB']+df['BB']+df['HP']+df['SF'])\n",
    "    #출루율\n",
    "    df['OBA']=(df['H1']+2*df['H2']+3*df['H3']+4*df['HR'])/df['AB']\n",
    "    df['W_OPS']=0.57* df['SLG']+0.43*df['OBA']\n",
    "    \n",
    "    \n",
    "    \n",
    "    df = df.drop(columns=['T_ID'])\n",
    "    #lags\n",
    "    #lags=[1,4,6,12,30,60]\n",
    "    #for lag in lags:\n",
    "    #    df['AVG_lag_'+str(lag)]=df['AVG'].shift(lag).astype(np.float16)\n",
    "    #SK Expanding window 추가\n",
    "    #df['expanding_AVG_mean']=df['AVG'].transform(lambda x: x.expanding(2).mean().astype(np.float16))\n",
    "    #rolling window\n",
    "    #df['rolling_AVG_mean']=df['AVG'].transform(lambda x: x.rolling(window=7).mean().astype(np.float16))\n",
    "    #trend\n",
    "    \n",
    "    # 경기당 타율 column 생성\n",
    "    df['AVG'] = df['HIT'] / df['AB']\n",
    "    \n",
    "    df['avg_AVG'] = df['AVG'].mean()\n",
    "    df['AVG_trend'] = (df['AVG'] - df['avg_AVG']).astype(np.float16)\n",
    "    df.drop(['avg_AVG'],axis=1,inplace=True)\n",
    "    \n",
    "    # Drop Categorical feature for 시계열 => coint 과정에서 singular matrix 발생\n",
    "    df = df.drop(columns=[ 'HOME_KEY'])\n",
    "    \n",
    "    \n",
    "\n",
    "#     df = df.drop(columns=['T_ID', 'HEADER_NO', 'CG_CK', 'BK'])\n",
    "#     df = df.drop(columns=['TB_SC', 'HR', 'SB', 'VS_T_ID', 'HOME_KEY', 'HOLD', 'INN2', 'BF', 'CS', 'SH', 'HP', 'GD', 'ERR' ,'ER'])\n",
    "    df_year_team_name = df\n",
    "    \n",
    "    return df_year_team_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 지정\n",
    "\n",
    "lgb_params_grid = {\n",
    "    'num_leaves': [5,10,20,30],\n",
    "    'reg_alpha': [0.1, 0.5],\n",
    "    'min_data_in_leaf': [10,30, 50, 100],\n",
    "    'lambda_l1': [0, 1, 1.5],\n",
    "    'lambda_l2': [0, 1]\n",
    "    }\n",
    "svr_params_grid = { 'kernel':['rbf','poly'],'degree':[2,3,4,5,6,7],'epsilon':[0.1,0.2,1,10,20,30] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class models_pitcher:\n",
    "    def __init__(self, eda,  team, yn_range, model_type):\n",
    "        self.eda = eda\n",
    "        self.team = team\n",
    "        self.yn_range = yn_range\n",
    "        self.model_type = model_type\n",
    "    \n",
    "    def run(self):\n",
    "        df_2016 = self.eda(2016, self.team)\n",
    "        df_2017 = self.eda(2017, self.team)\n",
    "        df_2018 = self.eda(2018, self.team)\n",
    "        df_2019 = self.eda(2019, self.team)\n",
    "        df_2020 = self.eda(2020, self.team)\n",
    "        \n",
    "        df_all = pd.concat([df_2016,df_2017,df_2018,df_2019,df_2020])\n",
    "        df_all =df_all.reset_index()\n",
    "        df_all = make_y_next(df_all, self.yn_range, 'ERA')\n",
    "        \n",
    "        # 테스트용 데이터 뽑기\n",
    "        final_test = df_all[df_all['y_next'].isnull()]; \n",
    "\n",
    "        df_all.dropna(inplace=True)#일단 드랍하고 진행\n",
    "\n",
    "        df_all=df_all.drop(['G_ID'],axis=1)\n",
    "\n",
    "        # 범주형 변수 지정\n",
    "        cat_features = ['VS_T_ID' ,'HEADER_NO', 'TB_SC' ]\n",
    "        df_all[cat_features] = df_all[cat_features].astype('category')\n",
    "\n",
    "        # X,y Train split\n",
    "        X = df_all.drop(columns = ['y_next'])\n",
    "        y = df_all['y_next']\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.15, shuffle = True, random_state = 2020)\n",
    "\n",
    "        X_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train.columns]\n",
    "        X_val.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_val.columns]\n",
    "\n",
    "        if(self.model_type==\"lgbm\"):\n",
    "            m = LGBMRegressor(boosting_type='gbdt', num_boost_round=2000, learning_rate=0.01)\n",
    "            m_grid = GridSearchCV(estimator=m,lgbm_param_grid=params_grid,n_jobs=10,verbose=3)\n",
    "            m_grid.fit(X_train,y_train) \n",
    "            m = m_grid\n",
    "            \n",
    "        elif(self.model_type==\"rf\"):\n",
    "            m =  RandomForestRegressor(bootstrap=True, max_depth=10, max_features='auto', min_samples_leaf=5, min_samples_split=2, n_estimators=1000)\n",
    "            m.fit(X_train,y_train)\n",
    "\n",
    "        elif(self.model_type==\"SVR\"):\n",
    "            m = SVR(kernel='rbf')\n",
    "            m_grid = GridSearchCV(estimator=m, svr_param_grid=params_grid) \n",
    "            m.fit(X_train, y_train)\n",
    "            \n",
    "\n",
    "        final_test = final_test.drop(['G_ID','y_next'],axis=1)\n",
    "        final_test[cat_features] = final_test[cat_features].astype('category')\n",
    "\n",
    "        final_predict = m.predict(final_test)\n",
    "        final_predict_mean = final_predict.mean()\n",
    "\n",
    "        return final_predict_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 랜포추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class models_hitter:\n",
    "    def __init__(self, eda,  team, yn_range, model_type):\n",
    "        self.eda = eda\n",
    "        self.team = team\n",
    "        self.yn_range = yn_range\n",
    "        self.model_type = model_type\n",
    "    def run(self):\n",
    "        df_2016 = self.eda(2016, self.team)\n",
    "        df_2017 = self.eda(2017, self.team)\n",
    "        df_2018 = self.eda(2018, self.team)\n",
    "        df_2019 = self.eda(2019, self.team)\n",
    "        df_2020 = self.eda(2020, self.team)\n",
    "\n",
    "        df_all = pd.concat([df_2016,df_2017,df_2018,df_2019,df_2020])\n",
    "        df_all =df_all.reset_index()\n",
    "        \n",
    "        df_all = make_y_next(df_all, self.yn_range, 'AVG')\n",
    "        \n",
    "        # 테스트용 데이터 뽑기\n",
    "        final_test = df_all[df_all['y_next'].isnull()]; \n",
    "        df_all.dropna(inplace=True)#일단 드랍하고 진행\n",
    "        df_all = df_all.drop(['GDAY_DS'],axis=1)\n",
    "\n",
    "        # 범주형 변수 지정\n",
    "        cat_features = ['VS_T_ID' ,'HEADER_NO', 'TB_SC' ]\n",
    "        df_all[cat_features] = df_all[cat_features].astype('category')\n",
    "\n",
    "        # X,y Train test val split\n",
    "        X = df_all.drop(columns = ['y_next'])\n",
    "        y = df_all['y_next']\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.15, shuffle = True, random_state = 2020)\n",
    "\n",
    "        X_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train.columns]\n",
    "        X_val.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_val.columns]\n",
    "\n",
    "        if(self.model_type==\"lgbm\"):\n",
    "            m = LGBMRegressor(boosting_type='gbdt', num_boost_round=2000, learning_rate=0.01)\n",
    "            m_grid = GridSearchCV(estimator=m,param_grid=lgb_params_grid,n_jobs=10,verbose=3)\n",
    "            m_grid.fit(X_train,y_train) \n",
    "            m = m_grid\n",
    "            \n",
    "        elif(self.model_type==\"rf\"):\n",
    "            m =  RandomForestRegressor(bootstrap=True, max_depth=10, max_features='auto', min_samples_leaf=5, min_samples_split=2, n_estimators=1000)\n",
    "            m.fit(X_train,y_train)\n",
    "\n",
    "        elif(self.model_type==\"SVR\"):\n",
    "            m = SVR(kernel='rbf')\n",
    "            m_grid = GridSearchCV(estimator=m, param_grid=svr_params_grid) \n",
    "            m_grid.fit(X_train, y_train)\n",
    "            m = m_grid\n",
    "\n",
    "        final_test = final_test.drop(['GDAY_DS','y_next'],axis=1)\n",
    "        final_test[cat_features] = final_test[cat_features].astype('category')\n",
    "\n",
    "        final_predict = m.predict(final_test)\n",
    "        final_predict_mean = final_predict.mean()\n",
    "\n",
    "        return final_predict_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 승률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class models_lose:\n",
    "    def __init__(self, eda,  team, yn_range, model_type):\n",
    "        self.eda = eda\n",
    "        self.team = team\n",
    "        self.yn_range = yn_range\n",
    "        self.model_type = model_type\n",
    "        \n",
    "    def run(self):\n",
    "        df_2016 = self.eda(2016, self.team)\n",
    "        df_2017 = self.eda(2017, self.team)\n",
    "        df_2018 = self.eda(2018, self.team)\n",
    "        df_2019 = self.eda(2019, self.team)\n",
    "        df_2020 = self.eda(2020, self.team)\n",
    "        \n",
    "        df_all = pd.concat([df_2016,df_2017,df_2018,df_2019,df_2020])\n",
    "        df_all =df_all.reset_index()\n",
    "\n",
    "        df_all = make_y_next(df_all, self.yn_range, 'R')\n",
    "        \n",
    "        # 테스트용 데이터 뽑기\n",
    "        final_test = df_all[df_all['y_next'].isnull()]; \n",
    "\n",
    "        df_all.dropna(inplace=True)#일단 드랍하고 진행\n",
    "\n",
    "        df_all=df_all.drop(['G_ID'],axis=1)\n",
    "\n",
    "        # 범주형 변수 지정\n",
    "        cat_features = ['VS_T_ID' ,'HEADER_NO', 'TB_SC' ]\n",
    "        df_all[cat_features] = df_all[cat_features].astype('category')\n",
    "\n",
    "        # X,y Train split\n",
    "        X = df_all.drop(columns = ['y_next'])\n",
    "        y = df_all['y_next']\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.15, shuffle = True, random_state = 2020)\n",
    "\n",
    "        X_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train.columns]\n",
    "        X_val.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_val.columns]\n",
    "\n",
    "        if(self.model_type==\"lgbm\"):\n",
    "            m = LGBMRegressor(boosting_type='gbdt', num_boost_round=2000, learning_rate=0.01)\n",
    "            m_grid = GridSearchCV(estimator=m,param_grid=lgb_params_grid,n_jobs=10,verbose=3)\n",
    "            m_grid.fit(X_train,y_train) \n",
    "            m = m_grid\n",
    "    \n",
    "        elif(self.model_type==\"rf\"):\n",
    "            m =  RandomForestRegressor(bootstrap=True, max_depth=10, max_features='auto', min_samples_leaf=5, min_samples_split=2, n_estimators=1000)\n",
    "            m.fit(X_train,y_train)\n",
    "                    \n",
    "        elif(self.model_type==\"SVR\"):\n",
    "            m = SVR(kernel='rbf')\n",
    "            m_grid = GridSearchCV(estimator=m, param_grid=svr_params_grid) \n",
    "            m_grid.fit(X_train, y_train)\n",
    "            m = m_grid\n",
    "\n",
    "        final_test = final_test.drop(['G_ID','y_next'],axis=1)\n",
    "        final_test[cat_features] = final_test[cat_features].astype('category')\n",
    "        \n",
    "        final_predict = m.predict(final_test)\n",
    "\n",
    "        return final_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LGBM_score:\n",
    "    def __init__(self, eda,  team, yn_range,model_type):\n",
    "        self.eda = eda\n",
    "        self.team = team\n",
    "        self.yn_range = yn_range\n",
    "        self.model_type = model_type\n",
    "        \n",
    "    def run(self):\n",
    "        df_2016 = self.eda(2016, self.team)\n",
    "        df_2017 = self.eda(2017, self.team)\n",
    "        df_2018 = self.eda(2018, self.team)\n",
    "        df_2019 = self.eda(2019, self.team)\n",
    "        df_2020 = self.eda(2020, self.team)\n",
    "\n",
    "        df_all = pd.concat([df_2016,df_2017,df_2018,df_2019,df_2020])\n",
    "        df_all =df_all.reset_index()\n",
    "        \n",
    "        df_all = make_y_next(df_all, self.yn_range, 'RUN')\n",
    "        \n",
    "        # 테스트용 데이터 뽑기\n",
    "        final_test = df_all[df_all['y_next'].isnull()]; \n",
    "\n",
    "        df_all.dropna(inplace=True)#일단 드랍하고 진행\n",
    "\n",
    "        df_all = df_all.drop(['GDAY_DS'],axis=1)\n",
    "\n",
    "        # 범주형 변수 지정\n",
    "        cat_features = ['VS_T_ID' ,'HEADER_NO', 'TB_SC' ]\n",
    "        df_all[cat_features] = df_all[cat_features].astype('category')\n",
    "\n",
    "        # X,y Train test val split\n",
    "        X = df_all.drop(columns = ['y_next'])\n",
    "        y = df_all['y_next']\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.15, shuffle = True, random_state = 2020)\n",
    "\n",
    "        X_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train.columns]\n",
    "        X_val.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_val.columns]\n",
    "\n",
    "\n",
    "        if(self.model_type==\"lgbm\"):\n",
    "            m = LGBMRegressor(boosting_type='gbdt', num_boost_round=2000, learning_rate=0.01)\n",
    "            m_grid = GridSearchCV(estimator=m,param_grid=lgb_params_grid,n_jobs=10,verbose=3)\n",
    "            m_grid.fit(X_train,y_train) \n",
    "            m = m_grid\n",
    "            \n",
    "        elif(self.model_type==\"rf\"):\n",
    "            m =  RandomForestRegressor(bootstrap=True, max_depth=10, max_features='auto', min_samples_leaf=5, min_samples_split=2, n_estimators=1000)\n",
    "            m.fit(X_train,y_train)\n",
    "\n",
    "        elif(self.model_type==\"SVR\"):\n",
    "            m = SVR(kernel='rbf')\n",
    "            m_grid = GridSearchCV(estimator=m, param_grid=svr_params_grid) \n",
    "            m_grid.fit(X_train, y_train)\n",
    "            m = m_grid\n",
    "\n",
    "        final_test = final_test.drop(['GDAY_DS','y_next'],axis=1)\n",
    "        final_test[cat_features] = final_test[cat_features].astype('category')\n",
    "\n",
    "        final_predict = m.predict(final_test)\n",
    "\n",
    "        return final_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_odds_predict(team, predict_range, model_type):\n",
    "    score = LGBM_score(lgbm_hitter_eda, team, predict_range, model_type)\n",
    "    lose = LGBM_lose(lgbm_pitcher_eda, team, predict_range, model_type)\n",
    "\n",
    "    lose = lose.run()\n",
    "    score = score.run()\n",
    "\n",
    "    score = np.array(score)\n",
    "    lose = np.array(lose)\n",
    "    win_rate= np.square(score)  / ( np.square(score)  +   np.square(lose) )\n",
    "    win_rate_mean = win_rate.mean()\n",
    "    return win_rate_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=10)]: Done 492 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=10)]: Done 960 out of 960 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   55.9s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=10)]: Done 492 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=10)]: Done 960 out of 960 | elapsed:  5.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5854154920309727"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_odds_predict('SS', 20, 'lgbm')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5875297349539067"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_odds_predict('SS', 20, 'rf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46079828823744845"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_odds_predict('SS', 20, 'SVR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library import\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from IPython.display import display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "\n",
    "def var_pitcher_eda(team_name):\n",
    "    \n",
    "    # team_pitcher data에 해당 경기의 해당 팀의 홈 어웨이 여부 column 추가\n",
    "    data_games_2016 = data_games[0]\n",
    "    data_games_2017 = data_games[1]\n",
    "    data_games_2018 = data_games[2]\n",
    "    data_games_2019 = data_games[3]\n",
    "    data_games_2020 = data_games[4]\n",
    "    data_games_year = pd.concat([data_games_2016,data_games_2017,data_games_2018,data_games_2019,data_games_2020])\n",
    "\n",
    "    data_games_year = data_games_year[['G_ID', 'VISIT_KEY', 'HOME_KEY']]\n",
    "    data_games_year = data_games_year.set_index('G_ID')\n",
    "    \n",
    "    data_team_pitcher_2016 = data_team_pitcher[0]\n",
    "    data_team_pitcher_2017 = data_team_pitcher[1]\n",
    "    data_team_pitcher_2018 = data_team_pitcher[2]\n",
    "    data_team_pitcher_2019 = data_team_pitcher[3]\n",
    "    data_team_pitcher_2020 = data_team_pitcher[4]\n",
    "    data_team_pitcher_year = pd.concat([data_team_pitcher_2016,data_team_pitcher_2017,data_team_pitcher_2018,\n",
    "                                        data_team_pitcher_2019,data_team_pitcher_2020])\n",
    "\n",
    "    data_team_pitcher_year = pd.merge(data_team_pitcher_year, data_games_year, how='left', on=['G_ID'])\n",
    "\n",
    "    data_team_pitcher_year_team = data_team_pitcher_year[data_team_pitcher_year.T_ID == team_name]\n",
    "    df = data_team_pitcher_year_team\n",
    "    df['HOME_KEY'] = df['HOME_KEY'].map(lambda x: 1 if x == team_name else 0 )\n",
    "    df = df.drop(columns = ['VISIT_KEY', 'G_ID'])\n",
    "    \n",
    "    df.reset_index(inplace=True, drop=True) #index 재정렬\n",
    "    \n",
    "    # LabelEncoding\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    df['VS_T_ID'] = encoder.fit_transform(df['VS_T_ID']) #상대편\n",
    "    df['TB_SC'] = encoder.fit_transform(df['TB_SC']) #이닝 초/말\n",
    "    df['WLS'] = encoder.fit_transform(df['WLS']) #승패무 여부\n",
    "    \n",
    "    ### 이부분은 모든 EDA에서 동일하게 작성\n",
    "    \n",
    "    # GDAY_DS => Datetime type\n",
    "    df['GDAY_DS'] = df['GDAY_DS'].astype('string') + (df['HEADER_NO']+1).astype('string')\n",
    "        \n",
    "    dt = []\n",
    "    for i in df['GDAY_DS']:\n",
    "        dt_ = datetime.strptime(i, '%Y%m%d%H')\n",
    "        dt.append(dt_)\n",
    "    df['GDAY_DS'] = pd.Series(dt)\n",
    "    \n",
    "    # 시계열 데이터(경기당 방어율 계산)이기 때문에 경기 코드를 index로\n",
    "    df = df.set_index('GDAY_DS')\n",
    "    \n",
    "    df = df.drop(columns=['T_ID'])\n",
    "    \n",
    "    ###\n",
    "    \n",
    "\n",
    "    # 경기당 방어율 column 생성\n",
    "    df['ERA'] = df['ER'] * 27 / df['INN2']\n",
    "    \n",
    "    # Drop Categorical feature for 시계열 => coint 과정에서 singular matrix 발생\n",
    "    df = df.drop(columns=['VS_T_ID', 'HEADER_NO', 'TB_SC', 'CG_CK', 'WLS', 'HOLD', 'HOME_KEY'])\n",
    "    \n",
    "\n",
    "#     df = df.drop(columns=['T_ID', 'HEADER_NO', 'CG_CK', 'BK'])\n",
    "#     df = df.drop(columns=['TB_SC', 'HR', 'SB', 'VS_T_ID', 'HOME_KEY', 'HOLD', 'INN2', 'BF', 'CS', 'SH', 'HP', 'GD', 'ERR' ,'ER'])\n",
    "    df_year_team_name = df\n",
    "    \n",
    "    return df_year_team_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_hitter_eda(team_name):\n",
    "    \n",
    "    # team_pitcher data에 해당 경기의 해당 팀의 홈 어웨이 여부 column 추가\n",
    "    data_games_2016 = data_games[0]\n",
    "    data_games_2017 = data_games[1]\n",
    "    data_games_2018 = data_games[2]\n",
    "    data_games_2019 = data_games[3]\n",
    "    data_games_2020 = data_games[4]\n",
    "    data_games_year = pd.concat([data_games_2016,data_games_2017,data_games_2018,data_games_2019,data_games_2020])\n",
    "\n",
    "    data_games_year = data_games_year[['G_ID', 'VISIT_KEY', 'HOME_KEY']]\n",
    "    data_games_year = data_games_year.set_index('G_ID')\n",
    "    \n",
    "    data_team_hitter_2016 = data_team_hitter[0]\n",
    "    data_team_hitter_2017 = data_team_hitter[1]\n",
    "    data_team_hitter_2018 = data_team_hitter[2]\n",
    "    data_team_hitter_2019 = data_team_hitter[3]\n",
    "    data_team_hitter_2020 = data_team_hitter[4]\n",
    "    data_team_hitter_year = pd.concat([data_team_hitter_2016,data_team_hitter_2017,data_team_hitter_2018,\n",
    "                                        data_team_hitter_2019,data_team_hitter_2020])\n",
    "\n",
    "    data_team_hitter_year = pd.merge(data_team_hitter_year, data_games_year, how='left', on=['G_ID'])\n",
    "\n",
    "    data_team_hitter_year_team = data_team_hitter_year[data_team_hitter_year.T_ID == team_name]\n",
    "    df = data_team_hitter_year_team\n",
    "    df['HOME_KEY'] = df['HOME_KEY'].map(lambda x: 1 if x == team_name else 0 )\n",
    "    df = df.drop(columns = ['VISIT_KEY', 'G_ID'])\n",
    "    \n",
    "    df.reset_index(inplace=True, drop=True) #index 재정렬\n",
    "    \n",
    "    # LabelEncoding\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    df['VS_T_ID'] = encoder.fit_transform(df['VS_T_ID']) #상대편\n",
    "    \n",
    "    ### 이부분은 모든 EDA에서 동일하게 작성\n",
    "    \n",
    "    # GDAY_DS => Datetime type\n",
    "    df['GDAY_DS'] = df['GDAY_DS'].astype('string') + (df['HEADER_NO']+1).astype('string')\n",
    "        \n",
    "    dt = []\n",
    "    for i in df['GDAY_DS']:\n",
    "        dt_ = datetime.strptime(i, '%Y%m%d%H')\n",
    "        dt.append(dt_)\n",
    "    df['GDAY_DS'] = pd.Series(dt)\n",
    "    \n",
    "    # 시계열 데이터(경기당 방어율 계산)이기 때문에 경기 코드를 index로\n",
    "    df = df.set_index('GDAY_DS')\n",
    "    \n",
    "    df = df.drop(columns=['T_ID'])\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    df['H1']=df['HIT']-df['H2']-df['H3']-df['HR']\n",
    "    #장타율\n",
    "    df['SLG']=(df['HIT']+df['HP']+df['BB'])/(df['AB']+df['BB']+df['HP']+df['SF'])\n",
    "    #출루율\n",
    "    df['OBA']=(df['H1']+2*df['H2']+3*df['H3']+4*df['HR'])/df['AB']\n",
    "    df['W_OPS']=0.57* df['SLG']+0.43*df['OBA']\n",
    "    # 경기당 타율 column 생성\n",
    "    df['AVG'] = df['HIT'] / df['AB']\n",
    "    \n",
    "    df['avg_AVG'] = df['AVG'].mean()\n",
    "    df['AVG_trend'] = (df['AVG'] - df['avg_AVG']).astype(np.float16)\n",
    "    df.drop(['avg_AVG'],axis=1,inplace=True)\n",
    "    \n",
    "    # Drop Categorical feature for 시계열 => coint 과정에서 singular matrix 발생\n",
    "    df = df.drop(columns=['VS_T_ID', 'HEADER_NO', 'TB_SC', 'HOME_KEY'])\n",
    "    \n",
    "    df_year_team_name = df\n",
    "    \n",
    "    return df_year_team_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Var_automation:\n",
    "    def __init__(self, df, target, predict_range, mode):\n",
    "        super(Var_automation, self).__init__()\n",
    "        \n",
    "        self.mode = mode\n",
    "        if self.mode=='train':\n",
    "            self.df_set = df\n",
    "            self.predict_range = predict_range\n",
    "            self.train = self.df_set.iloc[:-self.predict_range]\n",
    "            self.test = self.df_set.iloc[-self.predict_range:]\n",
    "            self.target = target\n",
    "        \n",
    "        else:\n",
    "            zero_np = np.zeros((predict_range,len(df.columns)))\n",
    "            df_np = df.to_numpy()\n",
    "            df_concat = np.concatenate((df_np, zero_np))\n",
    "            self.df_set = pd.DataFrame(df_concat, columns = df.columns)\n",
    "\n",
    "            self.predict_range = predict_range\n",
    "            self.train = self.df_set.iloc[:-self.predict_range]\n",
    "            self.test = self.df_set.iloc[-self.predict_range:]\n",
    "            self.target = target\n",
    "\n",
    "    \n",
    "    def grangers_causation_matirx(self, test='ssr_chi2test', verbose=False): #인과관계 검정\n",
    "        variables = self.df_set.columns\n",
    "        maxlag = 12\n",
    "        \n",
    "        df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns = variables, index = variables)\n",
    "        for column in df.columns:\n",
    "            for index in df.index:\n",
    "                test_result = grangercausalitytests(self.df_set[[index,column]], maxlag=maxlag, verbose=False)\n",
    "                p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "                if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "                min_p_value = np.min(p_values)\n",
    "                df.loc[index, column] = min_p_value\n",
    "        df.columns = [var + '_x' for var in variables]\n",
    "        df.index = [var + '_y' for var in variables]\n",
    "        \n",
    "        target_p_value = df.loc['{}_y'.format(self.target)]\n",
    "        effective_var = []\n",
    "        for i, v in enumerate(target_p_value):\n",
    "            if v < 0.05:\n",
    "                effective_var.append(df.index[i].split('_y')[0])\n",
    "        \n",
    "        return df, effective_var\n",
    "    \n",
    "    def cointegration_test(self, alpha=0.05): #공적분 검정\n",
    "        df_grangers, var_grangers = self.grangers_causation_matirx()\n",
    "        var_coint = var_grangers\n",
    "        var_coint.append(self.target) #뒤에 넣어야 하는듯\n",
    "        df = self.df_set[var_coint] \n",
    "#         df = self.df_set[['ERA','H3']]\n",
    "        df_shape = df.shape\n",
    "                \n",
    "        try:\n",
    "            out = coint_johansen(df, -1, 5)\n",
    "        except:\n",
    "            df = df + 0.00001*np.random.rand(df_shape[0],df_shape[1]) #Singular Matrix Error Solution\n",
    "            out = coint_johansen(df, -1, 5)\n",
    "            \n",
    "        d = {'0.90': 0, '0.95': 1, '0.99': 2}\n",
    "        traces = out.lr1\n",
    "        cvts = out.cvt[:, d[str(1-alpha)]]\n",
    "        def adjust(val, length= 6): return str(val).ljust(length)    \n",
    "        \n",
    "        dict = {'Name': [], 'Test Stat': [], '> C(95%)': [], 'Signif': []}\n",
    "        effective_var = []\n",
    "\n",
    "        for col, trace, cvt in zip(df.columns, traces, cvts):\n",
    "            \n",
    "            dict['Name'].append(adjust(col))\n",
    "            dict['Test Stat'].append(adjust(round(trace,2), 9))\n",
    "            dict['> C(95%)'].append(adjust(cvt, 8))\n",
    "            dict['Signif'].append(trace > cvt)\n",
    "            \n",
    "            if trace>cvt:\n",
    "                effective_var.append(col)\n",
    "        \n",
    "        df_coint = pd.DataFrame(dict)                \n",
    "        \n",
    "        return df_coint, effective_var\n",
    "    \n",
    "    def adfuller_test(self, signif=0.05, verbose=False): #단위근 검정\n",
    "        df_coint, effective_var = self.cointegration_test()\n",
    "        effective_var.append(self.target)\n",
    "        \n",
    "        df_train = self.train[effective_var]\n",
    "        df_test = self.test[effective_var]\n",
    "        \n",
    "        def adjust(val, length=6): return str(val).ljust(length)\n",
    "        \n",
    "        adfuller_dict = {}\n",
    "        stationary_var = []\n",
    "        \n",
    "        for name, series in df_train.items():\n",
    "            name = series.name\n",
    "            \n",
    "            dict = {'adfuller_test var': [], 'Significance Level': [], 'Test Statistic': [], 'No. Lags Chosen': []}\n",
    "    \n",
    "            r = adfuller(series, autolag = 'AIC')\n",
    "            output = {'test_static': round(r[0], 4), 'pvalue': round(r[1], 4), 'n_lags': round(r[2], 4), 'n_obs': r[3]}\n",
    "            p_value = output['pvalue']\n",
    "            \n",
    "            dict['adfuller_test var'].append(name)\n",
    "            dict['Significance Level'].append(signif)\n",
    "            dict['Test Statistic'].append(output['test_static'])\n",
    "            dict['No. Lags Chosen'].append(output['n_lags'])\n",
    "            \n",
    "            for key, val in r[4].items():\n",
    "                dict['Critical value {}'.format(adjust(key))] = round(val, 3)\n",
    "                \n",
    "            if p_value <= signif:\n",
    "                dict['P-Value'] = '{} => Series is Stationary'.format(p_value)\n",
    "                stationary_var.append(name)\n",
    "            else:\n",
    "                dict['P-Value'] = '{} => Series is Non-Stationary'.format(p_value)\n",
    "            \n",
    "            adfuller_dict[name] = dict\n",
    "        \n",
    "        return adfuller_dict, stationary_var\n",
    "    \n",
    "    def aic_test(self, lag=8):#대부분 lag 범위를 10이상 주지 않는듯\n",
    "        dict_, effective_var = self.adfuller_test()\n",
    "        df_train = self.train[effective_var]\n",
    "        \n",
    "        model = sm.tsa.VAR(df_train)\n",
    "        \n",
    "        dict = {}\n",
    "        for i in range(1, lag+1, 1):\n",
    "            try:\n",
    "                result = model.fit(i)\n",
    "                dict[i] = result.aic\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "            \n",
    "        sorted_dict = sorted(dict.items(), key = lambda x: x[1])\n",
    "        min_aic = sorted_dict[0]\n",
    "        \n",
    "        min_lag = min_aic[0]\n",
    "        min_lag_aic = min_aic[1]\n",
    "        \n",
    "        return min_lag, min_lag_aic, model, effective_var\n",
    "    \n",
    "    def var_fit(self, model_summary=False):\n",
    "        min_lag, min_lag_aic, model, effective_var = self.aic_test()\n",
    "        model_fitted = model.fit(min_lag)\n",
    "        \n",
    "        if model_summary:\n",
    "            model_fitted.summary()\n",
    "            \n",
    "        return model_fitted, effective_var\n",
    "    \n",
    "    def durbin_watson_test(self):\n",
    "        model_fitted, effective_var = self.var_fit()\n",
    "        df_train = self.train[effective_var]\n",
    "        out = durbin_watson(model_fitted.resid)\n",
    "        \n",
    "        durbin_dict = {}\n",
    "        for col, val in zip(df_train.columns, out):\n",
    "            durbin_dict[col] = round(val,2)\n",
    "        \n",
    "        return durbin_dict, model_fitted, effective_var\n",
    "    \n",
    "    def run(self, nobs=20, displaying=False):\n",
    "        model_fitted, effective_var = self.var_fit()\n",
    "        df_train = self.train[effective_var]\n",
    "        df_test = self.test[effective_var]\n",
    "        \n",
    "        lag_order = model_fitted.k_ar\n",
    "        forecast_input = df_train.values[-lag_order:]\n",
    "        \n",
    "        fc = model_fitted.forecast(y=forecast_input, steps=nobs)\n",
    "        df_forecast = pd.DataFrame(fc, index = df_test.index, columns=effective_var)\n",
    "        \n",
    "#         mae_result = mean_absolute_error(df_test[self.target], df_forecast[self.target])  \n",
    "# => 우리가 알고 싶은 것은 target의 전체 평균의 차이\n",
    "        \n",
    "        \n",
    "        #display\n",
    "        if displaying:\n",
    "            \n",
    "            display(df_forecast)\n",
    "        \n",
    "            fig,axes = plt.subplots(nrows=int(len(effective_var)/2), ncols=2, dpi=150, figsize=(10,10))\n",
    "            for i, (col,ax) in enumerate(zip(effective_var, axes.flatten())):\n",
    "                df_test[col].plot(legend=True, ax=ax);\n",
    "                df_forecast[col].plot(legend=True, ax=ax).autoscale(axis='x', tight=True)\n",
    "\n",
    "                ax.set_title(col + \": Forecast vs Actuals\")\n",
    "                ax.xaxis.set_ticks_position('none')\n",
    "                ax.yaxis.set_ticks_position('none')\n",
    "                ax.spines[\"top\"].set_alpha(0)\n",
    "                ax.tick_params(labelsize=6)\n",
    "\n",
    "            plt.tight_layout()\n",
    "        true_mean = self.test[self.target].mean()\n",
    "        predict_mean = df_forecast[self.target].mean()\n",
    "        \n",
    "        return df_forecast[self.target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_SS = Var_automation(df_SS, 'ERA', 20, mode='train')\n",
    "var_SS.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM,Dropout,Bidirectional\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(signal_data, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(signal_data)-look_back):\n",
    "        dataX.append(signal_data[i:(i+look_back), 0])\n",
    "        dataY.append(signal_data[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lstm_model:\n",
    "    \n",
    "    def __init__(self, df, target, look_back, predict_range, mode):\n",
    "        self.df = df[[target]]\n",
    "        self.target = target\n",
    "        self.look_back = 10\n",
    "        self.predict_range = predict_range\n",
    "        self.mode = mode\n",
    "        \n",
    "    def univariate(self):\n",
    "        if self.mode == 'train':\n",
    "            signal_data =self.df\n",
    "            # 데이터 전처리\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            signal_data = scaler.fit_transform(signal_data)\n",
    "\n",
    "            # 데이터 분리\n",
    "            train = signal_data[0:-self.predict_range]\n",
    "            test = signal_data[-self.predict_range:]\n",
    "\n",
    "            # 데이터셋 생성\n",
    "            x_train, y_train = create_dataset(train, self.look_back)\n",
    "            x_test, y_test = create_dataset(test, self.look_back)\n",
    "\n",
    "            x_train = np.reshape(x_train,(x_train.shape[0],self.look_back,1)) #(size, timestamp,feature)\n",
    "            x_test = np.reshape(x_test,(x_test.shape[0],self.look_back,1))\n",
    "\n",
    "            return x_train, x_test, y_train, y_test,scaler, self.look_back\n",
    "        \n",
    "        else:\n",
    "            signal_data = self.df\n",
    "            # 데이터 전처리\n",
    "\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            signal_data = scaler.fit_transform(signal_data)\n",
    "            last_data = signal_data[-self.look_back:]\n",
    "            # 데이터셋 생성\n",
    "            x_train, y_train = create_dataset(signal_data, look_back)\n",
    "\n",
    "            x_train = np.reshape(x_train,(x_train.shape[0],look_back,1)) #(size, timestamp,feature)\n",
    "            #x_test = np.reshape(x_test,(x_test.shape[0],look_back,1))\n",
    "\n",
    "            return x_train, y_train,scaler, self.look_back, last_data\n",
    "    \n",
    "    def run_stateful_stack(self, x_train, x_test, y_train, y_test, look_back, scaler,box1,box2):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(box1, batch_input_shape=(1, look_back, 1), stateful=True,return_sequences=True))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(LSTM(box2, batch_input_shape=(1, look_back, 1), stateful=True))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='loss', patience=3, verbose=1)\n",
    "\n",
    "        model.fit(x_train, y_train, epochs=100, batch_size=1, verbose=1, callbacks=[early_stop])\n",
    "        score = model.evaluate(x_test,y_test,batch_size=1)\n",
    "        pre =model.predict(x_test,batch_size=1) #모든 batch_size 바꾸면 error\n",
    "        #sc_pre = scaler.inverse_transform(pre)\n",
    "        ans = y_test\n",
    "\n",
    "        return score, pre, ans\n",
    "    \n",
    "    def run_stateful_ans(self, x_train,  y_train, look_back,box):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(box, batch_input_shape=(1, look_back, 1), stateful=True))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='loss', patience=3, verbose=1)\n",
    "\n",
    "        model.fit(x_train, y_train, epochs=100, batch_size=1, verbose=1, callbacks=[early_stop])\n",
    "        return model\n",
    "    \n",
    "    def run(self):\n",
    "        if self.mode == 'train':\n",
    "            x_train, x_test, y_train, y_test,scaler,look_back = self.univariate()\n",
    "            score, pre, ans = self.run_stateful_stack(x_train, x_test, y_train, y_test,look_back, scaler,10,10)\n",
    "            sc_pre=scaler.inverse_transform(pre)\n",
    "            sc_ans=scaler.inverse_transform(ans.reshape(ans.shape[0],1))\n",
    "            predict_mean = sc_pre.mean()\n",
    "            real_mean = sc_ans.mean()\n",
    "        \n",
    "            return predict_mean, real_mean\n",
    "    \n",
    "        else:\n",
    "            x_train, y_train,scaler, look_back,last_data = self.univariate()\n",
    "            model = self.run_stateful_ans(x_train,  y_train, look_back,10)\n",
    "            pred_date = self.predict_range\n",
    "            seq_in = [i[0] for i in last_data]\n",
    "            seq_out = []\n",
    "            for i in range(pred_date):\n",
    "                #sample_in = np.array(seq_in)\n",
    "                sample_in = np.reshape(seq_in, (1, look_back,1)) # batch_size, feature\n",
    "                pred_out = model.predict(sample_in)[0][0]\n",
    "                seq_out.append(pred_out)\n",
    "                seq_in.append(pred_out)\n",
    "                seq_in.pop(0)\n",
    "            ans = scaler.inverse_transform(np.array(seq_out).reshape(pred_date,1))\n",
    "            ans_mean=ans.mean()\n",
    "            return ans_mean\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Lstm_model(df_SS, 'ERA', 10, 20, mode='inference')\n",
    "obj.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dnn_score:\n",
    "    def __init__(self,AVG,ERA,team,hiter,pitcher,mode):\n",
    "        self.AVG =AVG\n",
    "        self.ERA =ERA\n",
    "        self.team = team\n",
    "        self.hiter = hiter\n",
    "        self.pitcher = pitcher\n",
    "        self.mode = mode\n",
    "        \n",
    "    def prepro(self, hiter, pitcher): #전처리\n",
    "        \n",
    "        hiter['AVG']= hiter['HIT']/hiter['AB']\n",
    "        hiter[\"y_m\"]=hiter.GDAY_DS.astype(str).str[2:6]\n",
    "        hiter=hiter[[\"T_ID\",\"AVG\",'y_m']]\n",
    "        #del hiter['HIT'], hiter['AB'], hiter[\"GDAY_DS\"]\n",
    "    \n",
    "        pitcher[\"ERA\"] = 27*pitcher.ER/pitcher.INN2\n",
    "        pitcher.WLS = pitcher.WLS.map({'W':1,'D':0.5,\"L\":0})\n",
    "        pitcher[\"y_m\"]=pitcher.GDAY_DS.astype(str).str[2:6]\n",
    "        pitcher=pitcher[[\"T_ID\",\"WLS\",\"ERA\",\"y_m\"]]\n",
    "        #del pitcher[\"ER\"], pitcher[\"INN2\"],pitcher[\"GDAY_DS\"]\n",
    "    \n",
    "        return hiter, pitcher\n",
    "    \n",
    "    def make_data(self):\n",
    "        hiter, pitcher = self.prepro(self.hiter, self.pitcher)\n",
    "        team_list = ['WO','OB','NC','SK','LG','KT','LT','HT','SS','HH']\n",
    "        df_mean=pd.DataFrame()\n",
    "        code=0\n",
    "        for team in team_list:\n",
    "            code +=0.1\n",
    "            pitcher_team= pitcher[pitcher.T_ID==team]\n",
    "            hiter_team=hiter[hiter.T_ID==team]\n",
    "            pitcher_mean = pitcher_team.groupby(['y_m']).mean()\n",
    "            pitcher_mean[\"count\"] =pitcher_team.groupby(['y_m']).count().T_ID\n",
    "            hiter_mean = hiter_team.groupby(['y_m']).mean()\n",
    "            df = pd.concat([pitcher_mean, hiter_mean], axis=1)\n",
    "            df[\"team_code\"] = code\n",
    "            df_mean=pd.concat([df_mean,df])\n",
    "    \n",
    "        df_mean.reset_index(inplace=True, drop=True)\n",
    "        df_mean = df_mean[df_mean[\"count\"] > 9]\n",
    "        df_mean.reset_index(inplace=True, drop=True)\n",
    "        del df_mean[\"count\"]\n",
    "        \n",
    "        sc_era = MinMaxScaler()\n",
    "        sc_avg = MinMaxScaler()\n",
    "        df_mean[\"AVG\"]=sc_avg.fit_transform(df_mean[[\"AVG\"]])\n",
    "        df_mean[\"ERA\"]=1-sc_era.fit_transform(df_mean[[\"ERA\"]])\n",
    "\n",
    "        df_mean=df_mean.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            x_train=df_mean.iloc[:240,1:]\n",
    "            y_train=df_mean.iloc[:240,0]\n",
    "            x_test=df_mean.iloc[240:,1:]\n",
    "            y_test=df_mean.iloc[240:,0]\n",
    "            return x_train, y_train,x_test,y_test, sc_era,sc_avg\n",
    "        \n",
    "        else:\n",
    "            x_train=df_mean.iloc[:,1:]\n",
    "            y_train=df_mean.iloc[:,0]\n",
    "            \n",
    "            return x_train, y_train, sc_era,sc_avg\n",
    "    \n",
    "    def run(self):\n",
    "        if self.mode == \"train\":\n",
    "            x_train, y_train,x_test,y_test, sc_era,sc_avg = self.make_data()\n",
    "        else:\n",
    "            x_train, y_train, sc_era,sc_avg = self.make_data()\n",
    "        model=Sequential()\n",
    "        model.add(Dense(20,activation='relu',input_dim=3))\n",
    "        model.add(Dense(5,activation='relu'))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "        #model.reset_states()\n",
    "        model.fit(x_train, y_train, epochs=100, validation_split=0.15,callbacks=[early_stop])\n",
    "        \n",
    "        team_map = {'WO':0.1,'OB':0.2,'NC':0.3,'SK':0.4,'LG':0.5,'KT':0.6,'LT':0.7,'HT':0.8,'SS':0.9,'HH':1.0}\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            evaluate = model.evaluate(x_test, y_test)\n",
    "            pre=model.predict(x_test)\n",
    "            ny_test=y_test.values.reshape(y_test.shape[0],1)\n",
    "            print(evaluate)\n",
    "            print(np.concatenate((pre,ny_test),axis=1))\n",
    "        else:\n",
    "            final_df=pd.DataFrame({\"AVG\":[self.AVG],\"ERA\":[self.ERA],\"team_code\":[team_map[self.team]]})\n",
    "            final_df[\"AVG\"] = sc_avg.transform(final_df[[\"AVG\"]])\n",
    "            final_df[\"ERA\"] = 1-sc_era.transform(final_df[[\"ERA\"]])\n",
    "            win_rate= model.predict(final_df)\n",
    "        \n",
    "            return win_rate[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ERA_ensemble(team, predict_range):\n",
    "    \n",
    "    var_df = var_pitcher_eda(team)\n",
    "    \n",
    "    obj1 = LGBM_pitcher(lgbm_pitcher_eda, team, predict_range)\n",
    "    model1_predict = obj1.run()\n",
    "    \n",
    "    obj2 = Var_automation(var_df, 'ERA', predict_range, mode='inference')\n",
    "    model2_predict = obj2.run()\n",
    "    \n",
    "    obj3 = Lstm_model(var_df, 'ERA', 10, predict_range, mode='inference')\n",
    "    model3_predict = obj3.run()\n",
    "    \n",
    "    past_mean = var_df['ERA'].mean()\n",
    "    \n",
    "    final_predict = (model1_predict+model2_predict+model3_predict+past_mean) / 4\n",
    "    \n",
    "    return final_predict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ERA_ensemble('SS', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AVG_ensemble(team, predict_range):\n",
    "    \n",
    "    var_df = var_hitter_eda(team)\n",
    "    \n",
    "    obj1 = LGBM_hitter(lgbm_hitter_eda, team, predict_range)\n",
    "    model1_predict = obj1.run()\n",
    "    \n",
    "    obj2 = Var_automation(var_df, 'AVG', predict_range, mode='inference')\n",
    "    model2_predict = obj2.run()\n",
    "    \n",
    "    obj3 = Lstm_model(var_df, 'AVG', 10, predict_range, mode='inference')\n",
    "    model3_predict = obj3.run()\n",
    "    \n",
    "    past_mean = var_df['AVG'].mean()\n",
    "    \n",
    "    final_predict = (model1_predict+model2_predict+model3_predict+past_mean) / 4\n",
    "    \n",
    "    return final_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVG_ensemble('SS', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odds_ensemble(team, predict_range):\n",
    "    \n",
    "    var_hitter_df = var_hitter_eda(team)\n",
    "    var_pitcher_df = var_pitcher_eda(team)\n",
    "    \n",
    "    hitter_df = pd.concat([data_team_hitter[0],data_team_hitter[1],data_team_hitter[2],data_team_hitter[3],data_team_hitter[4]])\n",
    "    pitcher_df = pd.concat([data_team_pitcher[0],data_team_pitcher[1],data_team_pitcher[2],data_team_pitcher[3],data_team_pitcher[4]])\n",
    "    \n",
    "    model1_predict = lgbm_odds_predict(team, predict_range)\n",
    "    print(model1_predict)\n",
    "    \n",
    "    lstm_ERA_obj = Lstm_model(var_hitter_df, 'AVG', 10, predict_range, mode='inference')\n",
    "    lstm_ERA= lstm_ERA_obj.run()\n",
    "    \n",
    "    lstm_AVG_obj = Lstm_model(var_pitcher_df, 'ERA', 10, predict_range, mode='inference')\n",
    "    lstm_AVG = lstm_AVG_obj.run()\n",
    "    \n",
    "    model2_obj = dnn_score(0.286249, 3.9050, team, hitter_df ,pitcher_df,mode='inference')\n",
    "    model2_predict = model2_obj.run()\n",
    "    print(model2_predict)\n",
    "        \n",
    "    final_predict = (model1_predict+model2_predict) / 2\n",
    "    \n",
    "    return final_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_ensemble('SS', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
