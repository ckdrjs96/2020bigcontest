{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import joblib \n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime \n",
    "import pickle\n",
    "\n",
    "# 회귀분석\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from math import sqrt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:/Users/nsd96/Desktop/빅콘테스트/2020빅콘테스트 문제데이터(데이터분석분야-퓨처스리그)/2020빅콘테스트 데이터분석분야-퓨쳐스리그_스포츠투아이_제공데이터(.CSV)_시즌별, 시트별 구분'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "['2020빅콘테스트_스포츠투아이_제공데이터_개인타자_2016.csv', '2020빅콘테스트_스포츠투아이_제공데이터_개인타자_2017.csv', '2020빅콘테스트_스포츠투아이_제공데이터_개인타자_2018.csv', '2020빅콘테스트_스포츠투아이_제공데이터_개인타자_2019.csv', '2020빅콘테스트_스포츠투아이_제공데이터_개인타자_2020.csv', '2020빅콘테스트_스포츠투아이_제공데이터_개인투수_2016.csv', '2020빅콘테스트_스포츠투아이_제공데이터_개인투수_2017.csv', '2020빅콘테스트_스포츠투아이_제공데이터_개인투수_2018.csv', '2020빅콘테스트_스포츠투아이_제공데이터_개인투수_2019.csv', '2020빅콘테스트_스포츠투아이_제공데이터_개인투수_2020.csv', '2020빅콘테스트_스포츠투아이_제공데이터_경기_2016.csv', '2020빅콘테스트_스포츠투아이_제공데이터_경기_2017.csv', '2020빅콘테스트_스포츠투아이_제공데이터_경기_2018.csv', '2020빅콘테스트_스포츠투아이_제공데이터_경기_2019.csv', '2020빅콘테스트_스포츠투아이_제공데이터_경기_2020.csv', '2020빅콘테스트_스포츠투아이_제공데이터_등록선수_2016.csv', '2020빅콘테스트_스포츠투아이_제공데이터_등록선수_2017.csv', '2020빅콘테스트_스포츠투아이_제공데이터_등록선수_2018.csv', '2020빅콘테스트_스포츠투아이_제공데이터_등록선수_2019.csv', '2020빅콘테스트_스포츠투아이_제공데이터_등록선수_2020.csv', '2020빅콘테스트_스포츠투아이_제공데이터_선수_2016.csv', '2020빅콘테스트_스포츠투아이_제공데이터_선수_2017.csv', '2020빅콘테스트_스포츠투아이_제공데이터_선수_2018.csv', '2020빅콘테스트_스포츠투아이_제공데이터_선수_2019.csv', '2020빅콘테스트_스포츠투아이_제공데이터_선수_2020.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀_2016.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀_2017.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀_2018.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀_2019.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀_2020.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀타자_2016.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀타자_2017.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀타자_2018.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀타자_2019.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀타자_2020.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀투수_2016.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀투수_2017.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀투수_2018.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀투수_2019.csv', '2020빅콘테스트_스포츠투아이_제공데이터_팀투수_2020.csv']\n"
     ]
    }
   ],
   "source": [
    "data_list = os.listdir(data_dir)\n",
    "print(len(data_list))\n",
    "print(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020년 데이터: \n",
      " [               G_ID   GDAY_DS T_ID VS_T_ID  HEADER_NO TB_SC   P_ID  START_CK  \\\n",
      "0     20200505HHSK0  20200505   HH      SK          0     T  61208         1   \n",
      "1     20200505HHSK0  20200505   SK      HH          0     B  61353         1   \n",
      "2     20200505HHSK0  20200505   HH      SK          0     T  62700         1   \n",
      "3     20200505HHSK0  20200505   SK      HH          0     B  62895         1   \n",
      "4     20200505HHSK0  20200505   SK      HH          0     B  63450         1   \n",
      "...             ...       ...  ...     ...        ...   ...    ...       ...   \n",
      "8055  20200719WOSK0  20200719   SK      WO          0     B  75847         1   \n",
      "8056  20200719WOSK0  20200719   SK      WO          0     B  76802         1   \n",
      "8057  20200719WOSK0  20200719   SK      WO          0     B  77463         0   \n",
      "8058  20200719WOSK0  20200719   WO      SK          0     T  78168         1   \n",
      "8059  20200719WOSK0  20200719   WO      SK          0     T  79456         1   \n",
      "\n",
      "      BAT_ORDER_NO  PA  ...  BB  IB  HP  KK  GD  ERR  LOB  P_HRA_RT  P_AB_CN  \\\n",
      "0                7   4  ...   0   0   0   0   0    0    0       0.0        1   \n",
      "1                2   4  ...   0   0   0   0   0    0    1       0.0        0   \n",
      "2                9   3  ...   0   0   0   1   1    0    1       0.5        2   \n",
      "3                5   3  ...   0   0   0   1   0    0    0       0.0        1   \n",
      "4                9   2  ...   0   0   0   1   0    0    0       0.0        0   \n",
      "...            ...  ..  ...  ..  ..  ..  ..  ..  ...  ...       ...      ...   \n",
      "8055             3   4  ...   0   0   0   1   0    0    1       0.0        1   \n",
      "8056             9   4  ...   1   0   0   0   0    0    4       1.0        1   \n",
      "8057             6   1  ...   1   0   0   0   0    0    0       0.0        0   \n",
      "8058             1   5  ...   0   0   0   0   1    0    0       0.0        1   \n",
      "8059             6   4  ...   0   0   0   0   0    0    0       0.0        0   \n",
      "\n",
      "      P_HIT_CN  \n",
      "0            0  \n",
      "1            0  \n",
      "2            1  \n",
      "3            0  \n",
      "4            0  \n",
      "...        ...  \n",
      "8055         0  \n",
      "8056         1  \n",
      "8057         0  \n",
      "8058         0  \n",
      "8059         0  \n",
      "\n",
      "[8060 rows x 31 columns],                G_ID   GDAY_DS T_ID VS_T_ID  HEADER_NO TB_SC   P_ID  START_CK  \\\n",
      "0     20200505HHSK0  20200505   SK      HH          0     B  50815         1   \n",
      "1     20200505HHSK0  20200505   SK      HH          0     B  63894         0   \n",
      "2     20200505HHSK0  20200505   HH      SK          0     T  69744         1   \n",
      "3     20200505HHSK0  20200505   SK      HH          0     B  76350         0   \n",
      "4     20200505LTKT0  20200505   KT      LT          0     B  50040         1   \n",
      "...             ...       ...  ...     ...        ...   ...    ...       ...   \n",
      "2882  20200719WOSK0  20200719   WO      SK          0     T  67313         1   \n",
      "2883  20200719WOSK0  20200719   WO      SK          0     T  67391         0   \n",
      "2884  20200719WOSK0  20200719   WO      SK          0     T  68341         0   \n",
      "2885  20200719WOSK0  20200719   WO      SK          0     T  69399         0   \n",
      "2886  20200719WOSK0  20200719   SK      WO          0     B  75138         0   \n",
      "\n",
      "      RELIEF_CK  CG_CK  ...  KK GD  WP  BK  ERR  R  ER  P_WHIP_RT  P2_WHIP_RT  \\\n",
      "0             0      0  ...   4  1   0   0    0  3   3        2.0    1.200000   \n",
      "1             1      0  ...   1  0   0   0    0  0   0        0.0    0.000000   \n",
      "2             0      1  ...   2  0   0   0    0  0   0        0.0    0.000000   \n",
      "3             1      0  ...   0  0   0   0    0  0   0        0.0    0.000000   \n",
      "4             0      0  ...   8  1   0   0    0  1   1        0.0    0.666667   \n",
      "...         ...    ...  ...  .. ..  ..  ..  ... ..  ..        ...         ...   \n",
      "2882          0      0  ...   7  0   1   0    0  0   0        0.0    1.000000   \n",
      "2883          1      0  ...   1  0   0   0    0  0   0        0.0    0.000000   \n",
      "2884          1      0  ...   2  0   1   0    0  4   4        0.0    0.000000   \n",
      "2885          1      0  ...   0  0   0   0    0  0   0        3.0    3.000000   \n",
      "2886          1      0  ...   0  0   0   0    0  0   0        0.0    0.000000   \n",
      "\n",
      "      CB_WHIP_RT  \n",
      "0       0.857143  \n",
      "1       1.500000  \n",
      "2       0.750000  \n",
      "3       0.000000  \n",
      "4       1.000000  \n",
      "...          ...  \n",
      "2882    2.250000  \n",
      "2883    0.000000  \n",
      "2884    1.500000  \n",
      "2885    0.000000  \n",
      "2886    0.000000  \n",
      "\n",
      "[2887 rows x 38 columns],               G_ID   GDAY_DS VISIT_KEY HOME_KEY  HEADER_NO GWEEK STADIUM\n",
      "0    20200505HHSK0  20200505        HH       SK          0     화      문학\n",
      "1    20200505LTKT0  20200505        LT       KT          0     화      수원\n",
      "2    20200505NCSS0  20200505        NC       SS          0     화      대구\n",
      "3    20200505OBLG0  20200505        OB       LG          0     화      잠실\n",
      "4    20200505WOHT0  20200505        WO       HT          0     화      광주\n",
      "..             ...       ...       ...      ...        ...   ...     ...\n",
      "315  20200719HHLG0  20200719        HH       LG          0     일      잠실\n",
      "316  20200719KTNC0  20200719        KT       NC          0     일      창원\n",
      "317  20200719LTSS0  20200719        LT       SS          0     일      대구\n",
      "318  20200719OBHT0  20200719        OB       HT          0     일      광주\n",
      "319  20200719WOSK0  20200719        WO       SK          0     일      문학\n",
      "\n",
      "[320 rows x 7 columns],         GDAY_DS T_ID   P_ID ENTRY_YN\n",
      "0      20200505   HH  62797        N\n",
      "1      20200505   HH  63464        N\n",
      "2      20200505   HH  63700        Y\n",
      "3      20200505   HH  63703        N\n",
      "4      20200505   HH  63765        N\n",
      "...         ...  ...    ...      ...\n",
      "45555  20200719   LG  79140        Y\n",
      "45556  20200719   LG  79150        Y\n",
      "45557  20200719   LG  79192        N\n",
      "45558  20200719   LG  79617        N\n",
      "45559  20200719   LG  79825        Y\n",
      "\n",
      "[45560 rows x 4 columns],      GYEAR  PCODE   NAME T_ID POSITION  AGE_VA     MONEY\n",
      "0     2020  50030    소형준   KT        투      19    2700만원\n",
      "1     2020  50036    이강준   KT        투      19    2700만원\n",
      "2     2020  50040  데스파이네   KT        투      33  450000달러\n",
      "3     2020  50054    천성호   KT        내      23    2700만원\n",
      "4     2020  50066    강현우   KT        포      19    2700만원\n",
      "..     ...    ...    ...  ...      ...     ...       ...\n",
      "622   2020  79705    김회성   HH        내      35    6500만원\n",
      "623   2020  79764    장민재   HH        투      30   11000만원\n",
      "624   2020  79825    여건욱   LG        투      34    7000만원\n",
      "625   2020  79847    김태훈   SK        투      30   24000만원\n",
      "626   2020  99445    권오준   SS        투      40    8000만원\n",
      "\n",
      "[627 rows x 7 columns],   T_ID T_NM\n",
      "0   HH   한화\n",
      "1   HT  KIA\n",
      "2   KT   KT\n",
      "3   LG   LG\n",
      "4   LT   롯데\n",
      "5   NC   NC\n",
      "6   OB   두산\n",
      "7   SK   SK\n",
      "8   SS   삼성\n",
      "9   WO   키움,               G_ID   GDAY_DS T_ID VS_T_ID  HEADER_NO TB_SC  PA  AB  RBI  RUN  \\\n",
      "0    20200505HHSK0  20200505   SK      HH          0     B  30  29    0    0   \n",
      "1    20200505HHSK0  20200505   HH      SK          0     T  35  31    3    3   \n",
      "2    20200505LTKT0  20200505   KT      LT          0     B  35  30    2    2   \n",
      "3    20200505LTKT0  20200505   LT      KT          0     T  37  32    7    7   \n",
      "4    20200505NCSS0  20200505   SS      NC          0     B  36  30    0    0   \n",
      "..             ...       ...  ...     ...        ...   ...  ..  ..  ...  ...   \n",
      "635  20200719LTSS0  20200719   LT      SS          0     T  31  29    2    2   \n",
      "636  20200719OBHT0  20200719   HT      OB          0     B  38  32    4    4   \n",
      "637  20200719OBHT0  20200719   OB      HT          0     T  41  37    8    8   \n",
      "638  20200719WOSK0  20200719   SK      WO          0     B  37  30    3    4   \n",
      "639  20200719WOSK0  20200719   WO      SK          0     T  37  33    3    3   \n",
      "\n",
      "     ...  BB  IB  HP  KK  GD  ERR  LOB  P_HRA_RT  P_AB_CN  P_HIT_CN  \n",
      "0    ...   1   0   0   2   0    0    3  0.000000        1         0  \n",
      "1    ...   2   0   0   5   1    0    5  0.250000        8         2  \n",
      "2    ...   5   0   0   8   1    0    6  0.000000        4         0  \n",
      "3    ...   4   0   0   9   1    1    3  0.666667        3         2  \n",
      "4    ...   6   0   0   8   0    0    9  0.000000        5         0  \n",
      "..   ...  ..  ..  ..  ..  ..  ...  ...       ...      ...       ...  \n",
      "635  ...   1   0   0   7   3    0    2  0.000000        2         0  \n",
      "636  ...   6   0   0   5   1    0    7  0.571429        7         4  \n",
      "637  ...   2   1   2   4   0    0    6  0.363636       11         4  \n",
      "638  ...   7   0   0  10   0    0    9  0.250000        8         2  \n",
      "639  ...   3   0   1   4   1    0    7  0.333333        3         1  \n",
      "\n",
      "[640 rows x 28 columns],               G_ID   GDAY_DS T_ID VS_T_ID  HEADER_NO TB_SC  CG_CK WLS  HOLD  \\\n",
      "0    20200505HHSK0  20200505   SK      HH          0     B      0   L     0   \n",
      "1    20200505HHSK0  20200505   HH      SK          0     T      1   W     0   \n",
      "2    20200505LTKT0  20200505   KT      LT          0     B      0   L     0   \n",
      "3    20200505LTKT0  20200505   LT      KT          0     T      0   W     0   \n",
      "4    20200505NCSS0  20200505   SS      NC          0     B      0   L     0   \n",
      "..             ...       ...  ...     ...        ...   ...    ...  ..   ...   \n",
      "635  20200719LTSS0  20200719   LT      SS          0     T      0   W     2   \n",
      "636  20200719OBHT0  20200719   HT      OB          0     B      0   L     0   \n",
      "637  20200719OBHT0  20200719   OB      HT          0     T      0   W     1   \n",
      "638  20200719WOSK0  20200719   SK      WO          0     B      0   W     0   \n",
      "639  20200719WOSK0  20200719   WO      SK          0     T      0   L     2   \n",
      "\n",
      "     INN2  ...  KK  GD  WP  BK  ERR  R  ER  P_WHIP_RT  P2_WHIP_RT  CB_WHIP_RT  \n",
      "0      27  ...   5   1   0   0    0  3   3   1.500000    1.200000    1.333333  \n",
      "1      27  ...   2   0   0   0    0  0   0   0.000000    0.000000    0.750000  \n",
      "2      27  ...   9   1   0   0    0  7   7   3.000000    1.285714    1.875000  \n",
      "3      27  ...   8   1   0   0    0  2   2   0.000000    1.000000    2.571429  \n",
      "4      27  ...   9   0   0   0    0  4   4   1.500000    1.058824    2.142857  \n",
      "..    ...  ...  ..  ..  ..  ..  ... ..  ..        ...         ...         ...  \n",
      "635    27  ...   9   1   0   0    0  1   1   0.600000    1.111111    1.500000  \n",
      "636    27  ...   4   0   0   0    0  8   8   2.142857    1.800000    0.750000  \n",
      "637    27  ...   5   1   0   0    0  4   4   3.000000    1.000000    1.875000  \n",
      "638    27  ...   4   1   0   0    0  3   3   1.500000    1.105263    2.142857  \n",
      "639    24  ...  10   0   2   0    0  4   4   1.800000    1.312500    1.333333  \n",
      "\n",
      "[640 rows x 34 columns]]\n",
      "팀투수 데이터:\n",
      " [               G_ID   GDAY_DS T_ID VS_T_ID  HEADER_NO TB_SC  CG_CK WLS  HOLD  \\\n",
      "0     20160401HHLG0  20160401   LG      HH          0     B      0   W     0   \n",
      "1     20160401HHLG0  20160401   HH      LG          0     T      0   L     0   \n",
      "2     20160401HTNC0  20160401   NC      HT          0     B      0   W     0   \n",
      "3     20160401HTNC0  20160401   HT      NC          0     T      0   L     0   \n",
      "4     20160401KTSK0  20160401   SK      KT          0     B      0   L     0   \n",
      "...             ...       ...  ...     ...        ...   ...    ...  ..   ...   \n",
      "1435  20161008SSSK0  20161008   SS      SK          0     T      0   L     0   \n",
      "1436  20161009KTNC0  20161009   NC      KT          0     B      0   L     0   \n",
      "1437  20161009KTNC0  20161009   KT      NC          0     T      0   W     1   \n",
      "1438  20161009WOLT0  20161009   LT      WO          0     B      0   W     1   \n",
      "1439  20161009WOLT0  20161009   WO      LT          0     T      0   L     0   \n",
      "\n",
      "      INN2  ...  KK  GD  WP  BK  ERR  R  ER  P_WHIP_RT  P2_WHIP_RT  CB_WHIP_RT  \n",
      "0       36  ...  10   1   0   0    0  4   4   0.642857    1.285714    2.400000  \n",
      "1       34  ...  11   0   1   0    0  5   4   1.500000    1.000000    0.750000  \n",
      "2       27  ...  10   1   2   0    0  4   4   1.333333    1.038462    2.142857  \n",
      "3       24  ...   9   1   0   0    0  5   5   0.500000    1.695652    1.875000  \n",
      "4       27  ...   7   0   0   0    0  8   8   1.000000    2.357143    2.250000  \n",
      "...    ...  ...  ..  ..  ..  ..  ... ..  ..        ...         ...         ...  \n",
      "1435    24  ...   9   1   0   0    0  7   6   1.500000    2.538462    1.666667  \n",
      "1436    27  ...   7   0   2   1    0  7   7   0.900000    2.217391    1.500000  \n",
      "1437    27  ...  13   0   0   0    1  4   3   0.562500    1.428571    1.090909  \n",
      "1438    27  ...   6   1   1   0    0  5   5   1.500000    1.500000    1.000000  \n",
      "1439    24  ...   4   2   1   0    0  8   4   2.250000    1.666667    3.000000  \n",
      "\n",
      "[1440 rows x 34 columns],                G_ID   GDAY_DS T_ID VS_T_ID  HEADER_NO TB_SC  CG_CK WLS  HOLD  \\\n",
      "0     20170331HHOB0  20170331   OB      HH          0     B      0   W     0   \n",
      "1     20170331HHOB0  20170331   HH      OB          0     T      0   L     0   \n",
      "2     20170331HTSS0  20170331   SS      HT          0     B      0   L     0   \n",
      "3     20170331HTSS0  20170331   HT      SS          0     T      0   W     0   \n",
      "4     20170331KTSK0  20170331   SK      KT          0     B      0   L     0   \n",
      "...             ...       ...  ...     ...        ...   ...    ...  ..   ...   \n",
      "1435  20171003NCHH0  20171003   NC      HH          0     T      0   D     0   \n",
      "1436  20171003SKOB0  20171003   OB      SK          0     B      0   L     1   \n",
      "1437  20171003SKOB0  20171003   SK      OB          0     T      0   W     2   \n",
      "1438  20171003WOSS0  20171003   SS      WO          0     B      0   W     0   \n",
      "1439  20171003WOSS0  20171003   WO      SS          0     T      0   L     0   \n",
      "\n",
      "      INN2  ...  KK  GD  WP  BK  ERR   R  ER  P_WHIP_RT  P2_WHIP_RT  \\\n",
      "0       27  ...   8   0   1   0    0   0   0   0.600000    0.857143   \n",
      "1       24  ...   8   0   0   0    0   3   1   0.600000    0.450000   \n",
      "2       27  ...   5   0   1   0    0   7   6   2.000000    1.363636   \n",
      "3       27  ...   7   1   1   0    0   2   2   0.000000    0.857143   \n",
      "4       27  ...   8   1   0   0    1   3   2   1.200000    1.222222   \n",
      "...    ...  ...  ..  ..  ..  ..  ...  ..  ..        ...         ...   \n",
      "1435    36  ...   6   0   1   0    0   8   7   2.000000    1.031250   \n",
      "1436    27  ...   8   0   0   0    0   3   2   1.500000    1.222222   \n",
      "1437    27  ...   5   1   0   0    0   2   2   1.200000    1.000000   \n",
      "1438    27  ...  12   1   1   0    0   9   8   1.666667    2.000000   \n",
      "1439    24  ...   5   0   0   0    0  10   6   2.571429    2.538462   \n",
      "\n",
      "      CB_WHIP_RT  \n",
      "0       1.200000  \n",
      "1       0.272727  \n",
      "2       3.428571  \n",
      "3       1.333333  \n",
      "4       0.818182  \n",
      "...          ...  \n",
      "1435    3.000000  \n",
      "1436    1.500000  \n",
      "1437    0.600000  \n",
      "1438    4.500000  \n",
      "1439    2.000000  \n",
      "\n",
      "[1440 rows x 34 columns],                G_ID   GDAY_DS T_ID VS_T_ID  HEADER_NO TB_SC  CG_CK WLS  HOLD  \\\n",
      "0     20180324HHWO0  20180324   WO      HH          0     B      0   W     2   \n",
      "1     20180324HHWO0  20180324   HH      WO          0     T      0   L     0   \n",
      "2     20180324KTHT0  20180324   HT      KT          0     B      0   L     0   \n",
      "3     20180324KTHT0  20180324   KT      HT          0     T      0   W     2   \n",
      "4     20180324LGNC0  20180324   NC      LG          0     B      0   W     2   \n",
      "...             ...       ...  ...     ...        ...   ...    ...  ..   ...   \n",
      "1435  20181013NCHH0  20181013   NC      HH          0     T      0   L     0   \n",
      "1436  20181013WOSS0  20181013   SS      WO          0     B      0   W     0   \n",
      "1437  20181013WOSS0  20181013   WO      SS          0     T      0   L     0   \n",
      "1438  20181014OBLT0  20181014   LT      OB          0     B      0   L     1   \n",
      "1439  20181014OBLT0  20181014   OB      LT          0     T      0   W     1   \n",
      "\n",
      "      INN2  ...  KK  GD  WP  BK  ERR   R  ER  P_WHIP_RT  P2_WHIP_RT  \\\n",
      "0       27  ...   9   0   0   0    1   3   2   1.090909    1.600000   \n",
      "1       24  ...  10   0   2   0    0   6   5   1.750000    3.000000   \n",
      "2       27  ...   9   0   0   0    0   5   5   2.000000    1.333333   \n",
      "3       27  ...   8   0   0   0    0   4   4   1.200000    1.555556   \n",
      "4       27  ...   9   1   0   0    0   2   2   0.000000    0.777778   \n",
      "...    ...  ...  ..  ..  ..  ..  ...  ..  ..        ...         ...   \n",
      "1435    24  ...   4   1   1   0    0  10  10   6.000000    2.250000   \n",
      "1436    27  ...   6   1   0   0    0   5   5   3.000000    0.500000   \n",
      "1437    24  ...   8   1   0   0    1  12  10   2.000000   15.000000   \n",
      "1438    27  ...   2   1   1   0    0   5   5   2.250000    0.954545   \n",
      "1439    27  ...   4   3   0   0    0   1   1   1.000000    1.285714   \n",
      "\n",
      "      CB_WHIP_RT  \n",
      "0       2.250000  \n",
      "1       2.250000  \n",
      "2       1.500000  \n",
      "3       2.625000  \n",
      "4       1.333333  \n",
      "...          ...  \n",
      "1435    1.200000  \n",
      "1436    3.000000  \n",
      "1437    2.625000  \n",
      "1438    1.000000  \n",
      "1439    0.900000  \n",
      "\n",
      "[1440 rows x 34 columns],                G_ID   GDAY_DS T_ID VS_T_ID  HEADER_NO TB_SC  CG_CK WLS  HOLD  \\\n",
      "0     20190323HHOB0  20190323   OB      HH          0     B      0   W     1   \n",
      "1     20190323HHOB0  20190323   HH      OB          0     T      0   L     0   \n",
      "2     20190323KTSK0  20190323   SK      KT          0     B      0   W     1   \n",
      "3     20190323KTSK0  20190323   KT      SK          0     T      0   L     0   \n",
      "4     20190323LGHT0  20190323   HT      LG          0     B      0   L     0   \n",
      "...             ...       ...  ...     ...        ...   ...    ...  ..   ...   \n",
      "1435  20190930SKHH0  20190930   SK      HH          0     T      0   W     1   \n",
      "1436  20191001NCOB0  20191001   OB      NC          0     B      0   W     0   \n",
      "1437  20191001NCOB0  20191001   NC      OB          0     T      0   L     4   \n",
      "1438  20191001WOLT0  20191001   LT      WO          0     B      0   L     0   \n",
      "1439  20191001WOLT0  20191001   WO      LT          0     T      0   W     5   \n",
      "\n",
      "      INN2  ...  KK  GD  WP  BK  ERR  R  ER  P_WHIP_RT  P2_WHIP_RT  CB_WHIP_RT  \n",
      "0       27  ...   7   2   1   0    0  4   4   1.200000    1.888889    3.857143  \n",
      "1       24  ...   3   1   1   0    0  5   5   3.000000    1.375000    3.000000  \n",
      "2       27  ...  10   1   0   0    0  4   4   0.666667    1.375000    0.818182  \n",
      "3       24  ...   5   1   0   0    0  7   6   2.000000    1.826087    1.500000  \n",
      "4       27  ...  12   3   0   0    0  2   2   0.500000    1.222222    0.600000  \n",
      "...    ...  ...  ..  ..  ..  ..  ... ..  ..        ...         ...         ...  \n",
      "1435    27  ...   5   2   1   0    0  2   2   0.000000    1.000000    0.600000  \n",
      "1436    27  ...   7   1   2   0    0  5   5   1.250000    2.192308    2.000000  \n",
      "1437    25  ...   6   0   0   0    2  6   5   1.363636    1.695652    1.500000  \n",
      "1438    27  ...   9   1   2   0    0  3   1   1.333333    1.222222    0.545455  \n",
      "1439    27  ...   8   0   0   0    0  1   1   0.600000    1.111111    1.500000  \n",
      "\n",
      "[1440 rows x 34 columns],               G_ID   GDAY_DS T_ID VS_T_ID  HEADER_NO TB_SC  CG_CK WLS  HOLD  \\\n",
      "0    20200505HHSK0  20200505   SK      HH          0     B      0   L     0   \n",
      "1    20200505HHSK0  20200505   HH      SK          0     T      1   W     0   \n",
      "2    20200505LTKT0  20200505   KT      LT          0     B      0   L     0   \n",
      "3    20200505LTKT0  20200505   LT      KT          0     T      0   W     0   \n",
      "4    20200505NCSS0  20200505   SS      NC          0     B      0   L     0   \n",
      "..             ...       ...  ...     ...        ...   ...    ...  ..   ...   \n",
      "635  20200719LTSS0  20200719   LT      SS          0     T      0   W     2   \n",
      "636  20200719OBHT0  20200719   HT      OB          0     B      0   L     0   \n",
      "637  20200719OBHT0  20200719   OB      HT          0     T      0   W     1   \n",
      "638  20200719WOSK0  20200719   SK      WO          0     B      0   W     0   \n",
      "639  20200719WOSK0  20200719   WO      SK          0     T      0   L     2   \n",
      "\n",
      "     INN2  ...  KK  GD  WP  BK  ERR  R  ER  P_WHIP_RT  P2_WHIP_RT  CB_WHIP_RT  \n",
      "0      27  ...   5   1   0   0    0  3   3   1.500000    1.200000    1.333333  \n",
      "1      27  ...   2   0   0   0    0  0   0   0.000000    0.000000    0.750000  \n",
      "2      27  ...   9   1   0   0    0  7   7   3.000000    1.285714    1.875000  \n",
      "3      27  ...   8   1   0   0    0  2   2   0.000000    1.000000    2.571429  \n",
      "4      27  ...   9   0   0   0    0  4   4   1.500000    1.058824    2.142857  \n",
      "..    ...  ...  ..  ..  ..  ..  ... ..  ..        ...         ...         ...  \n",
      "635    27  ...   9   1   0   0    0  1   1   0.600000    1.111111    1.500000  \n",
      "636    27  ...   4   0   0   0    0  8   8   2.142857    1.800000    0.750000  \n",
      "637    27  ...   5   1   0   0    0  4   4   3.000000    1.000000    1.875000  \n",
      "638    27  ...   4   1   0   0    0  3   3   1.500000    1.105263    2.142857  \n",
      "639    24  ...  10   0   2   0    0  4   4   1.800000    1.312500    1.333333  \n",
      "\n",
      "[640 rows x 34 columns]]\n"
     ]
    }
   ],
   "source": [
    "#년도별 저장\n",
    "data_list_2016 = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-1].startswith('2016') ]\n",
    "data_2016 = [pd.read_csv(os.path.join(data_dir,data_list_2016[x]),encoding='cp949') for x in range(len(data_list_2016))]\n",
    "data_list_2017 = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-1].startswith('2017') ]\n",
    "data_2017 = [pd.read_csv(os.path.join(data_dir,data_list_2017[x]),encoding='cp949') for x in range(len(data_list_2017))]\n",
    "data_list_2018 = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-1].startswith('2018') ]\n",
    "data_2018 = [pd.read_csv(os.path.join(data_dir,data_list_2018[x]),encoding='cp949') for x in range(len(data_list_2018))]\n",
    "data_list_2019 = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-1].startswith('2019') ]\n",
    "data_2019 = [pd.read_csv(os.path.join(data_dir,data_list_2019[x]),encoding='cp949') for x in range(len(data_list_2019))]\n",
    "data_list_2020 = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-1].startswith('2020') ]\n",
    "data_2020 = [pd.read_csv(os.path.join(data_dir,data_list_2020[x]),encoding='cp949') for x in range(len(data_list_2020))]\n",
    "print('2020년 데이터: \\n', data_2020)\n",
    "\n",
    "#항목별 저장\n",
    "data_list_single_hitter = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-2].startswith('개인타자')]\n",
    "data_single_hitter = [pd.read_csv(os.path.join(data_dir, data_list_single_hitter[x]), encoding='cp949') for x in range(len(data_list_single_hitter))]\n",
    "data_list_single_pitcher = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-2].startswith('개인투수')]\n",
    "data_single_pitcher = [pd.read_csv(os.path.join(data_dir, data_list_single_pitcher[x]), encoding='cp949') for x in range(len(data_list_single_pitcher))]\n",
    "data_list_games = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-2].startswith('경기')]\n",
    "data_games = [pd.read_csv(os.path.join(data_dir, data_list_games[x]), encoding='cp949') for x in range(len(data_list_games))]\n",
    "data_list_player_enroll = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-2].startswith('등록선수')]\n",
    "data_player_enroll = [pd.read_csv(os.path.join(data_dir, data_list_player_enroll[x]), encoding='cp949') for x in range(len(data_list_player_enroll))]\n",
    "data_list_players = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-2].startswith('선수')]\n",
    "data_players = [pd.read_csv(os.path.join(data_dir, data_list_players[x]), encoding='cp949') for x in range(len(data_list_players))]\n",
    "data_list_teams = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-2].startswith('팀')]\n",
    "data_teams = [pd.read_csv(os.path.join(data_dir, data_list_teams[x]), encoding='cp949') for x in range(len(data_list_teams))]\n",
    "data_list_team_hitter = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-2].startswith('팀타자')]\n",
    "data_team_hitter = [pd.read_csv(os.path.join(data_dir, data_list_team_hitter[x]), encoding='cp949') for x in range(len(data_list_team_hitter))]\n",
    "data_list_team_pitcher = [data_list[x] for x in range(len(data_list)) if data_list[x].split('_')[-2].startswith('팀투수')]\n",
    "data_team_pitcher = [pd.read_csv(os.path.join(data_dir, data_list_team_pitcher[x]), encoding='cp949') for x in range(len(data_list_team_pitcher))]\n",
    "print('팀투수 데이터:\\n', data_team_pitcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_pitcher = pd.read_csv('C:/Users/nsd96/Desktop/빅콘테스트/pitcher_final.csv')\n",
    "add_hitter = pd.read_csv('C:/Users/nsd96/Desktop/빅콘테스트/hitter_final.csv')\n",
    "team_list = ['WO','OB','NC','SK','LG','KT','LT','HT','SS','HH']\n",
    "data=[0.5689655172413793,0.43636363636363634,0.5471698113207547,0.34545454545454546,\n",
    "     0.5636363636363636,0.660377358490566,0.5185185185185185,0.5283018867924528,\n",
    "     0.33962264150943394,0.3584905660377358]\n",
    "add_odds = pd.DataFrame(index=team_list, data=data, columns=[\"odds\"])\n",
    "\n",
    "def add_data(team, target):\n",
    "    if target == 'ERA':\n",
    "        df = add_pitcher[add_pitcher['T_ID'] == team]\n",
    "        df['ERA'] = df['ER'] * 27 / df['INN2']\n",
    "        return len(df), df['ERA'].mean()\n",
    "    elif target == 'AVG':\n",
    "        df = add_hitter[add_hitter['T_ID'] == team]\n",
    "        df['AVG'] = df['HIT'] / df['AB']\n",
    "        return len(df), df['AVG'].mean()\n",
    "    elif target == 'odds':\n",
    "        mean = add_odds[add_odds.index == team]\n",
    "        mean = mean['odds'].mean() \n",
    "        return mean\n",
    "    else:\n",
    "        raise 'wrong target name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34545454545454546"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data('SK', 'odds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_y_next(df, rm_range, target):\n",
    "    df['y_next'] = 0\n",
    "    \n",
    "    t_dict = {}\n",
    "    t_list = []\n",
    "    count = 0\n",
    "    f_count = 0\n",
    "    for i, v in enumerate(df[target]):\n",
    "\n",
    "        if df[target].index[i] == df[target].index[-1]:\n",
    "            t_dict[str(f_count)] = round(np.mean(np.array(t_list)),5)\n",
    "\n",
    "        t_list.append(v)\n",
    "        count += 1\n",
    "\n",
    "        if count == rm_range:\n",
    "            t_dict[str(f_count)] = round(np.mean(np.array(t_list)),5)\n",
    "            count = 0\n",
    "            t_list = []\n",
    "            f_count += 1\n",
    "\n",
    "    rm_dict = {}\n",
    "    rm_list = []\n",
    "    rm_count = 0\n",
    "    rm_fcount = 0\n",
    "\n",
    "    for i, v in enumerate(df['y_next']):\n",
    "        if df['y_next'].index[i] == df['y_next'].index[-1]:\n",
    "            rm_dict[str(rm_fcount)] = rm_list\n",
    "\n",
    "        rm_list.append(i)\n",
    "        rm_count += 1\n",
    "\n",
    "        if rm_count == rm_range:\n",
    "            rm_dict[str(rm_fcount)] = rm_list\n",
    "            rm_count = 0\n",
    "            rm_list = []\n",
    "            rm_fcount +=1\n",
    "\n",
    "    for k, v in rm_dict.items():\n",
    "        try:\n",
    "            df['y_next'].iloc[v] = t_dict[str(int(k)+1)]\n",
    "        except: \n",
    "            df['y_next'].iloc[v] = np.nan\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitter_outlier = ['20190825WOSS0','20170702WOKT0','20170930WONC0','20190831WONC0','20180811HTSK0','20170708HTKT0','20170521WOKT0',\n",
    "                  '20190630HTKT0','20170719NCHH0','20180904LGKT0','20180613SKHT0','20170702HTLG0','20170617SKSS0','20170702SSSK0',\n",
    "                  '20160831NCKT0','20170917OBSS0','20190823NCLG0','20200625NCKT2','20190515KTHT0','20180915NCOB0','20190820HTLG0',\n",
    "                  '20170919KTLG0','20170818HHNC0','20200610HTKT0','20190928NCKT0','20190407HHLT0']\n",
    "\n",
    "pitcher_outlier = ['20170702WOKT0','20200705LGSS0','20190608LTKT0','20180426OBSK0','20180422WOHH0','20160408HTKT0','20190521OBKT0',\n",
    "                   '20170502OBSS0','20170702HTLG0','20170702SSSK0','20200512HTHH0','20170616HHKT0','20180610WOKT0','20160619NCKT0',\n",
    "                   '20180529KTSS0','20200610HTKT0','20200602WOHH0','20180523OBHH0','20190407HHLT0','20160505OBLG0']\n",
    "hitter_outlier = [line.strip() for line in hitter_outlier]\n",
    "pithcer_outlier = [line.strip() for line in pitcher_outlier]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def lgbm_pitcher_eda(year, team_name):\n",
    "    year_index = year - 2016 \n",
    "    \n",
    "    # team_pitcher data에 해당 경기의 해당 팀의 홈 어웨이 여부 column 추가\n",
    "\n",
    "    data_team_pitcher_year = data_team_pitcher[year_index]\n",
    "\n",
    "    data_team_pitcher_year_team = data_team_pitcher_year[data_team_pitcher_year.T_ID == team_name]\n",
    "    df = data_team_pitcher_year_team\n",
    "\n",
    "    df['TB_SC'] = df['TB_SC'].map(lambda x: 1 if x == 'B' else 0 )\n",
    "\n",
    "    df = df.drop(columns = ['GDAY_DS'])\n",
    "    \n",
    "    # LabelEncoding\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    df['VS_T_ID'] = encoder.fit_transform(df['VS_T_ID']) #상대편\n",
    "    df['TB_SC'] = encoder.fit_transform(df['TB_SC']) #이닝 초/말\n",
    "    df['WLS'] = encoder.fit_transform(df['WLS']) #승패무 여부\n",
    "    \n",
    "    \n",
    "    ### 이부분은 모든 EDA에서 동일하게 작성\n",
    "    \n",
    "    df['H1']=df['HIT']-df['H2']-df['H3']-df['HR']\n",
    "\n",
    "    #df['G_ID'] = pd.Series(dt)\n",
    "    df['year']=df['G_ID'].str.slice(0,4).astype(int)\n",
    "    df['month']=df['G_ID'].str.slice(4,6).astype(int)\n",
    "    \n",
    "    df[\"FIP\"] = ((13*df.HR+3*(df.BB+df.HP-df.IB)-2*df.KK)/(df.INN2/3)) +3.2\n",
    "    df[\"BABIP\"] = (df.HIT-df.HR)/(df.AB-df.KK-df.HR+df.SF)\n",
    "    df['SLG']=(df['HIT']+df['HP']+df['BB'])/(df['AB']+df['BB']+df['HP']+df['SF'])\n",
    "    df['OBA']=(df['H1']+2*df['H2']+3*df['H3']+4*df['HR'])/df['AB']\n",
    "    df['W_OPS']=0.57* df['SLG']+0.43*df['OBA']\n",
    "\n",
    "    # 시계열 데이터(경기당 방어율 계산)이기 때문에 경기 코드를 index로\n",
    "    df = df.set_index('G_ID')\n",
    "\n",
    "    # 경기당 방어율 column 생성\n",
    "    df['ERA'] = df['ER'] * 27 / df['INN2']\n",
    "    \n",
    "    df['avg_ERA'] = df['ERA'].mean()\n",
    "    df['ERA_trend'] = (df['ERA'] - df['avg_ERA']).astype(np.float16)\n",
    "    df.drop(['avg_ERA'],axis=1,inplace=True)\n",
    "\n",
    "    df = df.drop(columns=['T_ID','P_WHIP_RT', 'P2_WHIP_RT', 'CB_WHIP_RT'])\n",
    "#     'P_WHIP_RT', 'P2_WHIP_RT', 'CB_WHIP_RT'\n",
    "    df_year_team_name = df\n",
    "    \n",
    "    return df_year_team_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def lgbm_hitter_eda(year, team_name):\n",
    "    year_index = year - 2016 \n",
    "    \n",
    "    # team_hitter data에 해당 경기의 해당 팀의 홈 어웨이 여부 column 추가\n",
    "    data_team_pitcher_year = data_team_pitcher[year_index]\n",
    "\n",
    "    data_team_pitcher_year_team = data_team_pitcher_year[data_team_pitcher_year.T_ID == team_name]\n",
    "    df = data_team_pitcher_year_team\n",
    "\n",
    "    df['TB_SC'] = df['TB_SC'].map(lambda x: 1 if x == 'B' else 0 )\n",
    "\n",
    "    df = df.drop(columns = ['GDAY_DS'])\n",
    "    \n",
    "    df.reset_index(inplace=True, drop=True) #index 재정렬\n",
    "    \n",
    "    # LabelEncoding\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    df['VS_T_ID'] = encoder.fit_transform(df['VS_T_ID']) #상대편\n",
    "    df['TB_SC'] = encoder.fit_transform(df['TB_SC']) #이닝 초/말\n",
    "    \n",
    "    \n",
    "    ### 이부분은 모든 EDA에서 동일하게 작성\n",
    "    \n",
    "    # GDAY_DS => Datetime type\n",
    "    df['GDAY_DS'] = df['GDAY_DS'].astype(str) + (df['HEADER_NO']+1).astype(str)\n",
    "    df['year']=df['GDAY_DS'].str.slice(0,4).astype(int)\n",
    "    df['month']=df['GDAY_DS'].str.slice(4,6).astype(int)\n",
    "    df['H1']=df['HIT']-df['H2']-df['H3']-df['HR']\n",
    "    dt = []\n",
    "    for i in df['GDAY_DS']:\n",
    "        dt_ = datetime.strptime(i, '%Y%m%d%H')\n",
    "        dt.append(dt_)\n",
    "    df['GDAY_DS'] = pd.Series(dt)\n",
    "    df['month']=df['GDAY_DS'].apply(lambda x: x.month)\n",
    "    \n",
    "    #장타율\n",
    "    df['SLG']=(df['HIT']+df['HP']+df['BB'])/(df['AB']+df['BB']+df['HP']+df['SF'])\n",
    "    #출루율\n",
    "    df['OBA']=(df['H1']+2*df['H2']+3*df['H3']+4*df['HR'])/df['AB']\n",
    "    df['W_OPS']=0.57* df['SLG']+0.43*df['OBA']\n",
    "    \n",
    "    \n",
    "    \n",
    "    df = df.drop(columns=['T_ID'])\n",
    "    #lags\n",
    "    #lags=[1,4,6,12,30,60]\n",
    "    #for lag in lags:\n",
    "    #    df['AVG_lag_'+str(lag)]=df['AVG'].shift(lag).astype(np.float16)\n",
    "    #SK Expanding window 추가\n",
    "    #df['expanding_AVG_mean']=df['AVG'].transform(lambda x: x.expanding(2).mean().astype(np.float16))\n",
    "    #rolling window\n",
    "    #df['rolling_AVG_mean']=df['AVG'].transform(lambda x: x.rolling(window=7).mean().astype(np.float16))\n",
    "    #trend\n",
    "    \n",
    "    # 경기당 타율 column 생성\n",
    "    df['AVG'] = df['HIT'] / df['AB']\n",
    "    \n",
    "    df['avg_AVG'] = df['AVG'].mean()\n",
    "    df['AVG_trend'] = (df['AVG'] - df['avg_AVG']).astype(np.float16)\n",
    "    df.drop(['avg_AVG'],axis=1,inplace=True)\n",
    "    \n",
    "    # Drop Categorical feature for 시계열 => coint 과정에서 singular matrix 발생\n",
    "    df = df.drop(columns=[ 'HOME_KEY'])\n",
    "    \n",
    "    \n",
    "\n",
    "#     df = df.drop(columns=['T_ID', 'HEADER_NO', 'CG_CK', 'BK'])\n",
    "#     df = df.drop(columns=['TB_SC', 'HR', 'SB', 'VS_T_ID', 'HOME_KEY', 'HOLD', 'INN2', 'BF', 'CS', 'SH', 'HP', 'GD', 'ERR' ,'ER'])\n",
    "    df_year_team_name = df\n",
    "    \n",
    "    return df_year_team_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 지정\n",
    "\n",
    "lgb_params_grid = {\n",
    "    'num_leaves': [5,10,20,30],\n",
    "    'min_data_in_leaf': [10,30, 50, 100],\n",
    "    'lambda_l1': [0, 1, 1.5],\n",
    "    'lambda_l2': [0, 1]\n",
    "}\n",
    "\n",
    "svr_params_grid = { \n",
    "    'kernel':['rbf','poly'],\n",
    "    'degree':[2,3,4,5,6,7],\n",
    "    'epsilon':[0.1,0.2,1,10,20,30]\n",
    "}\n",
    "\n",
    "rf_params_grid = {\n",
    "    'n_estimators': [10, 100],\n",
    "    'max_depth': [6,8,10,12],\n",
    "    'min_samples_leaf': [8,12,18],\n",
    "    'min_samples_split': [8,16,20]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LGBM_pitcher:\n",
    "    def __init__(self, eda,  team, yn_range, model_type):\n",
    "        self.eda = eda\n",
    "        self.team = team\n",
    "        self.yn_range = yn_range\n",
    "        self.model_type = model_type\n",
    "    \n",
    "    def run(self):\n",
    "        df_2016 = self.eda(2016, self.team)\n",
    "        df_2017 = self.eda(2017, self.team)\n",
    "        df_2018 = self.eda(2018, self.team)\n",
    "        df_2019 = self.eda(2019, self.team)\n",
    "        df_2020 = self.eda(2020, self.team)\n",
    "        \n",
    "        df_all = pd.concat([df_2016,df_2017,df_2018,df_2019,df_2020])\n",
    "        \n",
    "        df_all = df_all[df_all.index.isin(pithcer_outlier)==False]\n",
    "        df_all =df_all.reset_index()\n",
    "        df_all = make_y_next(df_all, self.yn_range, 'ERA')\n",
    "        \n",
    "        # 테스트용 데이터 뽑기\n",
    "        final_test = df_all[df_all['y_next'].isnull()]; \n",
    "\n",
    "        df_all.dropna(inplace=True)#일단 드랍하고 진행\n",
    "\n",
    "        df_all=df_all.drop(['G_ID'],axis=1)\n",
    "\n",
    "        # 범주형 변수 지정\n",
    "        cat_features = ['VS_T_ID' ,'HEADER_NO', 'TB_SC' ]\n",
    "        df_all[cat_features] = df_all[cat_features].astype('category')\n",
    "\n",
    "        # X,y Train split\n",
    "        X = df_all.drop(columns = ['y_next'])\n",
    "        y = df_all['y_next']\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.15, shuffle = True, random_state = 2020)\n",
    "\n",
    "        X_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train.columns]\n",
    "        X_val.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_val.columns]\n",
    "\n",
    "        if(self.model_type==\"lgbm\"):\n",
    "            m = LGBMRegressor(boosting_type='gbdt', num_boost_round=2000, learning_rate=0.01)\n",
    "            m_grid = GridSearchCV(estimator=m,param_grid=lgb_params_grid,n_jobs=10,verbose=3)\n",
    "            m_grid.fit(X_train,y_train) \n",
    "            m = m_grid\n",
    "            \n",
    "            final_test = final_test.drop(['G_ID','y_next'],axis=1)\n",
    "            final_test[cat_features] = final_test[cat_features].astype('category')\n",
    "            \n",
    "            final_predict = m.predict(final_test)\n",
    "            final_predict_mean = final_predict.mean()\n",
    "            \n",
    "        elif(self.model_type==\"rf\"):\n",
    "            m = RandomForestRegressor(random_state=0, n_jobs=1)\n",
    "            m_grid = GridSearchCV(m, param_grid = rf_params_grid, cv = 3, n_jobs = -1)\n",
    "            m_grid.fit(X_train,y_train) \n",
    "            m = m_grid\n",
    "            \n",
    "            final_test = final_test.drop(['G_ID','y_next'],axis=1)\n",
    "            final_test[cat_features] = final_test[cat_features].astype('category')\n",
    "            \n",
    "            final_predict = m.predict(final_test)\n",
    "            final_predict_mean = final_predict.mean()\n",
    "            \n",
    "\n",
    "        elif(self.model_type==\"SVR\"):\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "            m = SVR(kernel='rbf')\n",
    "            m_grid = GridSearchCV(estimator=m, param_grid=svr_params_grid) \n",
    "            m_grid.fit(X_train, y_train)\n",
    "            m = m_grid\n",
    "            \n",
    "            final_test = final_test.drop(['G_ID','y_next'],axis=1)\n",
    "            final_test[cat_features] = final_test[cat_features].astype('category')\n",
    "            \n",
    "            final_test = scaler.fit_transform(final_test)\n",
    "            final_predict = m.predict(final_test)\n",
    "            final_predict_mean = final_predict.mean()\n",
    "\n",
    "        return final_predict_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LGBM_hitter:\n",
    "    def __init__(self, eda,  team, yn_range, model_type):\n",
    "        self.eda = eda\n",
    "        self.team = team\n",
    "        self.yn_range = yn_range\n",
    "        self.model_type = model_type\n",
    "    def run(self):\n",
    "        df_2016 = self.eda(2016, self.team)\n",
    "        df_2017 = self.eda(2017, self.team)\n",
    "        df_2018 = self.eda(2018, self.team)\n",
    "        df_2019 = self.eda(2019, self.team)\n",
    "        df_2020 = self.eda(2020, self.team)\n",
    "\n",
    "        df_all = pd.concat([df_2016,df_2017,df_2018,df_2019,df_2020])\n",
    "        df_all = df_all[df_all.index.isin(hitter_outlier)==False]\n",
    "        df_all =df_all.reset_index()\n",
    "        \n",
    "        df_all = make_y_next(df_all, self.yn_range, 'AVG')\n",
    "        \n",
    "        # 테스트용 데이터 뽑기\n",
    "        final_test = df_all[df_all['y_next'].isnull()]; \n",
    "        df_all.dropna(inplace=True)#일단 드랍하고 진행\n",
    "        df_all = df_all.drop(['GDAY_DS'],axis=1)\n",
    "\n",
    "        # 범주형 변수 지정\n",
    "        cat_features = ['VS_T_ID' ,'HEADER_NO', 'TB_SC' ]\n",
    "        df_all[cat_features] = df_all[cat_features].astype('category')\n",
    "\n",
    "        # X,y Train test val split\n",
    "        X = df_all.drop(columns = ['y_next'])\n",
    "        y = df_all['y_next']\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.15, shuffle = True, random_state = 2020)\n",
    "\n",
    "        X_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train.columns]\n",
    "        X_val.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_val.columns]\n",
    "\n",
    "        if(self.model_type==\"lgbm\"):\n",
    "            m = LGBMRegressor(boosting_type='gbdt', num_boost_round=2000, learning_rate=0.01)\n",
    "            m_grid = GridSearchCV(estimator=m,param_grid=lgb_params_grid,n_jobs=10,verbose=3)\n",
    "            m_grid.fit(X_train,y_train) \n",
    "            m = m_grid\n",
    "            \n",
    "            final_test = final_test.drop(['G_ID','y_next'],axis=1)\n",
    "            final_test[cat_features] = final_test[cat_features].astype('category')\n",
    "            \n",
    "            final_predict = m.predict(final_test)\n",
    "            final_predict_mean = final_predict.mean()\n",
    "            \n",
    "        elif(self.model_type==\"rf\"):\n",
    "            m = RandomForestRegressor(random_state=0, n_jobs=1)\n",
    "            m_grid = GridSearchCV(m, param_grid = rf_params_grid, cv = 3, n_jobs = -1)\n",
    "            m_grid.fit(X_train,y_train) \n",
    "            m = m_grid\n",
    "            \n",
    "            final_test = final_test.drop(['G_ID','y_next'],axis=1)\n",
    "            final_test[cat_features] = final_test[cat_features].astype('category')\n",
    "            \n",
    "            final_predict = m.predict(final_test)\n",
    "            final_predict_mean = final_predict.mean()\n",
    "            \n",
    "\n",
    "        elif(self.model_type==\"SVR\"):\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "            m = SVR(kernel='rbf')\n",
    "            m_grid = GridSearchCV(estimator=m, param_grid=svr_params_grid) \n",
    "            m_grid.fit(X_train, y_train)\n",
    "            m = m_grid\n",
    "            \n",
    "            final_test = final_test.drop(['G_ID','y_next'],axis=1)\n",
    "            final_test[cat_features] = final_test[cat_features].astype('category')\n",
    "            \n",
    "            final_test = scaler.fit_transform(final_test)\n",
    "            final_predict = m.predict(final_test)\n",
    "            final_predict_mean = final_predict.mean()\n",
    "\n",
    "        return final_predict_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 승률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LGBM_lose:\n",
    "    def __init__(self, eda,  team, yn_range, model_type):\n",
    "        self.eda = eda\n",
    "        self.team = team\n",
    "        self.yn_range = yn_range\n",
    "        self.model_type = model_type\n",
    "        \n",
    "    def run(self):\n",
    "        df_2016 = self.eda(2016, self.team)\n",
    "        df_2017 = self.eda(2017, self.team)\n",
    "        df_2018 = self.eda(2018, self.team)\n",
    "        df_2019 = self.eda(2019, self.team)\n",
    "        df_2020 = self.eda(2020, self.team)\n",
    "        \n",
    "        df_all = pd.concat([df_2016,df_2017,df_2018,df_2019,df_2020])\n",
    "        df_all = df_all[df_all.index.isin(pithcer_outlier)==False]\n",
    "        df_all =df_all.reset_index()\n",
    "\n",
    "        df_all = make_y_next(df_all, self.yn_range, 'R')\n",
    "        \n",
    "        # 테스트용 데이터 뽑기\n",
    "        final_test = df_all[df_all['y_next'].isnull()]; \n",
    "\n",
    "        df_all.dropna(inplace=True)#일단 드랍하고 진행\n",
    "\n",
    "        df_all=df_all.drop(['G_ID'],axis=1)\n",
    "\n",
    "        # 범주형 변수 지정\n",
    "        cat_features = ['VS_T_ID' ,'HEADER_NO', 'TB_SC' ]\n",
    "        df_all[cat_features] = df_all[cat_features].astype('category')\n",
    "\n",
    "        # X,y Train split\n",
    "        X = df_all.drop(columns = ['y_next'])\n",
    "        y = df_all['y_next']\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.15, shuffle = True, random_state = 2020)\n",
    "\n",
    "        X_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train.columns]\n",
    "        X_val.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_val.columns]\n",
    "\n",
    "        if(self.model_type==\"lgbm\"):\n",
    "            m = LGBMRegressor(boosting_type='gbdt', num_boost_round=2000, learning_rate=0.01)\n",
    "            m_grid = GridSearchCV(estimator=m,param_grid=lgb_params_grid,n_jobs=10,verbose=3)\n",
    "            m_grid.fit(X_train,y_train) \n",
    "            m = m_grid\n",
    "            \n",
    "            final_test = final_test.drop(['G_ID','y_next'],axis=1)\n",
    "            final_test[cat_features] = final_test[cat_features].astype('category')\n",
    "            \n",
    "            final_predict = m.predict(final_test)\n",
    "            final_predict_mean = final_predict.mean()\n",
    "            \n",
    "        elif(self.model_type==\"rf\"):\n",
    "            m = RandomForestRegressor(random_state=0, n_jobs=1)\n",
    "            m_grid = GridSearchCV(m, param_grid = rf_params_grid, cv = 3, n_jobs = -1)\n",
    "            m_grid.fit(X_train,y_train) \n",
    "            m = m_grid\n",
    "            \n",
    "            final_test = final_test.drop(['G_ID','y_next'],axis=1)\n",
    "            final_test[cat_features] = final_test[cat_features].astype('category')\n",
    "            \n",
    "            final_predict = m.predict(final_test)\n",
    "            final_predict_mean = final_predict.mean()\n",
    "            \n",
    "\n",
    "        elif(self.model_type==\"SVR\"):\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "            m = SVR(kernel='rbf')\n",
    "            m_grid = GridSearchCV(estimator=m, param_grid=svr_params_grid) \n",
    "            m_grid.fit(X_train, y_train)\n",
    "            m = m_grid\n",
    "            \n",
    "            final_test = final_test.drop(['G_ID','y_next'],axis=1)\n",
    "            final_test[cat_features] = final_test[cat_features].astype('category')\n",
    "            \n",
    "            final_test = scaler.fit_transform(final_test)\n",
    "            final_predict = m.predict(final_test)\n",
    "            final_predict_mean = final_predict.mean()\n",
    "\n",
    "        return final_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LGBM_score:\n",
    "    def __init__(self, eda,  team, yn_range,model_type):\n",
    "        self.eda = eda\n",
    "        self.team = team\n",
    "        self.yn_range = yn_range\n",
    "        self.model_type = model_type\n",
    "        \n",
    "    def run(self):\n",
    "        df_2016 = self.eda(2016, self.team)\n",
    "        df_2017 = self.eda(2017, self.team)\n",
    "        df_2018 = self.eda(2018, self.team)\n",
    "        df_2019 = self.eda(2019, self.team)\n",
    "        df_2020 = self.eda(2020, self.team)\n",
    "\n",
    "        df_all = pd.concat([df_2016,df_2017,df_2018,df_2019,df_2020])\n",
    "        df_all = df_all[df_all.index.isin(hitter_outlier)==False]\n",
    "        df_all =df_all.reset_index()\n",
    "        \n",
    "        df_all = make_y_next(df_all, self.yn_range, 'RUN')\n",
    "        \n",
    "        # 테스트용 데이터 뽑기\n",
    "        final_test = df_all[df_all['y_next'].isnull()]; \n",
    "\n",
    "        df_all.dropna(inplace=True)#일단 드랍하고 진행\n",
    "\n",
    "        df_all = df_all.drop(['GDAY_DS'],axis=1)\n",
    "\n",
    "        # 범주형 변수 지정\n",
    "        cat_features = ['VS_T_ID' ,'HEADER_NO', 'TB_SC' ]\n",
    "        df_all[cat_features] = df_all[cat_features].astype('category')\n",
    "\n",
    "        # X,y Train test val split\n",
    "        X = df_all.drop(columns = ['y_next'])\n",
    "        y = df_all['y_next']\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.15, shuffle = True, random_state = 2020)\n",
    "\n",
    "        X_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train.columns]\n",
    "        X_val.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_val.columns]\n",
    "\n",
    "\n",
    "        if(self.model_type==\"lgbm\"):\n",
    "            m = LGBMRegressor(boosting_type='gbdt', num_boost_round=2000, learning_rate=0.01)\n",
    "            m_grid = GridSearchCV(estimator=m,param_grid=lgb_params_grid,n_jobs=10,verbose=3)\n",
    "            m_grid.fit(X_train,y_train) \n",
    "            m = m_grid\n",
    "            \n",
    "            final_test = final_test.drop(['G_ID','y_next'],axis=1)\n",
    "            final_test[cat_features] = final_test[cat_features].astype('category')\n",
    "            \n",
    "            final_predict = m.predict(final_test)\n",
    "            final_predict_mean = final_predict.mean()\n",
    "            \n",
    "        elif(self.model_type==\"rf\"):\n",
    "            m = RandomForestRegressor(random_state=0, n_jobs=1)\n",
    "            m_grid = GridSearchCV(m, param_grid = rf_params_grid, cv = 3, n_jobs = -1)\n",
    "            m_grid.fit(X_train,y_train) \n",
    "            m = m_grid\n",
    "            \n",
    "            final_test = final_test.drop(['G_ID','y_next'],axis=1)\n",
    "            final_test[cat_features] = final_test[cat_features].astype('category')\n",
    "            \n",
    "            final_predict = m.predict(final_test)\n",
    "            final_predict_mean = final_predict.mean()\n",
    "            \n",
    "\n",
    "        elif(self.model_type==\"SVR\"):\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "            m = SVR(kernel='rbf')\n",
    "            m_grid = GridSearchCV(estimator=m, param_grid=svr_params_grid) \n",
    "            m_grid.fit(X_train, y_train)\n",
    "            m = m_grid\n",
    "            \n",
    "            final_test = final_test.drop(['G_ID','y_next'],axis=1)\n",
    "            final_test[cat_features] = final_test[cat_features].astype('category')\n",
    "            \n",
    "            final_test = scaler.fit_transform(final_test)\n",
    "            final_predict = m.predict(final_test)\n",
    "            final_predict_mean = final_predict.mean()\n",
    "\n",
    "        return final_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_odds_predict(team, predict_range, model_type):\n",
    "    score = LGBM_score(lgbm_hitter_eda, team, predict_range, model_type)\n",
    "    lose = LGBM_lose(lgbm_pitcher_eda, team, predict_range, model_type)\n",
    "\n",
    "    lose = lose.run()\n",
    "    score = score.run()\n",
    "\n",
    "#     score = np.array(score)\n",
    "#     lose = np.array(lose)\n",
    "#     win_rate= np.square(score)  / ( np.square(score)  +   np.square(lose) )\n",
    "    score = np.array(score).mean()\n",
    "    lose = np.array(lose).mean()\n",
    "    win_rate_mean = score**2/(score**2 + lose**2)\n",
    "#     win_rate_mean = win_rate.mean()\n",
    "    return win_rate_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library import\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from IPython.display import display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "\n",
    "def var_pitcher_eda(team_name):\n",
    "    \n",
    "    # team_pitcher data에 해당 경기의 해당 팀의 홈 어웨이 여부 column 추가\n",
    "    data_games_2016 = data_games[0]\n",
    "    data_games_2017 = data_games[1]\n",
    "    data_games_2018 = data_games[2]\n",
    "    data_games_2019 = data_games[3]\n",
    "    data_games_2020 = data_games[4]\n",
    "    data_games_year = pd.concat([data_games_2016,data_games_2017,data_games_2018,data_games_2019,data_games_2020])\n",
    "\n",
    "    data_games_year = data_games_year[['G_ID', 'VISIT_KEY', 'HOME_KEY']]\n",
    "    data_games_year = data_games_year.set_index('G_ID')\n",
    "    \n",
    "    data_team_pitcher_2016 = data_team_pitcher[0]\n",
    "    data_team_pitcher_2017 = data_team_pitcher[1]\n",
    "    data_team_pitcher_2018 = data_team_pitcher[2]\n",
    "    data_team_pitcher_2019 = data_team_pitcher[3]\n",
    "    data_team_pitcher_2020 = data_team_pitcher[4]\n",
    "    data_team_pitcher_year = pd.concat([data_team_pitcher_2016,data_team_pitcher_2017,data_team_pitcher_2018,\n",
    "                                        data_team_pitcher_2019,data_team_pitcher_2020])\n",
    "\n",
    "    data_team_pitcher_year = pd.merge(data_team_pitcher_year, data_games_year, how='left', on=['G_ID'])\n",
    "\n",
    "    data_team_pitcher_year_team = data_team_pitcher_year[data_team_pitcher_year.T_ID == team_name]\n",
    "    df = data_team_pitcher_year_team\n",
    "    df= df[df.G_ID.isin(pithcer_outlier)==False]\n",
    "    df['HOME_KEY'] = df['HOME_KEY'].map(lambda x: 1 if x == team_name else 0 )\n",
    "    df = df.drop(columns = ['VISIT_KEY', 'G_ID'])\n",
    "    \n",
    "    df.reset_index(inplace=True, drop=True) #index 재정렬\n",
    "    \n",
    "    # LabelEncoding\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    df['VS_T_ID'] = encoder.fit_transform(df['VS_T_ID']) #상대편\n",
    "    df['TB_SC'] = encoder.fit_transform(df['TB_SC']) #이닝 초/말\n",
    "    df['WLS'] = encoder.fit_transform(df['WLS']) #승패무 여부\n",
    "    \n",
    "    ### 이부분은 모든 EDA에서 동일하게 작성\n",
    "    \n",
    "    # GDAY_DS => Datetime type\n",
    "    df['GDAY_DS'] = df['GDAY_DS'].astype('string') + (df['HEADER_NO']+1).astype('string')\n",
    "        \n",
    "    dt = []\n",
    "    for i in df['GDAY_DS']:\n",
    "        dt_ = datetime.strptime(i, '%Y%m%d%H')\n",
    "        dt.append(dt_)\n",
    "    df['GDAY_DS'] = pd.Series(dt)\n",
    "    \n",
    "    # 시계열 데이터(경기당 방어율 계산)이기 때문에 경기 코드를 index로\n",
    "    df = df.set_index('GDAY_DS')\n",
    "    \n",
    "    df = df.drop(columns=['T_ID'])\n",
    "    \n",
    "    ###\n",
    "    \n",
    "\n",
    "    # 경기당 방어율 column 생성\n",
    "    df['ERA'] = df['ER'] * 27 / df['INN2']\n",
    "    df['avg_ERA'] = df['ERA'].mean()\n",
    "    df['ERA_trend'] = (df['ERA'] - df['avg_ERA']).astype(np.float16)\n",
    "    df.drop(['avg_ERA'],axis=1,inplace=True)\n",
    "    \n",
    "    # Drop Categorical feature for 시계열 => coint 과정에서 singular matrix 발생\n",
    "    df = df.drop(columns=['VS_T_ID', 'HEADER_NO', 'TB_SC', 'CG_CK', 'WLS', 'HOLD', 'HOME_KEY'])\n",
    "    \n",
    "\n",
    "#     df = df.drop(columns=['T_ID', 'HEADER_NO', 'CG_CK', 'BK'])\n",
    "#     df = df.drop(columns=['TB_SC', 'HR', 'SB', 'VS_T_ID', 'HOME_KEY', 'HOLD', 'INN2', 'BF', 'CS', 'SH', 'HP', 'GD', 'ERR' ,'ER'])\n",
    "    df_year_team_name = df\n",
    "    \n",
    "    return df_year_team_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_hitter_eda(team_name):\n",
    "    \n",
    "    # team_pitcher data에 해당 경기의 해당 팀의 홈 어웨이 여부 column 추가\n",
    "    data_games_2016 = data_games[0]\n",
    "    data_games_2017 = data_games[1]\n",
    "    data_games_2018 = data_games[2]\n",
    "    data_games_2019 = data_games[3]\n",
    "    data_games_2020 = data_games[4]\n",
    "    data_games_year = pd.concat([data_games_2016,data_games_2017,data_games_2018,data_games_2019,data_games_2020])\n",
    "\n",
    "    data_games_year = data_games_year[['G_ID', 'VISIT_KEY', 'HOME_KEY']]\n",
    "    data_games_year = data_games_year.set_index('G_ID')\n",
    "    \n",
    "    data_team_hitter_2016 = data_team_hitter[0]\n",
    "    data_team_hitter_2017 = data_team_hitter[1]\n",
    "    data_team_hitter_2018 = data_team_hitter[2]\n",
    "    data_team_hitter_2019 = data_team_hitter[3]\n",
    "    data_team_hitter_2020 = data_team_hitter[4]\n",
    "    data_team_hitter_year = pd.concat([data_team_hitter_2016,data_team_hitter_2017,data_team_hitter_2018,\n",
    "                                        data_team_hitter_2019,data_team_hitter_2020])\n",
    "\n",
    "    data_team_hitter_year = pd.merge(data_team_hitter_year, data_games_year, how='left', on=['G_ID'])\n",
    "\n",
    "    data_team_hitter_year_team = data_team_hitter_year[data_team_hitter_year.T_ID == team_name]\n",
    "    df = data_team_hitter_year_team\n",
    "    df= df[df.G_ID.isin(hitter_outlier)==False]\n",
    "    df['HOME_KEY'] = df['HOME_KEY'].map(lambda x: 1 if x == team_name else 0 )\n",
    "    df = df.drop(columns = ['VISIT_KEY', 'G_ID'])\n",
    "    \n",
    "    df.reset_index(inplace=True, drop=True) #index 재정렬\n",
    "    \n",
    "    # LabelEncoding\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    df['VS_T_ID'] = encoder.fit_transform(df['VS_T_ID']) #상대편\n",
    "    \n",
    "    ### 이부분은 모든 EDA에서 동일하게 작성\n",
    "    \n",
    "    # GDAY_DS => Datetime type\n",
    "    df['GDAY_DS'] = df['GDAY_DS'].astype('string') + (df['HEADER_NO']+1).astype('string')\n",
    "        \n",
    "    dt = []\n",
    "    for i in df['GDAY_DS']:\n",
    "        dt_ = datetime.strptime(i, '%Y%m%d%H')\n",
    "        dt.append(dt_)\n",
    "    df['GDAY_DS'] = pd.Series(dt)\n",
    "    \n",
    "    # 시계열 데이터(경기당 방어율 계산)이기 때문에 경기 코드를 index로\n",
    "    df = df.set_index('GDAY_DS')\n",
    "    \n",
    "    df = df.drop(columns=['T_ID'])\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    df['H1']=df['HIT']-df['H2']-df['H3']-df['HR']\n",
    "    #장타율\n",
    "    df['SLG']=(df['HIT']+df['HP']+df['BB'])/(df['AB']+df['BB']+df['HP']+df['SF'])\n",
    "    #출루율\n",
    "    df['OBA']=(df['H1']+2*df['H2']+3*df['H3']+4*df['HR'])/df['AB']\n",
    "    df['W_OPS']=0.57* df['SLG']+0.43*df['OBA']\n",
    "    # 경기당 타율 column 생성\n",
    "    df['AVG'] = df['HIT'] / df['AB']\n",
    "    \n",
    "    df['avg_AVG'] = df['AVG'].mean()\n",
    "    df['AVG_trend'] = (df['AVG'] - df['avg_AVG']).astype(np.float16)\n",
    "    df.drop(['avg_AVG'],axis=1,inplace=True)\n",
    "    \n",
    "    # Drop Categorical feature for 시계열 => coint 과정에서 singular matrix 발생\n",
    "    df = df.drop(columns=['VS_T_ID', 'HEADER_NO', 'TB_SC', 'HOME_KEY'])\n",
    "    \n",
    "    df_year_team_name = df\n",
    "    \n",
    "    return df_year_team_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Var_automation:\n",
    "    def __init__(self, df, target, predict_range, mode):\n",
    "        super(Var_automation, self).__init__()\n",
    "        \n",
    "        self.mode = mode\n",
    "        if self.mode=='train':\n",
    "            self.df_set = df\n",
    "            self.predict_range = predict_range\n",
    "            self.train = self.df_set.iloc[:-self.predict_range]\n",
    "            self.test = self.df_set.iloc[-self.predict_range:]\n",
    "            self.target = target\n",
    "        \n",
    "        else:\n",
    "            zero_np = np.zeros((predict_range,len(df.columns)))\n",
    "            df_np = df.to_numpy()\n",
    "            df_concat = np.concatenate((df_np, zero_np))\n",
    "            self.df_set = pd.DataFrame(df_concat, columns = df.columns)\n",
    "\n",
    "            self.predict_range = predict_range\n",
    "            self.train = self.df_set.iloc[:-self.predict_range]\n",
    "            self.test = self.df_set.iloc[-self.predict_range:]\n",
    "            self.target = target\n",
    "\n",
    "    \n",
    "    def grangers_causation_matirx(self, test='ssr_chi2test', verbose=False): #인과관계 검정\n",
    "        variables = self.df_set.columns\n",
    "        maxlag = 8\n",
    "        \n",
    "        df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns = variables, index = variables)\n",
    "        for column in df.columns:\n",
    "            for index in df.index:\n",
    "                test_result = grangercausalitytests(self.df_set[[index,column]], maxlag=maxlag, verbose=False)\n",
    "                p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "                if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "                min_p_value = np.min(p_values)\n",
    "                df.loc[index, column] = min_p_value\n",
    "        df.columns = [var + '_x' for var in variables]\n",
    "        df.index = [var + '_y' for var in variables]\n",
    "        \n",
    "        target_p_value = df.loc['{}_y'.format(self.target)]\n",
    "        effective_var = []\n",
    "        for i, v in enumerate(target_p_value):\n",
    "            if v < 0.05:\n",
    "                effective_var.append(df.index[i].split('_y')[0])\n",
    "        \n",
    "        return df, effective_var\n",
    "    \n",
    "    def cointegration_test(self, alpha=0.05): #공적분 검정\n",
    "        df_grangers, var_grangers = self.grangers_causation_matirx()\n",
    "        var_coint = var_grangers\n",
    "        var_coint.append(self.target) #뒤에 넣어야 하는듯\n",
    "        df = self.df_set[var_coint] \n",
    "#         df = self.df_set[['ERA','H3']]\n",
    "        df_shape = df.shape\n",
    "                \n",
    "        try:\n",
    "            out = coint_johansen(df, -1, 5)\n",
    "        except:\n",
    "            df = df + 0.00001*np.random.rand(df_shape[0],df_shape[1]) #Singular Matrix Error Solution\n",
    "            out = coint_johansen(df, -1, 5)\n",
    "            \n",
    "        d = {'0.90': 0, '0.95': 1, '0.99': 2}\n",
    "        traces = out.lr1\n",
    "        cvts = out.cvt[:, d[str(1-alpha)]]\n",
    "        def adjust(val, length= 6): return str(val).ljust(length)    \n",
    "        \n",
    "        dict = {'Name': [], 'Test Stat': [], '> C(95%)': [], 'Signif': []}\n",
    "        effective_var = []\n",
    "\n",
    "        for col, trace, cvt in zip(df.columns, traces, cvts):\n",
    "            \n",
    "            dict['Name'].append(adjust(col))\n",
    "            dict['Test Stat'].append(adjust(round(trace,2), 9))\n",
    "            dict['> C(95%)'].append(adjust(cvt, 8))\n",
    "            dict['Signif'].append(trace > cvt)\n",
    "            \n",
    "            if trace>cvt:\n",
    "                effective_var.append(col)\n",
    "        \n",
    "        df_coint = pd.DataFrame(dict)                \n",
    "        \n",
    "        return df_coint, effective_var\n",
    "    \n",
    "    def adfuller_test(self, signif=0.05, verbose=False): #단위근 검정\n",
    "        df_coint, effective_var = self.cointegration_test()\n",
    "        effective_var.append(self.target)\n",
    "        \n",
    "        df_train = self.train[effective_var]\n",
    "        df_test = self.test[effective_var]\n",
    "        \n",
    "        def adjust(val, length=6): return str(val).ljust(length)\n",
    "        \n",
    "        adfuller_dict = {}\n",
    "        stationary_var = []\n",
    "        \n",
    "        for name, series in df_train.items():\n",
    "            name = series.name\n",
    "            \n",
    "            dict = {'adfuller_test var': [], 'Significance Level': [], 'Test Statistic': [], 'No. Lags Chosen': []}\n",
    "    \n",
    "            r = adfuller(series, autolag = 'AIC')\n",
    "            output = {'test_static': round(r[0], 4), 'pvalue': round(r[1], 4), 'n_lags': round(r[2], 4), 'n_obs': r[3]}\n",
    "            p_value = output['pvalue']\n",
    "            \n",
    "            dict['adfuller_test var'].append(name)\n",
    "            dict['Significance Level'].append(signif)\n",
    "            dict['Test Statistic'].append(output['test_static'])\n",
    "            dict['No. Lags Chosen'].append(output['n_lags'])\n",
    "            \n",
    "            for key, val in r[4].items():\n",
    "                dict['Critical value {}'.format(adjust(key))] = round(val, 3)\n",
    "                \n",
    "            if p_value <= signif:\n",
    "                dict['P-Value'] = '{} => Series is Stationary'.format(p_value)\n",
    "                stationary_var.append(name)\n",
    "            else:\n",
    "                dict['P-Value'] = '{} => Series is Non-Stationary'.format(p_value)\n",
    "            \n",
    "            adfuller_dict[name] = dict\n",
    "        \n",
    "        return adfuller_dict, stationary_var\n",
    "    \n",
    "    def aic_test(self, lag=8):#대부분 lag 범위를 10이상 주지 않는듯\n",
    "        dict_, effective_var = self.adfuller_test()\n",
    "        df_train = self.train[effective_var]\n",
    "        \n",
    "        model = sm.tsa.VAR(df_train)\n",
    "        \n",
    "        dict = {}\n",
    "        for i in range(1, lag+1, 1):\n",
    "            try:\n",
    "                result = model.fit(i)\n",
    "                dict[i] = result.aic\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "            \n",
    "        sorted_dict = sorted(dict.items(), key = lambda x: x[1])\n",
    "        min_aic = sorted_dict[0]\n",
    "        \n",
    "        min_lag = min_aic[0]\n",
    "        min_lag_aic = min_aic[1]\n",
    "        \n",
    "        return min_lag, min_lag_aic, model, effective_var\n",
    "    \n",
    "    def var_fit(self, model_summary=False):\n",
    "        min_lag, min_lag_aic, model, effective_var = self.aic_test()\n",
    "        model_fitted = model.fit(min_lag)\n",
    "        \n",
    "        if model_summary:\n",
    "            model_fitted.summary()\n",
    "            \n",
    "        return model_fitted, effective_var\n",
    "    \n",
    "    def durbin_watson_test(self):\n",
    "        model_fitted, effective_var = self.var_fit()\n",
    "        df_train = self.train[effective_var]\n",
    "        out = durbin_watson(model_fitted.resid)\n",
    "        \n",
    "        durbin_dict = {}\n",
    "        for col, val in zip(df_train.columns, out):\n",
    "            durbin_dict[col] = round(val,2)\n",
    "        \n",
    "        return durbin_dict, model_fitted, effective_var\n",
    "    \n",
    "    def run(self, displaying=False): #nobs=20, \n",
    "        model_fitted, effective_var = self.var_fit()\n",
    "        df_train = self.train[effective_var]\n",
    "        df_test = self.test[effective_var]\n",
    "        \n",
    "        lag_order = model_fitted.k_ar\n",
    "        forecast_input = df_train.values[-lag_order:]\n",
    "        \n",
    "        fc = model_fitted.forecast(y=forecast_input, steps=self.predict_range)\n",
    "        df_forecast = pd.DataFrame(fc, index = df_test.index, columns=effective_var)\n",
    "        \n",
    "#         mae_result = mean_absolute_error(df_test[self.target], df_forecast[self.target])  \n",
    "# => 우리가 알고 싶은 것은 target의 전체 평균의 차이\n",
    "        \n",
    "        \n",
    "        #display\n",
    "        if displaying:\n",
    "            \n",
    "            display(df_forecast)\n",
    "        \n",
    "            fig,axes = plt.subplots(nrows=int(len(effective_var)/2), ncols=2, dpi=150, figsize=(10,10))\n",
    "            for i, (col,ax) in enumerate(zip(effective_var, axes.flatten())):\n",
    "                df_test[col].plot(legend=True, ax=ax);\n",
    "                df_forecast[col].plot(legend=True, ax=ax).autoscale(axis='x', tight=True)\n",
    "\n",
    "                ax.set_title(col + \": Forecast vs Actuals\")\n",
    "                ax.xaxis.set_ticks_position('none')\n",
    "                ax.yaxis.set_ticks_position('none')\n",
    "                ax.spines[\"top\"].set_alpha(0)\n",
    "                ax.tick_params(labelsize=6)\n",
    "\n",
    "            plt.tight_layout()\n",
    "        true_mean = self.test[self.target].mean()\n",
    "        predict_mean = df_forecast[self.target].mean()\n",
    "        \n",
    "        return df_forecast[self.target].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM,Dropout,Bidirectional\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(signal_data, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(signal_data)-look_back):\n",
    "        dataX.append(signal_data[i:(i+look_back), 0])\n",
    "        dataY.append(signal_data[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lstm_model:\n",
    "    \n",
    "    def __init__(self, df, target, look_back, predict_range, mode):\n",
    "        self.df = df[[target]]\n",
    "        self.target = target\n",
    "        self.look_back = 5\n",
    "        self.predict_range = predict_range\n",
    "        self.mode = mode\n",
    "        \n",
    "    def univariate(self):\n",
    "        if self.mode == 'train':\n",
    "            signal_data =self.df\n",
    "            # 데이터 전처리\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            signal_data = scaler.fit_transform(signal_data)\n",
    "\n",
    "            # 데이터 분리\n",
    "            train = signal_data[0:-self.predict_range]\n",
    "            test = signal_data[-self.predict_range:]\n",
    "\n",
    "            # 데이터셋 생성\n",
    "            x_train, y_train = create_dataset(train, self.look_back)\n",
    "            x_test, y_test = create_dataset(test, self.look_back)\n",
    "\n",
    "            x_train = np.reshape(x_train,(x_train.shape[0],self.look_back,1)) #(size, timestamp,feature)\n",
    "            x_test = np.reshape(x_test,(x_test.shape[0],self.look_back,1))\n",
    "\n",
    "            return x_train, x_test, y_train, y_test,scaler, self.look_back\n",
    "        \n",
    "        else:\n",
    "            signal_data = self.df\n",
    "            # 데이터 전처리\n",
    "\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            signal_data = scaler.fit_transform(signal_data)\n",
    "            last_data = signal_data[-self.look_back:]\n",
    "            # 데이터셋 생성\n",
    "            x_train, y_train = create_dataset(signal_data, self.look_back)\n",
    "\n",
    "            x_train = np.reshape(x_train,(x_train.shape[0],self.look_back,1)) #(size, timestamp,feature)\n",
    "            #x_test = np.reshape(x_test,(x_test.shape[0],look_back,1))\n",
    "\n",
    "            return x_train, y_train,scaler, self.look_back, last_data\n",
    "    \n",
    "    def run_stateful(self, x_train, x_test, y_train, y_test, look_back, scaler,box1):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(box1, batch_input_shape=(1, look_back, 1), stateful=True))\n",
    "        model.add(Dropout(0.1))\n",
    "#         model.add(LSTM(box2, batch_input_shape=(1, look_back, 1), stateful=True))\n",
    "#         model.add(Dropout(0.1))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='loss', patience=3, verbose=1)\n",
    "\n",
    "        model.fit(x_train, y_train, epochs=100, batch_size=1, verbose=1, callbacks=[early_stop])\n",
    "        score = model.evaluate(x_test,y_test,batch_size=1)\n",
    "        pre =model.predict(x_test,batch_size=1) #모든 batch_size 바꾸면 error\n",
    "        #sc_pre = scaler.inverse_transform(pre)\n",
    "        ans = y_test\n",
    "\n",
    "        return score, pre, ans\n",
    "    \n",
    "    def run_stateful_ans(self, x_train,  y_train, look_back,box):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(box, batch_input_shape=(1, look_back, 1), stateful=True))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='loss', patience=3, verbose=1)\n",
    "\n",
    "        model.fit(x_train, y_train, epochs=100, batch_size=1, verbose=1, callbacks=[early_stop])\n",
    "        return model\n",
    "    \n",
    "    def run(self):\n",
    "        if self.mode == 'train':\n",
    "            x_train, x_test, y_train, y_test,scaler,look_back = self.univariate()\n",
    "            score, pre, ans = self.run_stateful_stack(x_train, x_test, y_train, y_test,look_back, scaler,5)\n",
    "            sc_pre=scaler.inverse_transform(pre)\n",
    "            sc_ans=scaler.inverse_transform(ans.reshape(ans.shape[0],1))\n",
    "            predict_mean = sc_pre.mean()\n",
    "            real_mean = sc_ans.mean()\n",
    "        \n",
    "            return predict_mean, real_mean\n",
    "    \n",
    "        else:\n",
    "            x_train, y_train,scaler, look_back,last_data = self.univariate()\n",
    "            model = self.run_stateful_ans(x_train,  y_train, look_back,5)\n",
    "            pred_date = self.predict_range\n",
    "            seq_in = [i[0] for i in last_data]\n",
    "            seq_out = []\n",
    "            for i in range(pred_date):\n",
    "                #sample_in = np.array(seq_in)\n",
    "                sample_in = np.reshape(seq_in, (1, look_back,1)) # batch_size, feature\n",
    "                pred_out = model.predict(sample_in)[0][0]\n",
    "                seq_out.append(pred_out)\n",
    "                seq_in.append(pred_out)\n",
    "                seq_in.pop(0)\n",
    "            ans = scaler.inverse_transform(np.array(seq_out).reshape(pred_date,1))\n",
    "            ans_mean=ans.mean()\n",
    "            return ans_mean\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dnn_score:\n",
    "    def __init__(self,AVG,ERA,team,hiter,pitcher,mode):\n",
    "        self.AVG =AVG\n",
    "        self.ERA =ERA\n",
    "        self.team = team\n",
    "        self.hiter = hiter\n",
    "        self.pitcher = pitcher\n",
    "        self.mode = mode\n",
    "        \n",
    "    def prepro(self, hiter, pitcher): #전처리\n",
    "        \n",
    "        hiter['AVG']= hiter['HIT']/hiter['AB']\n",
    "        hiter[\"y_m\"]=hiter.GDAY_DS.astype(str).str[2:6]\n",
    "        hiter=hiter[[\"T_ID\",\"AVG\",'y_m']]\n",
    "        #del hiter['HIT'], hiter['AB'], hiter[\"GDAY_DS\"]\n",
    "    \n",
    "        pitcher[\"ERA\"] = 27*pitcher.ER/pitcher.INN2\n",
    "        pitcher.WLS = pitcher.WLS.map({'W':1,'D':0.5,\"L\":0})\n",
    "        pitcher[\"y_m\"]=pitcher.GDAY_DS.astype(str).str[2:6]\n",
    "        pitcher=pitcher[[\"T_ID\",\"WLS\",\"ERA\",\"y_m\"]]\n",
    "        #del pitcher[\"ER\"], pitcher[\"INN2\"],pitcher[\"GDAY_DS\"]\n",
    "    \n",
    "        return hiter, pitcher\n",
    "    \n",
    "    def make_data(self):\n",
    "        hiter, pitcher = self.prepro(self.hiter, self.pitcher)\n",
    "        team_list = ['WO','OB','NC','SK','LG','KT','LT','HT','SS','HH']\n",
    "        df_mean=pd.DataFrame()\n",
    "        code=0\n",
    "        for team in team_list:\n",
    "            code +=0.1\n",
    "            pitcher_team= pitcher[pitcher.T_ID==team]\n",
    "            hiter_team=hiter[hiter.T_ID==team]\n",
    "            pitcher_mean = pitcher_team.groupby(['y_m']).mean()\n",
    "            pitcher_mean[\"count\"] =pitcher_team.groupby(['y_m']).count().T_ID\n",
    "            hiter_mean = hiter_team.groupby(['y_m']).mean()\n",
    "            df = pd.concat([pitcher_mean, hiter_mean], axis=1)\n",
    "            df[\"team_code\"] = code\n",
    "            df_mean=pd.concat([df_mean,df])\n",
    "    \n",
    "        df_mean.reset_index(inplace=True, drop=True)\n",
    "        df_mean = df_mean[df_mean[\"count\"] > 9]\n",
    "        df_mean.reset_index(inplace=True, drop=True)\n",
    "        del df_mean[\"count\"]\n",
    "        \n",
    "        sc_era = MinMaxScaler()\n",
    "        sc_avg = MinMaxScaler()\n",
    "        df_mean[\"AVG\"]=sc_avg.fit_transform(df_mean[[\"AVG\"]])\n",
    "        df_mean[\"ERA\"]=1-sc_era.fit_transform(df_mean[[\"ERA\"]])\n",
    "\n",
    "        df_mean=df_mean.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            x_train=df_mean.iloc[:240,1:]\n",
    "            y_train=df_mean.iloc[:240,0]\n",
    "            x_test=df_mean.iloc[240:,1:]\n",
    "            y_test=df_mean.iloc[240:,0]\n",
    "            return x_train, y_train,x_test,y_test, sc_era,sc_avg\n",
    "        \n",
    "        else:\n",
    "            x_train=df_mean.iloc[:,1:]\n",
    "            y_train=df_mean.iloc[:,0]\n",
    "            \n",
    "            return x_train, y_train, sc_era,sc_avg\n",
    "    \n",
    "    def run(self):\n",
    "        if self.mode == \"train\":\n",
    "            x_train, y_train,x_test,y_test, sc_era,sc_avg = self.make_data()\n",
    "        else:\n",
    "            x_train, y_train, sc_era,sc_avg = self.make_data()\n",
    "        model=Sequential()\n",
    "        model.add(Dense(20,activation='relu',input_dim=3))\n",
    "        model.add(Dense(5,activation='relu'))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "        #model.reset_states()\n",
    "        model.fit(x_train, y_train, epochs=100, validation_split=0.15,callbacks=[early_stop])\n",
    "        \n",
    "        team_map = {'WO':0.1,'OB':0.2,'NC':0.3,'SK':0.4,'LG':0.5,'KT':0.6,'LT':0.7,'HT':0.8,'SS':0.9,'HH':1.0}\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            evaluate = model.evaluate(x_test, y_test)\n",
    "            pre=model.predict(x_test)\n",
    "            ny_test=y_test.values.reshape(y_test.shape[0],1)\n",
    "            print(evaluate)\n",
    "            print(np.concatenate((pre,ny_test),axis=1))\n",
    "        else:\n",
    "            final_df=pd.DataFrame({\"AVG\":[self.AVG],\"ERA\":[self.ERA],\"team_code\":[team_map[self.team]]})\n",
    "            final_df[\"AVG\"] = sc_avg.transform(final_df[[\"AVG\"]])\n",
    "            final_df[\"ERA\"] = 1-sc_era.transform(final_df[[\"ERA\"]])\n",
    "            win_rate= model.predict(final_df)\n",
    "        \n",
    "            return win_rate[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ERA_ensemble(team, predict_range):\n",
    "    \n",
    "    var_df = var_pitcher_eda(team)\n",
    "    \n",
    "    obj1 = LGBM_pitcher(lgbm_pitcher_eda, team, predict_range, 'lgbm')\n",
    "    model1_predict = obj1.run()\n",
    "    \n",
    "    obj2 = LGBM_pitcher(lgbm_pitcher_eda, team, predict_range, 'rf')\n",
    "    model2_predict = obj2.run()\n",
    "    \n",
    "    obj3 = LGBM_pitcher(lgbm_pitcher_eda, team, predict_range, 'SVR')\n",
    "    model3_predict = obj3.run()\n",
    "    \n",
    "    obj4 = Var_automation(var_df, 'ERA', predict_range, mode='inference')\n",
    "    model4_predict = obj4.run()\n",
    "    \n",
    "    obj5 = Lstm_model(var_df, 'ERA', 5, predict_range, mode='inference')\n",
    "    model5_predict = obj5.run()\n",
    "    \n",
    "    list_2020 = []\n",
    "    for i,v in enumerate(var_df.index):\n",
    "        if str(v).split('-')[0] == '2020':\n",
    "            list_2020.append(i)\n",
    "    var_df_2020 = var_df.iloc[list_2020[0]:,:]\n",
    "    \n",
    "    past_mean = var_df_2020['ERA'].mean() #해당 연도의 평균만\n",
    "    \n",
    "    predict_list = [model1_predict, model2_predict, model3_predict, model4_predict, model5_predict, past_mean]\n",
    "    \n",
    "#     X = np.array(predict_list)\n",
    "#     X = X.reshape(1,-1)\n",
    "    \n",
    "#     lr_from_pickle = joblib.load('SK_0.pkl') \n",
    "#     final_predict = lr_from_pickle.predict(X)[0,0]\n",
    "    \n",
    "    final_predict = (model1_predict+model2_predict+model3_predict+model4_predict+model5_predict+past_mean) / 6\n",
    "    \n",
    "    return predict_list, final_predict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AVG_ensemble(team, predict_range):\n",
    "    \n",
    "    var_df = var_hitter_eda(team)\n",
    "    \n",
    "    obj1 = LGBM_hitter(lgbm_hitter_eda, team, predict_range, 'lgbm')\n",
    "    model1_predict = obj1.run()\n",
    "    \n",
    "    obj2 = LGBM_hitter(lgbm_hitter_eda, team, predict_range, 'rf')\n",
    "    model2_predict = obj2.run()\n",
    "    \n",
    "    obj3 = LGBM_hitter(lgbm_hitter_eda, team, predict_range, 'SVR')\n",
    "    model3_predict = obj3.run()\n",
    "    \n",
    "    obj4 = Var_automation(var_df, 'AVG', predict_range, mode='inference')\n",
    "    model4_predict = obj4.run()\n",
    "    \n",
    "    obj5 = Lstm_model(var_df, 'AVG', 5, predict_range, mode='inference')\n",
    "    model5_predict = obj5.run()\n",
    "    \n",
    "    list_2020 = []\n",
    "    for i,v in enumerate(var_df.index):\n",
    "        if str(v).split('-')[0] == '2020':\n",
    "            list_2020.append(i)\n",
    "    var_df_2020 = var_df.iloc[list_2020[0]:,:]\n",
    "    \n",
    "    past_mean = var_df_2020['AVG'].mean() #해당 연도의 평균만\n",
    "    \n",
    "    predict_list = [model1_predict, model2_predict, model3_predict, model4_predict, model5_predict, past_mean]\n",
    "    \n",
    "    final_predict = (model1_predict+model2_predict+model3_predict+model4_predict+model5_predict+past_mean) / 6\n",
    "    \n",
    "    return predict_list, final_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odds_ensemble(team, predict_range):\n",
    "    \n",
    "    var_hitter_df = var_hitter_eda(team)\n",
    "    var_pitcher_df = var_pitcher_eda(team)\n",
    "    \n",
    "    hitter_df = pd.concat([data_team_hitter[0],data_team_hitter[1],data_team_hitter[2],data_team_hitter[3],data_team_hitter[4]])\n",
    "    pitcher_df = pd.concat([data_team_pitcher[0],data_team_pitcher[1],data_team_pitcher[2],data_team_pitcher[3],data_team_pitcher[4]])\n",
    "    \n",
    "    model1_predict = lgbm_odds_predict(team, predict_range, 'lgbm')\n",
    "    model2_predict = lgbm_odds_predict(team, predict_range, 'rf')\n",
    "    model3_predict = lgbm_odds_predict(team, predict_range, 'SVR')\n",
    "    \n",
    "    lstm_ERA_obj = Lstm_model(var_hitter_df, 'AVG', 5, predict_range, mode='inference')\n",
    "    lstm_ERA= lstm_ERA_obj.run()\n",
    "    \n",
    "    lstm_AVG_obj = Lstm_model(var_pitcher_df, 'ERA', 5, predict_range, mode='inference')\n",
    "    lstm_AVG = lstm_AVG_obj.run()\n",
    "    \n",
    "    model4_obj = dnn_score(lstm_ERA, lstm_AVG, team, hitter_df ,pitcher_df,mode='inference')\n",
    "    model4_predict = model4_obj.run()\n",
    "    \n",
    "    predict_list = [model1_predict, model2_predict, model3_predict, model4_predict]    \n",
    "        \n",
    "    final_predict = (model1_predict+model2_predict+model3_predict+model4_predict) / 4\n",
    "    \n",
    "    return predict_list, final_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, ensemble_model, target, predict_range):\n",
    "        super(Experiment, self).__init__()\n",
    "        \n",
    "        self.model = ensemble_model\n",
    "        self.target = target\n",
    "        self.predict_range = predict_range\n",
    "    \n",
    "    def run(self):\n",
    "        model_key = 'Pred.{}.{},for {} matches'.format(self.target, self.model.__name__,self.predict_range)\n",
    "        real_key = 'Real.{},for {} matches'.format(self.target, self.predict_range)\n",
    "        dict = {'team': [], real_key: [], model_key : []}\n",
    "\n",
    "        #2016~2019, OB, LG, WO, SK, KT, HH, HT, SS, LT, NC\n",
    "        \n",
    "        for team in ['OB', 'LG', 'WO', 'SK', 'KT', 'HH', 'HT', 'SS', 'LT', 'NC']:\n",
    "\n",
    "            try:\n",
    "                model_result = self.model(team,self.predict_range)\n",
    "                real_result = add_data(team, self.target, self.predict_range)\n",
    "    \n",
    "                dict['team'].append(team)\n",
    "\n",
    "                dict[real_key].append(real_result)\n",
    "                dict[model_key].append(model_result)\n",
    "            except:\n",
    "                print(team)\n",
    "                continue\n",
    "                    \n",
    "        \n",
    "        df = pd.DataFrame(dict)\n",
    "        \n",
    "        #모델 오차의 절대값  \n",
    "        difference_column = '{}_abs_difference'.format(self.model.__name__)\n",
    "        df[difference_column] = abs(df[real_key]-df[model_key])\n",
    "        model_difference_mean =  df[difference_column].mean()\n",
    "        model_difference_std = df[difference_column].std()\n",
    "        \n",
    "        print(\"{} 모델의 오차 절대값 평균:\".format(self.model.__name__), model_difference_mean)\n",
    "        print(\"{} 모델의 오차 절대값 표준편차:\".format(self.model.__name__), model_difference_std)\n",
    "        display(df)\n",
    "        \n",
    "        return model_difference_mean, model_difference_std, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   45.4s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=10)]: Done 492 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=10)]: Done 960 out of 960 | elapsed:  4.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nsd96\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/100\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0304\n",
      "Epoch 2/100\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0277\n",
      "Epoch 3/100\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0277\n",
      "Epoch 4/100\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0275\n",
      "Epoch 5/100\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0269\n",
      "Epoch 6/100\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0273\n",
      "Epoch 7/100\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0274\n",
      "Epoch 8/100\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0272\n",
      "Epoch 00008: early stopping\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   38.4s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=10)]: Done 492 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=10)]: Done 960 out of 960 | elapsed:  4.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "632/632 [==============================] - 3s 4ms/step - loss: 0.0526\n",
      "Epoch 2/100\n",
      "632/632 [==============================] - 2s 4ms/step - loss: 0.0388\n",
      "Epoch 3/100\n",
      "632/632 [==============================] - 2s 4ms/step - loss: 0.0382\n",
      "Epoch 4/100\n",
      "632/632 [==============================] - 2s 4ms/step - loss: 0.0381A: \n",
      "Epoch 5/100\n",
      "632/632 [==============================] - 2s 4ms/step - loss: 0.0382\n",
      "Epoch 6/100\n",
      "632/632 [==============================] - 2s 4ms/step - loss: 0.0386\n",
      "Epoch 7/100\n",
      "632/632 [==============================] - 2s 4ms/step - loss: 0.0379\n",
      "Epoch 8/100\n",
      "632/632 [==============================] - 2s 4ms/step - loss: 0.0382\n",
      "Epoch 9/100\n",
      "632/632 [==============================] - 3s 4ms/step - loss: 0.0376\n",
      "Epoch 10/100\n",
      "632/632 [==============================] - 3s 5ms/step - loss: 0.0381\n",
      "Epoch 11/100\n",
      "632/632 [==============================] - 3s 4ms/step - loss: 0.0377\n",
      "Epoch 12/100\n",
      "632/632 [==============================] - 3s 4ms/step - loss: 0.0378\n",
      "Epoch 00012: early stopping\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   50.2s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=10)]: Done 492 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=10)]: Done 960 out of 960 | elapsed:  4.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "633/633 [==============================] - 3s 5ms/step - loss: 0.0366\n",
      "Epoch 2/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0345\n",
      "Epoch 3/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0335\n",
      "Epoch 4/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0335\n",
      "Epoch 5/100\n",
      "633/633 [==============================] - 3s 5ms/step - loss: 0.0333\n",
      "Epoch 6/100\n",
      "633/633 [==============================] - 3s 5ms/step - loss: 0.0327\n",
      "Epoch 7/100\n",
      "633/633 [==============================] - 3s 5ms/step - loss: 0.0326\n",
      "Epoch 8/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0327\n",
      "Epoch 9/100\n",
      "633/633 [==============================] - 3s 4ms/step - loss: 0.0325\n",
      "Epoch 10/100\n",
      "633/633 [==============================] - 3s 4ms/step - loss: 0.0335\n",
      "Epoch 11/100\n",
      "633/633 [==============================] - 3s 4ms/step - loss: 0.0328\n",
      "Epoch 12/100\n",
      "633/633 [==============================] - 3s 4ms/step - loss: 0.0326\n",
      "Epoch 00012: early stopping\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   48.0s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=10)]: Done 492 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=10)]: Done 960 out of 960 | elapsed:  4.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "634/634 [==============================] - 4s 6ms/step - loss: 0.0289\n",
      "Epoch 2/100\n",
      "634/634 [==============================] - 4s 6ms/step - loss: 0.0276\n",
      "Epoch 3/100\n",
      "634/634 [==============================] - 4s 6ms/step - loss: 0.0281\n",
      "Epoch 4/100\n",
      "634/634 [==============================] - 3s 5ms/step - loss: 0.0271\n",
      "Epoch 5/100\n",
      "634/634 [==============================] - 3s 5ms/step - loss: 0.0272\n",
      "Epoch 6/100\n",
      "634/634 [==============================] - 3s 5ms/step - loss: 0.0272\n",
      "Epoch 7/100\n",
      "634/634 [==============================] - 3s 5ms/step - loss: 0.0275\n",
      "Epoch 00007: early stopping\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   38.6s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=10)]: Done 492 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=10)]: Done 960 out of 960 | elapsed:  4.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "626/626 [==============================] - 3s 5ms/step - loss: 0.0420\n",
      "Epoch 2/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0386\n",
      "Epoch 3/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0383\n",
      "Epoch 4/100\n",
      "626/626 [==============================] - 3s 4ms/step - loss: 0.0385\n",
      "Epoch 5/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0378\n",
      "Epoch 6/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0371\n",
      "Epoch 7/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0372\n",
      "Epoch 8/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0367\n",
      "Epoch 9/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0364\n",
      "Epoch 10/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0360\n",
      "Epoch 11/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0366\n",
      "Epoch 12/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0364\n",
      "Epoch 13/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0369\n",
      "Epoch 00013: early stopping\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   40.6s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=10)]: Done 492 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=10)]: Done 960 out of 960 | elapsed:  4.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0371\n",
      "Epoch 2/100\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0359\n",
      "Epoch 3/100\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0352\n",
      "Epoch 4/100\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0351\n",
      "Epoch 5/100\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0342\n",
      "Epoch 6/100\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0335\n",
      "Epoch 7/100\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0344\n",
      "Epoch 8/100\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0347\n",
      "Epoch 9/100\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0341\n",
      "Epoch 00009: early stopping\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   47.1s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=10)]: Done 492 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=10)]: Done 960 out of 960 | elapsed:  4.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "629/629 [==============================] - 3s 5ms/step - loss: 0.0307\n",
      "Epoch 2/100\n",
      "629/629 [==============================] - 2s 4ms/step - loss: 0.0289\n",
      "Epoch 3/100\n",
      "629/629 [==============================] - 2s 4ms/step - loss: 0.0287\n",
      "Epoch 4/100\n",
      "629/629 [==============================] - 2s 4ms/step - loss: 0.0283\n",
      "Epoch 5/100\n",
      "629/629 [==============================] - 2s 4ms/step - loss: 0.0271\n",
      "Epoch 6/100\n",
      "629/629 [==============================] - 2s 4ms/step - loss: 0.0274\n",
      "Epoch 7/100\n",
      "629/629 [==============================] - 2s 4ms/step - loss: 0.0275\n",
      "Epoch 8/100\n",
      "629/629 [==============================] - 3s 4ms/step - loss: 0.0275\n",
      "Epoch 00008: early stopping\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   47.0s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=10)]: Done 492 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=10)]: Done 960 out of 960 | elapsed:  4.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "632/632 [==============================] - 4s 6ms/step - loss: 0.0269\n",
      "Epoch 2/100\n",
      "632/632 [==============================] - 4s 6ms/step - loss: 0.0244\n",
      "Epoch 3/100\n",
      "632/632 [==============================] - 4s 6ms/step - loss: 0.0244\n",
      "Epoch 4/100\n",
      "632/632 [==============================] - 4s 6ms/step - loss: 0.0236\n",
      "Epoch 5/100\n",
      "632/632 [==============================] - 3s 5ms/step - loss: 0.0234\n",
      "Epoch 6/100\n",
      "632/632 [==============================] - 3s 5ms/step - loss: 0.0236\n",
      "Epoch 7/100\n",
      "632/632 [==============================] - 3s 5ms/step - loss: 0.0231\n",
      "Epoch 8/100\n",
      "632/632 [==============================] - 3s 5ms/step - loss: 0.0229\n",
      "Epoch 9/100\n",
      "632/632 [==============================] - 3s 4ms/step - loss: 0.0236\n",
      "Epoch 10/100\n",
      "632/632 [==============================] - 3s 4ms/step - loss: 0.0235\n",
      "Epoch 11/100\n",
      "632/632 [==============================] - 3s 4ms/step - loss: 0.0232\n",
      "Epoch 00011: early stopping\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   38.1s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=10)]: Done 492 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=10)]: Done 960 out of 960 | elapsed:  4.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "631/631 [==============================] - 3s 5ms/step - loss: 0.0343\n",
      "Epoch 2/100\n",
      "631/631 [==============================] - 3s 4ms/step - loss: 0.0326\n",
      "Epoch 3/100\n",
      "631/631 [==============================] - 3s 4ms/step - loss: 0.0319\n",
      "Epoch 4/100\n",
      "631/631 [==============================] - 3s 4ms/step - loss: 0.0314\n",
      "Epoch 5/100\n",
      "631/631 [==============================] - 3s 4ms/step - loss: 0.0307\n",
      "Epoch 6/100\n",
      "631/631 [==============================] - 3s 4ms/step - loss: 0.0312\n",
      "Epoch 7/100\n",
      "631/631 [==============================] - 3s 4ms/step - loss: 0.0310\n",
      "Epoch 8/100\n",
      "631/631 [==============================] - 3s 4ms/step - loss: 0.0310\n",
      "Epoch 00008: early stopping\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   53.4s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=10)]: Done 492 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=10)]: Done 960 out of 960 | elapsed:  4.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0351\n",
      "Epoch 2/100\n",
      "633/633 [==============================] - 3s 5ms/step - loss: 0.0332\n",
      "Epoch 3/100\n",
      "633/633 [==============================] - 3s 5ms/step - loss: 0.0317\n",
      "Epoch 4/100\n",
      "633/633 [==============================] - 3s 5ms/step - loss: 0.0313\n",
      "Epoch 5/100\n",
      "633/633 [==============================] - 3s 5ms/step - loss: 0.0317\n",
      "Epoch 6/100\n",
      "633/633 [==============================] - 3s 5ms/step - loss: 0.0310\n",
      "Epoch 7/100\n",
      "633/633 [==============================] - 3s 4ms/step - loss: 0.0314\n",
      "Epoch 8/100\n",
      "633/633 [==============================] - 3s 4ms/step - loss: 0.0301\n",
      "Epoch 9/100\n",
      "633/633 [==============================] - 3s 4ms/step - loss: 0.0306\n",
      "Epoch 10/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0309\n",
      "Epoch 11/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0314\n",
      "Epoch 00011: early stopping\n",
      "ERA_ensemble 모델의 오차 절대값 평균: 0.9791649514981998\n",
      "ERA_ensemble 모델의 오차 절대값 표준편차: 0.923085539229192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team</th>\n",
       "      <th>Real.ERA,for 20 matches</th>\n",
       "      <th>Pred.ERA.ERA_ensemble,for 20 matches</th>\n",
       "      <th>ERA_ensemble_abs_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OB</td>\n",
       "      <td>4.316806</td>\n",
       "      <td>4.569628</td>\n",
       "      <td>0.252822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LG</td>\n",
       "      <td>4.475000</td>\n",
       "      <td>4.869038</td>\n",
       "      <td>0.394038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WO</td>\n",
       "      <td>3.186250</td>\n",
       "      <td>5.297228</td>\n",
       "      <td>2.110978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SK</td>\n",
       "      <td>7.656389</td>\n",
       "      <td>4.745283</td>\n",
       "      <td>2.911106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KT</td>\n",
       "      <td>3.725762</td>\n",
       "      <td>5.086276</td>\n",
       "      <td>1.360514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HH</td>\n",
       "      <td>5.886219</td>\n",
       "      <td>5.167359</td>\n",
       "      <td>0.718861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HT</td>\n",
       "      <td>4.903503</td>\n",
       "      <td>4.941197</td>\n",
       "      <td>0.037694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SS</td>\n",
       "      <td>5.244203</td>\n",
       "      <td>5.089411</td>\n",
       "      <td>0.154792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LT</td>\n",
       "      <td>4.084444</td>\n",
       "      <td>5.025987</td>\n",
       "      <td>0.941542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NC</td>\n",
       "      <td>5.292409</td>\n",
       "      <td>4.383106</td>\n",
       "      <td>0.909302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  team  Real.ERA,for 20 matches  Pred.ERA.ERA_ensemble,for 20 matches  \\\n",
       "0   OB                 4.316806                              4.569628   \n",
       "1   LG                 4.475000                              4.869038   \n",
       "2   WO                 3.186250                              5.297228   \n",
       "3   SK                 7.656389                              4.745283   \n",
       "4   KT                 3.725762                              5.086276   \n",
       "5   HH                 5.886219                              5.167359   \n",
       "6   HT                 4.903503                              4.941197   \n",
       "7   SS                 5.244203                              5.089411   \n",
       "8   LT                 4.084444                              5.025987   \n",
       "9   NC                 5.292409                              4.383106   \n",
       "\n",
       "   ERA_ensemble_abs_difference  \n",
       "0                     0.252822  \n",
       "1                     0.394038  \n",
       "2                     2.110978  \n",
       "3                     2.911106  \n",
       "4                     1.360514  \n",
       "5                     0.718861  \n",
       "6                     0.037694  \n",
       "7                     0.154792  \n",
       "8                     0.941542  \n",
       "9                     0.909302  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.9791649514981998,\n",
       " 0.923085539229192,\n",
       "   team  Real.ERA,for 20 matches  Pred.ERA.ERA_ensemble,for 20 matches  \\\n",
       " 0   OB                 4.316806                              4.569628   \n",
       " 1   LG                 4.475000                              4.869038   \n",
       " 2   WO                 3.186250                              5.297228   \n",
       " 3   SK                 7.656389                              4.745283   \n",
       " 4   KT                 3.725762                              5.086276   \n",
       " 5   HH                 5.886219                              5.167359   \n",
       " 6   HT                 4.903503                              4.941197   \n",
       " 7   SS                 5.244203                              5.089411   \n",
       " 8   LT                 4.084444                              5.025987   \n",
       " 9   NC                 5.292409                              4.383106   \n",
       " \n",
       "    ERA_ensemble_abs_difference  \n",
       " 0                     0.252822  \n",
       " 1                     0.394038  \n",
       " 2                     2.110978  \n",
       " 3                     2.911106  \n",
       " 4                     1.360514  \n",
       " 5                     0.718861  \n",
       " 6                     0.037694  \n",
       " 7                     0.154792  \n",
       " 8                     0.941542  \n",
       " 9                     0.909302  )"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_era = Experiment(ERA_ensemble, 'ERA', 20)\n",
    "experiment_era.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from datetime import datetime\n",
    "\n",
    "def find_weight_era(team, predict_range):\n",
    "\n",
    "    predict_list, _ = ERA_ensemble(team, predict_range)\n",
    "    X = np.array(predict_list)\n",
    "    X = X.reshape(1,-1)\n",
    "\n",
    "    y = add_data(team,'ERA')\n",
    "    y = np.array(y)\n",
    "    y = y.reshape(1,-1)\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X,y)\n",
    "\n",
    "    now = datetime.now()\n",
    "    nowDatetime = now.strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    \n",
    "    file_name = './model_weight/ERA_{}_{}_{}.pkl'.format(team,predict_range,nowDatetime)\n",
    "    joblib.dump(lr, file_name)\n",
    "    \n",
    "    return file_name\n",
    "    \n",
    "def find_weight_avg(team, predict_range):\n",
    "\n",
    "    predict_list, _ = AVG_ensemble(team, predict_range)\n",
    "    X = np.array(predict_list)\n",
    "    X = X.reshape(1,-1)\n",
    "\n",
    "    y = add_data(team,'AVG')\n",
    "    y = np.array(y)\n",
    "    y = y.reshape(1,-1)\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X,y)\n",
    "\n",
    "    now = datetime.now()\n",
    "    nowDatetime = now.strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    \n",
    "    file_name = './model_weight/AVG_{}_{}_{}.pkl'.format(team,predict_range,nowDatetime)\n",
    "    joblib.dump(lr, file_name)\n",
    "    \n",
    "    return file_name\n",
    "\n",
    "def find_weight_odds(team, predict_range):\n",
    "\n",
    "    predict_list, _ = odds_ensemble(team, predict_range)\n",
    "    X = np.array(predict_list)\n",
    "    X = X.reshape(1,-1)\n",
    "\n",
    "    y = add_data(team,'odds')\n",
    "    y = np.array(y)\n",
    "    y = y.reshape(1,-1)\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X,y)\n",
    "\n",
    "    now = datetime.now()\n",
    "    nowDatetime = now.strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    \n",
    "    file_name = './model_weight/odds_{}_{}_{}.pkl'.format(team,predict_range,nowDatetime)\n",
    "    joblib.dump(lr, file_name)\n",
    "    \n",
    "    return file_name\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   38.1s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "632/632 [==============================] - 6s 9ms/step - loss: 0.0295\n",
      "Epoch 2/100\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.0241\n",
      "Epoch 3/100\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.0234\n",
      "Epoch 4/100\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.0235\n",
      "Epoch 5/100\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.0234\n",
      "Epoch 6/100\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.0236\n",
      "Epoch 00006: early stopping\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   37.4s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "631/631 [==============================] - 5s 9ms/step - loss: 0.0346\n",
      "Epoch 2/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0327\n",
      "Epoch 3/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0310\n",
      "Epoch 4/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0310\n",
      "Epoch 5/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0318\n",
      "Epoch 6/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0309\n",
      "Epoch 7/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0313\n",
      "Epoch 8/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0306\n",
      "Epoch 9/100\n",
      "631/631 [==============================] - 5s 7ms/step - loss: 0.0313\n",
      "Epoch 10/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0300\n",
      "Epoch 11/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0305\n",
      "Epoch 12/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0305\n",
      "Epoch 13/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0301\n",
      "Epoch 00013: early stopping\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   41.4s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "633/633 [==============================] - 6s 9ms/step - loss: 0.0338\n",
      "Epoch 2/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0325\n",
      "Epoch 3/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0314\n",
      "Epoch 4/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0317\n",
      "Epoch 5/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0314\n",
      "Epoch 6/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0305\n",
      "Epoch 7/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0309\n",
      "Epoch 8/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0310\n",
      "Epoch 9/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0309\n",
      "Epoch 00009: early stopping\n"
     ]
    }
   ],
   "source": [
    "for team in ['OB', 'LG', 'WO', 'SK', 'KT', 'HH', 'HT', 'SS', 'LT', 'NC']:\n",
    "\n",
    "    len_df, mean_df = add_data(team,'ERA')\n",
    "    find_weight_era(team, len_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for team in ['OB', 'LG', 'WO', 'SK', 'KT', 'HH', 'HT', 'SS', 'LT', 'NC']:\n",
    "\n",
    "    len_df, mean_df = add_data(team,'AVG')\n",
    "    find_weight_avg(team, len_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ERA_ensemble_weighted(team, predict_range, weight_pkl):\n",
    "    \n",
    "    var_df = var_pitcher_eda(team)\n",
    "    \n",
    "    obj1 = LGBM_pitcher(lgbm_pitcher_eda, team, predict_range, 'lgbm')\n",
    "    model1_predict = obj1.run()\n",
    "    \n",
    "    obj2 = LGBM_pitcher(lgbm_pitcher_eda, team, predict_range, 'rf')\n",
    "    model2_predict = obj2.run()\n",
    "    \n",
    "    obj3 = LGBM_pitcher(lgbm_pitcher_eda, team, predict_range, 'SVR')\n",
    "    model3_predict = obj3.run()\n",
    "    \n",
    "    obj4 = Var_automation(var_df, 'ERA', predict_range, mode='inference')\n",
    "    model4_predict = obj4.run()\n",
    "    \n",
    "    obj5 = Lstm_model(var_df, 'ERA', 5, predict_range, mode='inference')\n",
    "    model5_predict = obj5.run()\n",
    "    \n",
    "    list_2020 = []\n",
    "    for i,v in enumerate(var_df.index):\n",
    "        if str(v).split('-')[0] == '2020':\n",
    "            list_2020.append(i)\n",
    "    var_df_2020 = var_df.iloc[list_2020[0]:,:]\n",
    "    \n",
    "    past_mean = var_df_2020['ERA'].mean() #해당 연도의 평균만\n",
    "    \n",
    "    predict_list = [model1_predict, model2_predict, model3_predict, model4_predict, model5_predict, past_mean]\n",
    "    \n",
    "    X = np.array(predict_list)\n",
    "    X = X.reshape(1,-1)\n",
    "    \n",
    "    lr_from_pickle = joblib.load(weight_pkl) \n",
    "    final_predict = lr_from_pickle.predict(X)[0,0]\n",
    "    \n",
    "#     final_predict = (model1_predict+model2_predict+model3_predict+model4_predict+model5_predict+past_mean) / 6\n",
    "    \n",
    "    return predict_list, final_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AVG_ensemble_weighted(team, predict_range, weight_pkl):\n",
    "    \n",
    "    var_df = var_hitter_eda(team)\n",
    "    \n",
    "    obj1 = LGBM_hitter(lgbm_hitter_eda, team, predict_range, 'lgbm')\n",
    "    model1_predict = obj1.run()\n",
    "    \n",
    "    obj2 = LGBM_hitter(lgbm_hitter_eda, team, predict_range, 'rf')\n",
    "    model2_predict = obj2.run()\n",
    "    \n",
    "    obj3 = LGBM_hitter(lgbm_hitter_eda, team, predict_range, 'SVR')\n",
    "    model3_predict = obj3.run()\n",
    "    \n",
    "    obj4 = Var_automation(var_df, 'AVG', predict_range, mode='inference')\n",
    "    model4_predict = obj4.run()\n",
    "    \n",
    "    obj5 = Lstm_model(var_df, 'AVG', 5, predict_range, mode='inference')\n",
    "    model5_predict = obj5.run()\n",
    "    \n",
    "    list_2020 = []\n",
    "    for i,v in enumerate(var_df.index):\n",
    "        if str(v).split('-')[0] == '2020':\n",
    "            list_2020.append(i)\n",
    "    var_df_2020 = var_df.iloc[list_2020[0]:,:]\n",
    "    \n",
    "    past_mean = var_df_2020['AVG'].mean() #해당 연도의 평균만\n",
    "    \n",
    "    predict_list = [model1_predict, model2_predict, model3_predict, model4_predict, model5_predict, past_mean]\n",
    "    \n",
    "    X = np.array(predict_list)\n",
    "    X = X.reshape(1,-1)\n",
    "    \n",
    "    lr_from_pickle = joblib.load(weight_pkl) \n",
    "    final_predict = lr_from_pickle.predict(X)[0,0]\n",
    "    \n",
    "#     final_predict = (model1_predict+model2_predict+model3_predict+model4_predict+model5_predict+past_mean) / 6\n",
    "    \n",
    "    return predict_list, final_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def weight_experiment_era(weight_range, predict_range):\n",
    "    dict = {'team': [], 'predict': [], 'real' : []}\n",
    "    \n",
    "    for team in ['OB', 'LG', 'WO', 'SK', 'KT', 'HH', 'HT', 'SS', 'LT', 'NC']:\n",
    "\n",
    "        weight_pkl = find_weight_era(team, weight_range)\n",
    "        _, final_predict = ERA_ensemble_weighted(team, predict_range, weight_pkl)\n",
    "\n",
    "        real_mean = add_data(team, 'ERA', predict_range)\n",
    "        \n",
    "        \n",
    "        dict['team'].append(team)\n",
    "        dict['predict'].append(final_predict)\n",
    "        dict['real'].append(real_mean)\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(dict)\n",
    "    df['abs_difference'] = abs(df['predict']-df['real'])\n",
    "    display(df)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   28.0s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nsd96\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/100\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0321\n",
      "Epoch 2/100\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0281\n",
      "Epoch 3/100\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0271\n",
      "Epoch 4/100\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0271\n",
      "Epoch 5/100\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0270\n",
      "Epoch 6/100\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0273\n",
      "Epoch 7/100\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0267\n",
      "Epoch 8/100\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0268\n",
      "Epoch 9/100\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0269\n",
      "Epoch 10/100\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0268\n",
      "Epoch 00010: early stopping\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   35.0s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0318\n",
      "Epoch 2/100\n",
      "630/630 [==============================] - ETA: 0s - loss: 0.027 - 3s 5ms/step - loss: 0.0273\n",
      "Epoch 3/100\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0272\n",
      "Epoch 4/100\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0266\n",
      "Epoch 5/100\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0276\n",
      "Epoch 6/100\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0271\n",
      "Epoch 7/100\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0270\n",
      "Epoch 00007: early stopping\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   36.2s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "632/632 [==============================] - 3s 5ms/step - loss: 0.0420\n",
      "Epoch 2/100\n",
      "632/632 [==============================] - 3s 4ms/step - loss: 0.0387\n",
      "Epoch 3/100\n",
      "632/632 [==============================] - 3s 5ms/step - loss: 0.0387\n",
      "Epoch 4/100\n",
      "632/632 [==============================] - 3s 5ms/step - loss: 0.0378\n",
      "Epoch 5/100\n",
      "632/632 [==============================] - 3s 5ms/step - loss: 0.0380\n",
      "Epoch 6/100\n",
      "632/632 [==============================] - 3s 5ms/step - loss: 0.0380\n",
      "Epoch 7/100\n",
      "632/632 [==============================] - 3s 5ms/step - loss: 0.0377\n",
      "Epoch 8/100\n",
      "632/632 [==============================] - 3s 5ms/step - loss: 0.0380\n",
      "Epoch 9/100\n",
      "632/632 [==============================] - 3s 4ms/step - loss: 0.0378\n",
      "Epoch 10/100\n",
      "632/632 [==============================] - 3s 4ms/step - loss: 0.0385\n",
      "Epoch 00010: early stopping\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "632/632 [==============================] - 3s 5ms/step - loss: 0.0411\n",
      "Epoch 2/100\n",
      "632/632 [==============================] - 3s 4ms/step - loss: 0.0399\n",
      "Epoch 3/100\n",
      "632/632 [==============================] - 2s 4ms/step - loss: 0.0389\n",
      "Epoch 4/100\n",
      "632/632 [==============================] - 2s 4ms/step - loss: 0.0397\n",
      "Epoch 5/100\n",
      "632/632 [==============================] - 2s 4ms/step - loss: 0.0389\n",
      "Epoch 6/100\n",
      "632/632 [==============================] - 2s 4ms/step - loss: 0.0390\n",
      "Epoch 7/100\n",
      "632/632 [==============================] - 3s 4ms/step - loss: 0.0382\n",
      "Epoch 8/100\n",
      "632/632 [==============================] - 3s 4ms/step - loss: 0.0379\n",
      "Epoch 9/100\n",
      "632/632 [==============================] - 3s 4ms/step - loss: 0.0385\n",
      "Epoch 10/100\n",
      "632/632 [==============================] - 3s 4ms/step - loss: 0.0381\n",
      "Epoch 11/100\n",
      "632/632 [==============================] - 3s 4ms/step - loss: 0.0379\n",
      "Epoch 12/100\n",
      "632/632 [==============================] - 3s 5ms/step - loss: 0.0376\n",
      "Epoch 13/100\n",
      "632/632 [==============================] - 3s 4ms/step - loss: 0.0378\n",
      "Epoch 14/100\n",
      "632/632 [==============================] - 3s 5ms/step - loss: 0.0377\n",
      "Epoch 15/100\n",
      "632/632 [==============================] - 3s 5ms/step - loss: 0.0379\n",
      "Epoch 00015: early stopping\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   38.6s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "633/633 [==============================] - 3s 5ms/step - loss: 0.0373\n",
      "Epoch 2/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0350\n",
      "Epoch 3/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0332\n",
      "Epoch 4/100\n",
      "633/633 [==============================] - 3s 4ms/step - loss: 0.0333\n",
      "Epoch 5/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0332\n",
      "Epoch 6/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0329\n",
      "Epoch 7/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0333\n",
      "Epoch 8/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0327\n",
      "Epoch 9/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0329\n",
      "Epoch 10/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0325\n",
      "Epoch 11/100\n",
      "633/633 [==============================] - 3s 4ms/step - loss: 0.0327\n",
      "Epoch 12/100\n",
      "633/633 [==============================] - 3s 4ms/step - loss: 0.0326\n",
      "Epoch 13/100\n",
      "633/633 [==============================] - 3s 4ms/step - loss: 0.0328\n",
      "Epoch 00013: early stopping\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   40.0s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "633/633 [==============================] - 3s 5ms/step - loss: 0.0354\n",
      "Epoch 2/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0345\n",
      "Epoch 3/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0341\n",
      "Epoch 4/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0334\n",
      "Epoch 5/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0338\n",
      "Epoch 6/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0331\n",
      "Epoch 7/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0337\n",
      "Epoch 8/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0331\n",
      "Epoch 9/100\n",
      "633/633 [==============================] - 3s 4ms/step - loss: 0.0330\n",
      "Epoch 10/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0331\n",
      "Epoch 11/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0324\n",
      "Epoch 12/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0329\n",
      "Epoch 13/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0329\n",
      "Epoch 14/100\n",
      "633/633 [==============================] - 2s 4ms/step - loss: 0.0327\n",
      "Epoch 00014: early stopping\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   34.2s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "634/634 [==============================] - 3s 6ms/step - loss: 0.0303\n",
      "Epoch 2/100\n",
      "634/634 [==============================] - 3s 5ms/step - loss: 0.0280\n",
      "Epoch 3/100\n",
      "634/634 [==============================] - 3s 5ms/step - loss: 0.0275\n",
      "Epoch 4/100\n",
      "634/634 [==============================] - 2s 4ms/step - loss: 0.0273\n",
      "Epoch 5/100\n",
      "634/634 [==============================] - 2s 4ms/step - loss: 0.0277\n",
      "Epoch 6/100\n",
      "634/634 [==============================] - 2s 3ms/step - loss: 0.0276\n",
      "Epoch 7/100\n",
      "634/634 [==============================] - 2s 4ms/step - loss: 0.0271\n",
      "Epoch 8/100\n",
      "634/634 [==============================] - 2s 3ms/step - loss: 0.0270\n",
      "Epoch 9/100\n",
      "634/634 [==============================] - 2s 3ms/step - loss: 0.0270\n",
      "Epoch 10/100\n",
      "634/634 [==============================] - 3s 4ms/step - loss: 0.0268\n",
      "Epoch 11/100\n",
      "634/634 [==============================] - 2s 4ms/step - loss: 0.0267\n",
      "Epoch 12/100\n",
      "634/634 [==============================] - 2s 3ms/step - loss: 0.0268\n",
      "Epoch 13/100\n",
      "634/634 [==============================] - 2s 3ms/step - loss: 0.0271\n",
      "Epoch 14/100\n",
      "634/634 [==============================] - 2s 4ms/step - loss: 0.0270\n",
      "Epoch 00014: early stopping\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   36.5s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "634/634 [==============================] - 3s 5ms/step - loss: 0.0351\n",
      "Epoch 2/100\n",
      "634/634 [==============================] - 3s 5ms/step - loss: 0.0291\n",
      "Epoch 3/100\n",
      "634/634 [==============================] - 3s 5ms/step - loss: 0.0277\n",
      "Epoch 4/100\n",
      "634/634 [==============================] - 3s 5ms/step - loss: 0.0279\n",
      "Epoch 5/100\n",
      "634/634 [==============================] - 3s 5ms/step - loss: 0.0272\n",
      "Epoch 6/100\n",
      "634/634 [==============================] - 3s 5ms/step - loss: 0.0272\n",
      "Epoch 7/100\n",
      "634/634 [==============================] - 3s 5ms/step - loss: 0.0275\n",
      "Epoch 8/100\n",
      "634/634 [==============================] - 3s 5ms/step - loss: 0.0273\n",
      "Epoch 9/100\n",
      "634/634 [==============================] - 3s 5ms/step - loss: 0.0270\n",
      "Epoch 10/100\n",
      "634/634 [==============================] - 3s 4ms/step - loss: 0.0271\n",
      "Epoch 11/100\n",
      "634/634 [==============================] - 3s 4ms/step - loss: 0.0270\n",
      "Epoch 12/100\n",
      "634/634 [==============================] - 3s 4ms/step - loss: 0.0267\n",
      "Epoch 13/100\n",
      "634/634 [==============================] - 3s 4ms/step - loss: 0.0267\n",
      "Epoch 14/100\n",
      "634/634 [==============================] - 2s 4ms/step - loss: 0.0268\n",
      "Epoch 15/100\n",
      "634/634 [==============================] - 2s 4ms/step - loss: 0.0268\n",
      "Epoch 16/100\n",
      "634/634 [==============================] - 2s 4ms/step - loss: 0.0269\n",
      "Epoch 00016: early stopping\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   37.0s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "626/626 [==============================] - 3s 5ms/step - loss: 0.0499\n",
      "Epoch 2/100\n",
      "626/626 [==============================] - 3s 4ms/step - loss: 0.0388\n",
      "Epoch 3/100\n",
      "626/626 [==============================] - 3s 4ms/step - loss: 0.0375\n",
      "Epoch 4/100\n",
      "626/626 [==============================] - 3s 4ms/step - loss: 0.0370\n",
      "Epoch 5/100\n",
      "626/626 [==============================] - 3s 4ms/step - loss: 0.0368\n",
      "Epoch 6/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0373\n",
      "Epoch 7/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0370\n",
      "Epoch 8/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0366\n",
      "Epoch 9/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0367\n",
      "Epoch 10/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0366\n",
      "Epoch 11/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0364\n",
      "Epoch 12/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0362\n",
      "Epoch 13/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0365\n",
      "Epoch 14/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0364\n",
      "Epoch 15/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0366\n",
      "Epoch 00015: early stopping\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   35.9s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "626/626 [==============================] - 3s 5ms/step - loss: 0.0454\n",
      "Epoch 2/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0387\n",
      "Epoch 3/100\n",
      "626/626 [==============================] - 3s 4ms/step - loss: 0.0377\n",
      "Epoch 4/100\n",
      "626/626 [==============================] - 3s 4ms/step - loss: 0.0381\n",
      "Epoch 5/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0369\n",
      "Epoch 6/100\n",
      "626/626 [==============================] - 3s 4ms/step - loss: 0.0373\n",
      "Epoch 7/100\n",
      "626/626 [==============================] - 2s 4ms/step - loss: 0.0370\n",
      "Epoch 8/100\n",
      "626/626 [==============================] - 3s 4ms/step - loss: 0.0371\n",
      "Epoch 00008: early stopping\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   45.9s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0364\n",
      "Epoch 2/100\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0359\n",
      "Epoch 3/100\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0356\n",
      "Epoch 4/100\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0346\n",
      "Epoch 5/100\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0344\n",
      "Epoch 6/100\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0348\n",
      "Epoch 7/100\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0342\n",
      "Epoch 8/100\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0341\n",
      "Epoch 9/100\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0341\n",
      "Epoch 10/100\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0338\n",
      "Epoch 11/100\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0343\n",
      "Epoch 12/100\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0338\n",
      "Epoch 13/100\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0339\n",
      "Epoch 00013: early stopping\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   40.7s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "630/630 [==============================] - 6s 9ms/step - loss: 0.0430\n",
      "Epoch 2/100\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0347\n",
      "Epoch 3/100\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0355\n",
      "Epoch 4/100\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0349\n",
      "Epoch 5/100\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0347\n",
      "Epoch 6/100\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0344\n",
      "Epoch 7/100\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0338\n",
      "Epoch 8/100\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0341\n",
      "Epoch 9/100\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0342\n",
      "Epoch 10/100\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0341\n",
      "Epoch 00010: early stopping\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   40.2s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "629/629 [==============================] - 6s 9ms/step - loss: 0.0320\n",
      "Epoch 2/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0306\n",
      "Epoch 3/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0297\n",
      "Epoch 4/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0287\n",
      "Epoch 5/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0282\n",
      "Epoch 6/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0282\n",
      "Epoch 7/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0281\n",
      "Epoch 8/100\n",
      "629/629 [==============================] - 5s 7ms/step - loss: 0.0279\n",
      "Epoch 9/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0275\n",
      "Epoch 10/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0273\n",
      "Epoch 11/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0273\n",
      "Epoch 12/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0275\n",
      "Epoch 13/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0271\n",
      "Epoch 14/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0274\n",
      "Epoch 15/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0274\n",
      "Epoch 16/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0271\n",
      "Epoch 00016: early stopping\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   41.0s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "629/629 [==============================] - 6s 10ms/step - loss: 0.0381\n",
      "Epoch 2/100\n",
      "629/629 [==============================] - 5s 7ms/step - loss: 0.0284\n",
      "Epoch 3/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0284\n",
      "Epoch 4/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0278\n",
      "Epoch 5/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0278\n",
      "Epoch 6/100\n",
      "629/629 [==============================] - 5s 7ms/step - loss: 0.0277\n",
      "Epoch 7/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0275\n",
      "Epoch 8/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0275\n",
      "Epoch 9/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0276\n",
      "Epoch 10/100\n",
      "629/629 [==============================] - 5s 7ms/step - loss: 0.0273\n",
      "Epoch 11/100\n",
      "629/629 [==============================] - 5s 7ms/step - loss: 0.0273\n",
      "Epoch 12/100\n",
      "629/629 [==============================] - 5s 7ms/step - loss: 0.0272\n",
      "Epoch 13/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0272\n",
      "Epoch 14/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0273\n",
      "Epoch 15/100\n",
      "629/629 [==============================] - 5s 7ms/step - loss: 0.0272\n",
      "Epoch 16/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0270\n",
      "Epoch 17/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0274\n",
      "Epoch 18/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0271\n",
      "Epoch 19/100\n",
      "629/629 [==============================] - 4s 7ms/step - loss: 0.0273\n",
      "Epoch 00019: early stopping\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   41.5s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "632/632 [==============================] - 6s 9ms/step - loss: 0.0308\n",
      "Epoch 2/100\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.0252\n",
      "Epoch 3/100\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.0242\n",
      "Epoch 4/100\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.0241\n",
      "Epoch 5/100\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.0236\n",
      "Epoch 6/100\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.0236\n",
      "Epoch 7/100\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.0232\n",
      "Epoch 8/100\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.0234\n",
      "Epoch 9/100\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 0.0234\n",
      "Epoch 10/100\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.0230\n",
      "Epoch 11/100\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 0.0231\n",
      "Epoch 12/100\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.0230\n",
      "Epoch 13/100\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.0230\n",
      "Epoch 14/100\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.0229\n",
      "Epoch 15/100\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.0231\n",
      "Epoch 16/100\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 0.0231\n",
      "Epoch 17/100\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 0.0231\n",
      "Epoch 00017: early stopping\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   41.6s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "632/632 [==============================] - 6s 9ms/step - loss: 0.0270\n",
      "Epoch 2/100\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.0238\n",
      "Epoch 3/100\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.0244\n",
      "Epoch 4/100\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.0232\n",
      "Epoch 5/100\n",
      "632/632 [==============================] - 4s 6ms/step - loss: 0.0232\n",
      "Epoch 6/100\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 0.0237\n",
      "Epoch 7/100\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 0.0233\n",
      "Epoch 00007: early stopping\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   43.1s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "631/631 [==============================] - 6s 9ms/step - loss: 0.0334\n",
      "Epoch 2/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0327A: \n",
      "Epoch 3/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0313\n",
      "Epoch 4/100\n",
      "631/631 [==============================] - 5s 7ms/step - loss: 0.0316\n",
      "Epoch 5/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0315\n",
      "Epoch 6/100\n",
      "631/631 [==============================] - 4s 6ms/step - loss: 0.0315\n",
      "Epoch 00006: early stopping\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   45.0s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:  2.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "631/631 [==============================] - 6s 10ms/step - loss: 0.0378\n",
      "Epoch 2/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0330\n",
      "Epoch 3/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0312\n",
      "Epoch 4/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0321\n",
      "Epoch 5/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0313\n",
      "Epoch 6/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0308\n",
      "Epoch 7/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0310\n",
      "Epoch 8/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0307\n",
      "Epoch 9/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0303\n",
      "Epoch 10/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0306\n",
      "Epoch 11/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0306\n",
      "Epoch 12/100\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0307\n",
      "Epoch 00012: early stopping\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   40.5s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "633/633 [==============================] - 6s 10ms/step - loss: 0.0347\n",
      "Epoch 2/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0320\n",
      "Epoch 3/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0319\n",
      "Epoch 4/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0312\n",
      "Epoch 5/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0317\n",
      "Epoch 6/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0308\n",
      "Epoch 7/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0313\n",
      "Epoch 8/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0308\n",
      "Epoch 9/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0310\n",
      "Epoch 10/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0307\n",
      "Epoch 11/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0308\n",
      "Epoch 12/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0309\n",
      "Epoch 13/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0308\n",
      "Epoch 00013: early stopping\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   37.9s\n",
      "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "633/633 [==============================] - 7s 11ms/step - loss: 0.0350\n",
      "Epoch 2/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0313\n",
      "Epoch 3/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0309\n",
      "Epoch 4/100\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.0308\n",
      "Epoch 5/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0310\n",
      "Epoch 6/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0308\n",
      "Epoch 7/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0307\n",
      "Epoch 8/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0308\n",
      "Epoch 9/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0307\n",
      "Epoch 10/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0307\n",
      "Epoch 11/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0306\n",
      "Epoch 12/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0307\n",
      "Epoch 13/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0306\n",
      "Epoch 14/100\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.0307\n",
      "Epoch 15/100\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.0306\n",
      "Epoch 16/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0309\n",
      "Epoch 00016: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team</th>\n",
       "      <th>predict</th>\n",
       "      <th>real</th>\n",
       "      <th>abs_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OB</td>\n",
       "      <td>4.316806</td>\n",
       "      <td>4.260797</td>\n",
       "      <td>0.056008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LG</td>\n",
       "      <td>4.475000</td>\n",
       "      <td>4.178030</td>\n",
       "      <td>0.296970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WO</td>\n",
       "      <td>3.186250</td>\n",
       "      <td>3.642802</td>\n",
       "      <td>0.456552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SK</td>\n",
       "      <td>7.656389</td>\n",
       "      <td>7.187593</td>\n",
       "      <td>0.468796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KT</td>\n",
       "      <td>3.725762</td>\n",
       "      <td>3.465459</td>\n",
       "      <td>0.260303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HH</td>\n",
       "      <td>5.886219</td>\n",
       "      <td>5.386646</td>\n",
       "      <td>0.499573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HT</td>\n",
       "      <td>4.903503</td>\n",
       "      <td>5.510668</td>\n",
       "      <td>0.607166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SS</td>\n",
       "      <td>5.244203</td>\n",
       "      <td>5.404469</td>\n",
       "      <td>0.160266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LT</td>\n",
       "      <td>4.084444</td>\n",
       "      <td>4.226215</td>\n",
       "      <td>0.141771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NC</td>\n",
       "      <td>5.292409</td>\n",
       "      <td>5.457439</td>\n",
       "      <td>0.165030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  team   predict      real  abs_difference\n",
       "0   OB  4.316806  4.260797        0.056008\n",
       "1   LG  4.475000  4.178030        0.296970\n",
       "2   WO  3.186250  3.642802        0.456552\n",
       "3   SK  7.656389  7.187593        0.468796\n",
       "4   KT  3.725762  3.465459        0.260303\n",
       "5   HH  5.886219  5.386646        0.499573\n",
       "6   HT  4.903503  5.510668        0.607166\n",
       "7   SS  5.244203  5.404469        0.160266\n",
       "8   LT  4.084444  4.226215        0.141771\n",
       "9   NC  5.292409  5.457439        0.165030"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team</th>\n",
       "      <th>predict</th>\n",
       "      <th>real</th>\n",
       "      <th>abs_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OB</td>\n",
       "      <td>4.316806</td>\n",
       "      <td>4.260797</td>\n",
       "      <td>0.056008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LG</td>\n",
       "      <td>4.475000</td>\n",
       "      <td>4.178030</td>\n",
       "      <td>0.296970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WO</td>\n",
       "      <td>3.186250</td>\n",
       "      <td>3.642802</td>\n",
       "      <td>0.456552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SK</td>\n",
       "      <td>7.656389</td>\n",
       "      <td>7.187593</td>\n",
       "      <td>0.468796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KT</td>\n",
       "      <td>3.725762</td>\n",
       "      <td>3.465459</td>\n",
       "      <td>0.260303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HH</td>\n",
       "      <td>5.886219</td>\n",
       "      <td>5.386646</td>\n",
       "      <td>0.499573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HT</td>\n",
       "      <td>4.903503</td>\n",
       "      <td>5.510668</td>\n",
       "      <td>0.607166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SS</td>\n",
       "      <td>5.244203</td>\n",
       "      <td>5.404469</td>\n",
       "      <td>0.160266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LT</td>\n",
       "      <td>4.084444</td>\n",
       "      <td>4.226215</td>\n",
       "      <td>0.141771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NC</td>\n",
       "      <td>5.292409</td>\n",
       "      <td>5.457439</td>\n",
       "      <td>0.165030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  team   predict      real  abs_difference\n",
       "0   OB  4.316806  4.260797        0.056008\n",
       "1   LG  4.475000  4.178030        0.296970\n",
       "2   WO  3.186250  3.642802        0.456552\n",
       "3   SK  7.656389  7.187593        0.468796\n",
       "4   KT  3.725762  3.465459        0.260303\n",
       "5   HH  5.886219  5.386646        0.499573\n",
       "6   HT  4.903503  5.510668        0.607166\n",
       "7   SS  5.244203  5.404469        0.160266\n",
       "8   LT  4.084444  4.226215        0.141771\n",
       "9   NC  5.292409  5.457439        0.165030"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_experiment_era(20,30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
